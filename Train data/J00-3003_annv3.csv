"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Email: stolcke@speech.sri.com.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DECLARATIVE-QUESTION You're a, you're senior now.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	APPRECIATION Oh, good you.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	B BACKCHANNEL Yeah.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	SIGNAL-NoN-UNDERSTANDING say?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	B STATEMENT N C State.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA represents meaning utterance level illocutionary force (Austin 1962).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 1 shows sample kind discourse structure interested.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	cases, DA labels would enrich available input higher-level processing spoken words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Another important role DA information could feedback lower-level processing.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 2 42 dialogue act labels.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA frequencies given percentages total number utterances overall corpus.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	36% Uh-huh.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	19% think it's great 13% So, -/ 6% That's exactly it.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5% imagine.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2% special training?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2% <Laughter>, < Throat_clearing> 2% Yes.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% Well, it's nice talking you.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% wear work today?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% No. 1% Oh, okay.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% don't know I'm making sense not.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% afford get house?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% Well give break, know.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% right?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.5% is. .4% don't go first .4% aren't contributing.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.2% I'm drawing blank.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.3% Well, .2% Uh, whole lot.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% Excuse me? .1% don't know .1% you?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% company?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% Well, much that.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% goodness, Diane, get there.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	.1% Something like <.1% Right?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	<.1% kind buff?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	<.1% I'm sorry.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	so, pull together previous approaches well new ideas.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	results, besides validating methods described, interest several reasons.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	keep presentation interesting concrete, alternate description general methods empirical results.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Section 2 describes task data detail.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Section 5 describe experiments DA classification.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Section 6 shows DA models used benefit speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prior related work summarized Section 7.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2.1 Utterance Segmentation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	refer units segmentation utterances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	utterance unit identified one DA, annotated single DA label.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2.2 Tag Set.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	began DAMSL markup system, modified several ways make relevant corpus task.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	different tasks, techniques used study might prove useful others could greater importance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 2 shows 42 categories examples relative frequencies.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2.3 Major Dialogue Act Types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	frequent DA types briefly characterized below.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Statements Opinions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	common types utterances STATEMENTS OPINIONS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	STATEMENT He's five months old OPINION Well, rabbits darling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	OPINION think would kind stressful.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	OPINIONS often include hedges think, believe, seems, mean.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Questions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Questions several types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	See Weber (1993) survey declarative questions various realizations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	YEs-No-QUESTION doesn't eliminate it, it?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DECLARATIVE- QUESTION you're taking government course?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	WH-QUESTION Well, old you?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Backchannels.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	APPRECIATION Urn.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Sounds good.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Turn Exits Abandoned Utterances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Abandoned utterances speaker breaks without finishing, followed restart.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Turn exits tend single words, often or.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Similarly, also coded NO-ANSWERS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Answers also semantically significant since likely contain new information.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	common AGREEMENT/AccEPTS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	often yes yeah, look lot like ANSWERS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	describe mathematical computational framework used study.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	(Speaker labels introduced Section 4.)	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	likelihood usually much straightforward model posterior itself.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	3.1 Dialogue Act Likelihoods.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	computation likelihoods P(EIU ) depends types evidence used.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	described later, involves considering multiple alternative recognized word sequences.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ease reference, random variables used summarized Table 4.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	variables used subscripts refer individual utterances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, Wi word transcription ith utterance within conversation (not ith word).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	means likelihood given complete conversation factored likelihoods given individual utterances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Ui,..., Un), n number utterances conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	3.2 Markov Modeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Ui1) (3) (k order Markov process describing U).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	n-gram-based discourse grammars used property.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	3.3 Dialogue Act Decoding.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Therefore, used method throughout.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Obviously, possible off-line processing, full conversation available.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	HMM terminology chosen mainly historical reasons..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 5 Perplexities DAs without turn information.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, incremental gain trigram small, higher-order models prove useful.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	4.2 Discourse Models.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Clearly, dialogue grammar adaptation needs research.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	structured tasks situation might different.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, found exploitable structure.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally, present results combination knowledge sources.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	4 5.1 Dialogue Act Classification Using Words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA classification using words based observation different DAs use distinctive word strings.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Similarly, find distinctive correlations certain phrases DA types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.1.1 Classification True Words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	4 frequency STATEMENTS across labeled data slightly different, cf.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 2..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	~ Ui ~ ....	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.1.2 Classification Recognized Words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	thorough use recognized speech derived follows.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Note another approximation modeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, different DAs common words may realized different word pronunciations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	According rationale, appropriate divideall score components ),.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also compared n-best DA classification approach straightforward 1-best approach.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2 Dialogue Act Classification Using Prosody.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prosody important DA recognition two reasons.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	First, saw earlier, word-based classification suffers recognition errors.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Second, utterances inherently ambiguous based words alone.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also touch briefly alternative machine learning models prosodic features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2.1 Prosodic Features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	node labeled majority class node, well posterior probabilities two classes.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	case utterance duration, measure correlates length words overall speaking rate.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	appropriate, included raw features values normalized utterance and/or conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2.2 Prosodic Decision Trees.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	chose second approach, downsampling training data equate DA proportions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2.3 Results Decision Trees.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	purpose model integration, likelihoods class assigned DA types comprised class.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 6).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2.4 Neural Network Classifiers.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Neural networks worth investigating since offer potential advantages decision trees.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	results summarized Table 8, along baseline result obtained decision tree model.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.2.5 Intonation Event Likelihoods.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.3 Using Multiple Knowledge Sources.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	mentioned earlier, expect improved performance combining word prosodic information.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	One important respect independence assumption violated modeling utterance length.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	need allow fact models combined Equation 8 give estimates differing qualities.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	reported results aggregate outcome two test set halves.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 9 Combined utterance classification accuracies (chance = 35%).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	first two columns correspond Tables 7 6, respectively.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Results summarized Table 9.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	5.3.2 Focused Classifications.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Chance performance experiments therefore 50%.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Results summarized Table 10.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	shown, combined classifier consistently accurate classifier using words alone.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	latter leveraged accurate speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	6.1 Integrating DA Modeling ASR.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	8 7 Note similarity Equations 10 1.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	identical except fact now.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	probabilities, extremely important practice.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach explained detail footnote 5 applies well.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	call mixture-of-LMs approach.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	6.2 Computational Structure Mixture Modeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	6.3 Experiments Results.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	thus approximation mixture approaches top DA considered.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA-specific LMs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	VERBMOBIL corpus consists two-party scheduling dialogues.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 13 shows commonly used versions tag sets three tasks.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	discussed earlier, domains differ Switchboard corpus task-oriented.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	tag sets also generally smaller, problems balance occur.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	results summarized table also illustrate differences inherent difficulty tasks.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	VERBMOBIL.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Examples Jekat et al. (1995).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"12 DAs ""move types"" used Map Task."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Examples Taylor et al. (1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	CHECK going Indian Country?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	QUERY-YN got graveyard written ? QUERY-W where?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	REPLY-N No, don't REPLY-W {And across to?}	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	pyramid.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	READY Okay ATR.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Examples English translations given Nagata (1992).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Word-based posterior probability estimators also possible, although less common.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	work mentioned far uses statistical models various kinds.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach presented differ various earlier models, particularly based HMMs?	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	made number significant simplifications arrive computationally statistically tractable formulation.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	research within framework characterized simplifications addressed.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	addition, model nonlocal aspects discourse structure, despite negative results far.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	study automatically extracted prosodic features DA modeling likewise infancy.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	reason skewed distribution nature task (or lack thereof) Switchboard.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	remains seen fine-grained DA distinctions made reliably corpus.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Hand-labeled DA categories may still serve important role initializing algorithm.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Despite shortcomings, Switchboard domain could serve purpose.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	grammar.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	components model automatically trained, thus applicable domains labeled data available.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	edu/ling/jurafsky/ws97/.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
"(Stolcke et al., 2000) use HMMs dialogue modelling, sequences observations correspond sequences dialogue act types.</S><S sid =""14"" ssid = ""3"">They also explore performance decision trees neural networks report highest accuracy 65% Switchboard corpus."	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Email: stolcke@speech.sri.com.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DECLARATIVE-QUESTION You're a, you're senior now.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	APPRECIATION Oh, good you.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	B BACKCHANNEL Yeah.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	SIGNAL-NoN-UNDERSTANDING say?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	B STATEMENT N C State.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA represents meaning utterance level illocutionary force (Austin 1962).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 1 shows sample kind discourse structure interested.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	cases, DA labels would enrich available input higher-level processing spoken words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Another important role DA information could feedback lower-level processing.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 2 42 dialogue act labels.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA frequencies given percentages total number utterances overall corpus.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	36% Uh-huh.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	19% think it's great 13% So, -/ 6% That's exactly it.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5% imagine.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2% special training?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2% <Laughter>, < Throat_clearing> 2% Yes.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% Well, it's nice talking you.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% wear work today?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% No. 1% Oh, okay.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% don't know I'm making sense not.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% afford get house?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% Well give break, know.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% right?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.5% is. .4% don't go first .4% aren't contributing.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.2% I'm drawing blank.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.3% Well, .2% Uh, whole lot.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% Excuse me? .1% don't know .1% you?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% company?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% Well, much that.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% goodness, Diane, get there.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	.1% Something like <.1% Right?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	<.1% kind buff?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	<.1% I'm sorry.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	so, pull together previous approaches well new ideas.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	results, besides validating methods described, interest several reasons.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	keep presentation interesting concrete, alternate description general methods empirical results.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Section 2 describes task data detail.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Section 5 describe experiments DA classification.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Section 6 shows DA models used benefit speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prior related work summarized Section 7.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2.1 Utterance Segmentation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	refer units segmentation utterances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	utterance unit identified one DA, annotated single DA label.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2.2 Tag Set.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	began DAMSL markup system, modified several ways make relevant corpus task.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	different tasks, techniques used study might prove useful others could greater importance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 2 shows 42 categories examples relative frequencies.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2.3 Major Dialogue Act Types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	frequent DA types briefly characterized below.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Statements Opinions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	common types utterances STATEMENTS OPINIONS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	STATEMENT He's five months old OPINION Well, rabbits darling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	OPINION think would kind stressful.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	OPINIONS often include hedges think, believe, seems, mean.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Questions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Questions several types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	See Weber (1993) survey declarative questions various realizations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	YEs-No-QUESTION doesn't eliminate it, it?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DECLARATIVE- QUESTION you're taking government course?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	WH-QUESTION Well, old you?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Backchannels.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	APPRECIATION Urn.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Sounds good.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Turn Exits Abandoned Utterances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Abandoned utterances speaker breaks without finishing, followed restart.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Turn exits tend single words, often or.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Similarly, also coded NO-ANSWERS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Answers also semantically significant since likely contain new information.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	common AGREEMENT/AccEPTS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	often yes yeah, look lot like ANSWERS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	describe mathematical computational framework used study.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	(Speaker labels introduced Section 4.)	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	likelihood usually much straightforward model posterior itself.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3.1 Dialogue Act Likelihoods.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	computation likelihoods P(EIU ) depends types evidence used.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	described later, involves considering multiple alternative recognized word sequences.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ease reference, random variables used summarized Table 4.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	variables used subscripts refer individual utterances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, Wi word transcription ith utterance within conversation (not ith word).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	means likelihood given complete conversation factored likelihoods given individual utterances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Ui,..., Un), n number utterances conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3.2 Markov Modeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Ui1) (3) (k order Markov process describing U).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	n-gram-based discourse grammars used property.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3.3 Dialogue Act Decoding.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Therefore, used method throughout.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Obviously, possible off-line processing, full conversation available.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	HMM terminology chosen mainly historical reasons..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 5 Perplexities DAs without turn information.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, incremental gain trigram small, higher-order models prove useful.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	4.2 Discourse Models.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Clearly, dialogue grammar adaptation needs research.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	structured tasks situation might different.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, found exploitable structure.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally, present results combination knowledge sources.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	4 5.1 Dialogue Act Classification Using Words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA classification using words based observation different DAs use distinctive word strings.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Similarly, find distinctive correlations certain phrases DA types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.1.1 Classification True Words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	4 frequency STATEMENTS across labeled data slightly different, cf.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 2..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	~ Ui ~ ....	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.1.2 Classification Recognized Words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	thorough use recognized speech derived follows.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Note another approximation modeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, different DAs common words may realized different word pronunciations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	According rationale, appropriate divideall score components ),.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also compared n-best DA classification approach straightforward 1-best approach.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2 Dialogue Act Classification Using Prosody.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prosody important DA recognition two reasons.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	First, saw earlier, word-based classification suffers recognition errors.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Second, utterances inherently ambiguous based words alone.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also touch briefly alternative machine learning models prosodic features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2.1 Prosodic Features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	node labeled majority class node, well posterior probabilities two classes.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	case utterance duration, measure correlates length words overall speaking rate.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	appropriate, included raw features values normalized utterance and/or conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2.2 Prosodic Decision Trees.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	chose second approach, downsampling training data equate DA proportions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2.3 Results Decision Trees.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	purpose model integration, likelihoods class assigned DA types comprised class.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 6).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2.4 Neural Network Classifiers.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Neural networks worth investigating since offer potential advantages decision trees.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	results summarized Table 8, along baseline result obtained decision tree model.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.2.5 Intonation Event Likelihoods.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.3 Using Multiple Knowledge Sources.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	mentioned earlier, expect improved performance combining word prosodic information.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	One important respect independence assumption violated modeling utterance length.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	need allow fact models combined Equation 8 give estimates differing qualities.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	reported results aggregate outcome two test set halves.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 9 Combined utterance classification accuracies (chance = 35%).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	first two columns correspond Tables 7 6, respectively.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Results summarized Table 9.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	5.3.2 Focused Classifications.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Chance performance experiments therefore 50%.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Results summarized Table 10.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	shown, combined classifier consistently accurate classifier using words alone.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	latter leveraged accurate speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	6.1 Integrating DA Modeling ASR.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	8 7 Note similarity Equations 10 1.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	identical except fact now.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	probabilities, extremely important practice.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach explained detail footnote 5 applies well.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	call mixture-of-LMs approach.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	6.2 Computational Structure Mixture Modeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	6.3 Experiments Results.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	thus approximation mixture approaches top DA considered.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA-specific LMs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	VERBMOBIL corpus consists two-party scheduling dialogues.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 13 shows commonly used versions tag sets three tasks.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	discussed earlier, domains differ Switchboard corpus task-oriented.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	tag sets also generally smaller, problems balance occur.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	results summarized table also illustrate differences inherent difficulty tasks.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	VERBMOBIL.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Examples Jekat et al. (1995).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"12 DAs ""move types"" used Map Task."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Examples Taylor et al. (1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	CHECK going Indian Country?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	QUERY-YN got graveyard written ? QUERY-W where?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	REPLY-N No, don't REPLY-W {And across to?}	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	pyramid.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	READY Okay ATR.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Examples English translations given Nagata (1992).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Word-based posterior probability estimators also possible, although less common.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	work mentioned far uses statistical models various kinds.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach presented differ various earlier models, particularly based HMMs?	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	made number significant simplifications arrive computationally statistically tractable formulation.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	research within framework characterized simplifications addressed.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	addition, model nonlocal aspects discourse structure, despite negative results far.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	study automatically extracted prosodic features DA modeling likewise infancy.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	reason skewed distribution nature task (or lack thereof) Switchboard.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	remains seen fine-grained DA distinctions made reliably corpus.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Hand-labeled DA categories may still serve important role initializing algorithm.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Despite shortcomings, Switchboard domain could serve purpose.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	grammar.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	components model automatically trained, thus applicable domains labeled data available.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	edu/ling/jurafsky/ws97/.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
date, majority work dialogue act modeling addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Email: stolcke@speech.sri.com.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DECLARATIVE-QUESTION You're a, you're senior now.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION Oh, good you.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B BACKCHANNEL Yeah.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	SIGNAL-NoN-UNDERSTANDING say?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B STATEMENT N C State.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA represents meaning utterance level illocutionary force (Austin 1962).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 1 shows sample kind discourse structure interested.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	cases, DA labels would enrich available input higher-level processing spoken words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Another important role DA information could feedback lower-level processing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2 42 dialogue act labels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA frequencies given percentages total number utterances overall corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	36% Uh-huh.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	19% think it's great 13% So, -/ 6% That's exactly it.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5% imagine.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2% special training?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2% <Laughter>, < Throat_clearing> 2% Yes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% Well, it's nice talking you.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% wear work today?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% No. 1% Oh, okay.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% don't know I'm making sense not.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% afford get house?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% Well give break, know.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.5% is. .4% don't go first .4% aren't contributing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.2% I'm drawing blank.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.3% Well, .2% Uh, whole lot.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Excuse me? .1% don't know .1% you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% company?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Well, much that.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% goodness, Diane, get there.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Something like <.1% Right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% kind buff?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% I'm sorry.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	so, pull together previous approaches well new ideas.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results, besides validating methods described, interest several reasons.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	keep presentation interesting concrete, alternate description general methods empirical results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 2 describes task data detail.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 5 describe experiments DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 6 shows DA models used benefit speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prior related work summarized Section 7.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.1 Utterance Segmentation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	refer units segmentation utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance unit identified one DA, annotated single DA label.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.2 Tag Set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	began DAMSL markup system, modified several ways make relevant corpus task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	different tasks, techniques used study might prove useful others could greater importance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2 shows 42 categories examples relative frequencies.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.3 Major Dialogue Act Types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	frequent DA types briefly characterized below.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Statements Opinions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	common types utterances STATEMENTS OPINIONS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	STATEMENT He's five months old OPINION Well, rabbits darling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	OPINION think would kind stressful.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	OPINIONS often include hedges think, believe, seems, mean.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Questions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Questions several types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	See Weber (1993) survey declarative questions various realizations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YEs-No-QUESTION doesn't eliminate it, it?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DECLARATIVE- QUESTION you're taking government course?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	WH-QUESTION Well, old you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Backchannels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION Urn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Sounds good.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn Exits Abandoned Utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Abandoned utterances speaker breaks without finishing, followed restart.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn exits tend single words, often or.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, also coded NO-ANSWERS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Answers also semantically significant since likely contain new information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	common AGREEMENT/AccEPTS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	often yes yeah, look lot like ANSWERS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe mathematical computational framework used study.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Speaker labels introduced Section 4.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	likelihood usually much straightforward model posterior itself.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.1 Dialogue Act Likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	computation likelihoods P(EIU ) depends types evidence used.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	described later, involves considering multiple alternative recognized word sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ease reference, random variables used summarized Table 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	variables used subscripts refer individual utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, Wi word transcription ith utterance within conversation (not ith word).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	means likelihood given complete conversation factored likelihoods given individual utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Ui,..., Un), n number utterances conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.2 Markov Modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Ui1) (3) (k order Markov process describing U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	n-gram-based discourse grammars used property.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.3 Dialogue Act Decoding.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, used method throughout.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Obviously, possible off-line processing, full conversation available.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM terminology chosen mainly historical reasons..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 5 Perplexities DAs without turn information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, incremental gain trigram small, higher-order models prove useful.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4.2 Discourse Models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Clearly, dialogue grammar adaptation needs research.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	structured tasks situation might different.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, found exploitable structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, present results combination knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4 5.1 Dialogue Act Classification Using Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA classification using words based observation different DAs use distinctive word strings.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, find distinctive correlations certain phrases DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.1.1 Classification True Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4 frequency STATEMENTS across labeled data slightly different, cf.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Ui ~ ....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.1.2 Classification Recognized Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thorough use recognized speech derived follows.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note another approximation modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, different DAs common words may realized different word pronunciations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	According rationale, appropriate divideall score components ),.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also compared n-best DA classification approach straightforward 1-best approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2 Dialogue Act Classification Using Prosody.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosody important DA recognition two reasons.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	First, saw earlier, word-based classification suffers recognition errors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, utterances inherently ambiguous based words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also touch briefly alternative machine learning models prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.1 Prosodic Features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	node labeled majority class node, well posterior probabilities two classes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	case utterance duration, measure correlates length words overall speaking rate.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	appropriate, included raw features values normalized utterance and/or conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.2 Prosodic Decision Trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chose second approach, downsampling training data equate DA proportions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.3 Results Decision Trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	purpose model integration, likelihoods class assigned DA types comprised class.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 6).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.4 Neural Network Classifiers.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Neural networks worth investigating since offer potential advantages decision trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results summarized Table 8, along baseline result obtained decision tree model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.5 Intonation Event Likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.3 Using Multiple Knowledge Sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	mentioned earlier, expect improved performance combining word prosodic information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One important respect independence assumption violated modeling utterance length.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	need allow fact models combined Equation 8 give estimates differing qualities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	reported results aggregate outcome two test set halves.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 9 Combined utterance classification accuracies (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	first two columns correspond Tables 7 6, respectively.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Results summarized Table 9.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.3.2 Focused Classifications.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Chance performance experiments therefore 50%.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Results summarized Table 10.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown, combined classifier consistently accurate classifier using words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	latter leveraged accurate speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.1 Integrating DA Modeling ASR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	8 7 Note similarity Equations 10 1.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	identical except fact now.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	probabilities, extremely important practice.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach explained detail footnote 5 applies well.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	call mixture-of-LMs approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.2 Computational Structure Mixture Modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.3 Experiments Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thus approximation mixture approaches top DA considered.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-specific LMs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	VERBMOBIL corpus consists two-party scheduling dialogues.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 13 shows commonly used versions tag sets three tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed earlier, domains differ Switchboard corpus task-oriented.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag sets also generally smaller, problems balance occur.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results summarized table also illustrate differences inherent difficulty tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	VERBMOBIL.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples Jekat et al. (1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"12 DAs ""move types"" used Map Task."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples Taylor et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	CHECK going Indian Country?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	QUERY-YN got graveyard written ? QUERY-W where?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	REPLY-N No, don't REPLY-W {And across to?}	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	pyramid.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	READY Okay ATR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples English translations given Nagata (1992).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Word-based posterior probability estimators also possible, although less common.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	work mentioned far uses statistical models various kinds.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach presented differ various earlier models, particularly based HMMs?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	made number significant simplifications arrive computationally statistically tractable formulation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	research within framework characterized simplifications addressed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	addition, model nonlocal aspects discourse structure, despite negative results far.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	study automatically extracted prosodic features DA modeling likewise infancy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	reason skewed distribution nature task (or lack thereof) Switchboard.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	remains seen fine-grained DA distinctions made reliably corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Hand-labeled DA categories may still serve important role initializing algorithm.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Despite shortcomings, Switchboard domain could serve purpose.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	components model automatically trained, thus applicable domains labeled data available.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	edu/ling/jurafsky/ws97/.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Email: stolcke@speech.sri.com.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DECLARATIVE-QUESTION You're a, you're senior now.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	APPRECIATION Oh, good you.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	B BACKCHANNEL Yeah.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	SIGNAL-NoN-UNDERSTANDING say?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	B STATEMENT N C State.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA represents meaning utterance level illocutionary force (Austin 1962).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 1 shows sample kind discourse structure interested.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	cases, DA labels would enrich available input higher-level processing spoken words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Another important role DA information could feedback lower-level processing.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 2 42 dialogue act labels.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA frequencies given percentages total number utterances overall corpus.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	36% Uh-huh.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	19% think it's great 13% So, -/ 6% That's exactly it.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5% imagine.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2% special training?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2% <Laughter>, < Throat_clearing> 2% Yes.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% Well, it's nice talking you.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% wear work today?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% No. 1% Oh, okay.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% don't know I'm making sense not.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% afford get house?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% Well give break, know.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% right?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.5% is. .4% don't go first .4% aren't contributing.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.2% I'm drawing blank.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.3% Well, .2% Uh, whole lot.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% Excuse me? .1% don't know .1% you?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% company?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% Well, much that.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% goodness, Diane, get there.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	.1% Something like <.1% Right?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	<.1% kind buff?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	<.1% I'm sorry.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	so, pull together previous approaches well new ideas.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	results, besides validating methods described, interest several reasons.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	keep presentation interesting concrete, alternate description general methods empirical results.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Section 2 describes task data detail.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Section 5 describe experiments DA classification.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Section 6 shows DA models used benefit speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prior related work summarized Section 7.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2.1 Utterance Segmentation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	refer units segmentation utterances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	utterance unit identified one DA, annotated single DA label.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2.2 Tag Set.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	began DAMSL markup system, modified several ways make relevant corpus task.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	different tasks, techniques used study might prove useful others could greater importance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 2 shows 42 categories examples relative frequencies.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2.3 Major Dialogue Act Types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	frequent DA types briefly characterized below.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Statements Opinions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	common types utterances STATEMENTS OPINIONS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	STATEMENT He's five months old OPINION Well, rabbits darling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	OPINION think would kind stressful.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	OPINIONS often include hedges think, believe, seems, mean.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Questions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Questions several types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	See Weber (1993) survey declarative questions various realizations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	YEs-No-QUESTION doesn't eliminate it, it?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DECLARATIVE- QUESTION you're taking government course?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	WH-QUESTION Well, old you?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Backchannels.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	APPRECIATION Urn.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Sounds good.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Turn Exits Abandoned Utterances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Abandoned utterances speaker breaks without finishing, followed restart.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Turn exits tend single words, often or.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Similarly, also coded NO-ANSWERS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Answers also semantically significant since likely contain new information.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	common AGREEMENT/AccEPTS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	often yes yeah, look lot like ANSWERS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	describe mathematical computational framework used study.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	(Speaker labels introduced Section 4.)	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	likelihood usually much straightforward model posterior itself.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	3.1 Dialogue Act Likelihoods.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	computation likelihoods P(EIU ) depends types evidence used.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	described later, involves considering multiple alternative recognized word sequences.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ease reference, random variables used summarized Table 4.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	variables used subscripts refer individual utterances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, Wi word transcription ith utterance within conversation (not ith word).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	means likelihood given complete conversation factored likelihoods given individual utterances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Ui,..., Un), n number utterances conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	3.2 Markov Modeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Ui1) (3) (k order Markov process describing U).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	n-gram-based discourse grammars used property.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	3.3 Dialogue Act Decoding.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Therefore, used method throughout.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Obviously, possible off-line processing, full conversation available.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	HMM terminology chosen mainly historical reasons..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 5 Perplexities DAs without turn information.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, incremental gain trigram small, higher-order models prove useful.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	4.2 Discourse Models.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Clearly, dialogue grammar adaptation needs research.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	structured tasks situation might different.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, found exploitable structure.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally, present results combination knowledge sources.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	4 5.1 Dialogue Act Classification Using Words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA classification using words based observation different DAs use distinctive word strings.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Similarly, find distinctive correlations certain phrases DA types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.1.1 Classification True Words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	4 frequency STATEMENTS across labeled data slightly different, cf.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 2..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	~ Ui ~ ....	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.1.2 Classification Recognized Words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	thorough use recognized speech derived follows.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Note another approximation modeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, different DAs common words may realized different word pronunciations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	According rationale, appropriate divideall score components ),.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also compared n-best DA classification approach straightforward 1-best approach.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2 Dialogue Act Classification Using Prosody.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prosody important DA recognition two reasons.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	First, saw earlier, word-based classification suffers recognition errors.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Second, utterances inherently ambiguous based words alone.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also touch briefly alternative machine learning models prosodic features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2.1 Prosodic Features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	node labeled majority class node, well posterior probabilities two classes.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	case utterance duration, measure correlates length words overall speaking rate.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	appropriate, included raw features values normalized utterance and/or conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2.2 Prosodic Decision Trees.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	chose second approach, downsampling training data equate DA proportions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2.3 Results Decision Trees.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	purpose model integration, likelihoods class assigned DA types comprised class.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 6).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2.4 Neural Network Classifiers.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Neural networks worth investigating since offer potential advantages decision trees.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	results summarized Table 8, along baseline result obtained decision tree model.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.2.5 Intonation Event Likelihoods.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.3 Using Multiple Knowledge Sources.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	mentioned earlier, expect improved performance combining word prosodic information.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	One important respect independence assumption violated modeling utterance length.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	need allow fact models combined Equation 8 give estimates differing qualities.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	reported results aggregate outcome two test set halves.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 9 Combined utterance classification accuracies (chance = 35%).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	first two columns correspond Tables 7 6, respectively.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Results summarized Table 9.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	5.3.2 Focused Classifications.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Chance performance experiments therefore 50%.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Results summarized Table 10.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	shown, combined classifier consistently accurate classifier using words alone.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	latter leveraged accurate speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	6.1 Integrating DA Modeling ASR.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	8 7 Note similarity Equations 10 1.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	identical except fact now.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	probabilities, extremely important practice.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach explained detail footnote 5 applies well.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	call mixture-of-LMs approach.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	6.2 Computational Structure Mixture Modeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	6.3 Experiments Results.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	thus approximation mixture approaches top DA considered.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA-specific LMs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	VERBMOBIL corpus consists two-party scheduling dialogues.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 13 shows commonly used versions tag sets three tasks.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	discussed earlier, domains differ Switchboard corpus task-oriented.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	tag sets also generally smaller, problems balance occur.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	results summarized table also illustrate differences inherent difficulty tasks.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	VERBMOBIL.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Examples Jekat et al. (1995).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"12 DAs ""move types"" used Map Task."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Examples Taylor et al. (1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	CHECK going Indian Country?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	QUERY-YN got graveyard written ? QUERY-W where?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	REPLY-N No, don't REPLY-W {And across to?}	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	pyramid.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	READY Okay ATR.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Examples English translations given Nagata (1992).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Word-based posterior probability estimators also possible, although less common.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	work mentioned far uses statistical models various kinds.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach presented differ various earlier models, particularly based HMMs?	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	made number significant simplifications arrive computationally statistically tractable formulation.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	research within framework characterized simplifications addressed.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	addition, model nonlocal aspects discourse structure, despite negative results far.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	study automatically extracted prosodic features DA modeling likewise infancy.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	reason skewed distribution nature task (or lack thereof) Switchboard.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	remains seen fine-grained DA distinctions made reliably corpus.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Hand-labeled DA categories may still serve important role initializing algorithm.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Despite shortcomings, Switchboard domain could serve purpose.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	grammar.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	components model automatically trained, thus applicable domains labeled data available.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	edu/ling/jurafsky/ws97/.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
also Dialogue Acts modeling approaches automatic tagging recognition conversational speech (Stolcke et al., 2000) related work corpus linguistics machine learning techniques used find conversational patterns spoken transcripts dialogue corpus (Shawar Atwell, 2005).	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Email: stolcke@speech.sri.com.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DECLARATIVE-QUESTION You're a, you're senior now.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	APPRECIATION Oh, good you.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	B BACKCHANNEL Yeah.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	SIGNAL-NoN-UNDERSTANDING say?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	B STATEMENT N C State.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA represents meaning utterance level illocutionary force (Austin 1962).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 1 shows sample kind discourse structure interested.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	cases, DA labels would enrich available input higher-level processing spoken words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Another important role DA information could feedback lower-level processing.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 2 42 dialogue act labels.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA frequencies given percentages total number utterances overall corpus.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	36% Uh-huh.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	19% think it's great 13% So, -/ 6% That's exactly it.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5% imagine.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2% special training?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2% <Laughter>, < Throat_clearing> 2% Yes.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% Well, it's nice talking you.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% wear work today?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% No. 1% Oh, okay.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% don't know I'm making sense not.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% afford get house?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% Well give break, know.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% right?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.5% is. .4% don't go first .4% aren't contributing.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.2% I'm drawing blank.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.3% Well, .2% Uh, whole lot.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% Excuse me? .1% don't know .1% you?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% company?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% Well, much that.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% goodness, Diane, get there.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	.1% Something like <.1% Right?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	<.1% kind buff?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	<.1% I'm sorry.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	so, pull together previous approaches well new ideas.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	results, besides validating methods described, interest several reasons.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	keep presentation interesting concrete, alternate description general methods empirical results.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Section 2 describes task data detail.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Section 5 describe experiments DA classification.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Section 6 shows DA models used benefit speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prior related work summarized Section 7.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2.1 Utterance Segmentation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	refer units segmentation utterances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	utterance unit identified one DA, annotated single DA label.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2.2 Tag Set.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	began DAMSL markup system, modified several ways make relevant corpus task.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	different tasks, techniques used study might prove useful others could greater importance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 2 shows 42 categories examples relative frequencies.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2.3 Major Dialogue Act Types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	frequent DA types briefly characterized below.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Statements Opinions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	common types utterances STATEMENTS OPINIONS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	STATEMENT He's five months old OPINION Well, rabbits darling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	OPINION think would kind stressful.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	OPINIONS often include hedges think, believe, seems, mean.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Questions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Questions several types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	See Weber (1993) survey declarative questions various realizations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	YEs-No-QUESTION doesn't eliminate it, it?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DECLARATIVE- QUESTION you're taking government course?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	WH-QUESTION Well, old you?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Backchannels.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	APPRECIATION Urn.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Sounds good.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Turn Exits Abandoned Utterances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Abandoned utterances speaker breaks without finishing, followed restart.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Turn exits tend single words, often or.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Similarly, also coded NO-ANSWERS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Answers also semantically significant since likely contain new information.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	common AGREEMENT/AccEPTS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	often yes yeah, look lot like ANSWERS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	describe mathematical computational framework used study.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	(Speaker labels introduced Section 4.)	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	likelihood usually much straightforward model posterior itself.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	3.1 Dialogue Act Likelihoods.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	computation likelihoods P(EIU ) depends types evidence used.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	described later, involves considering multiple alternative recognized word sequences.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ease reference, random variables used summarized Table 4.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	variables used subscripts refer individual utterances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, Wi word transcription ith utterance within conversation (not ith word).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	means likelihood given complete conversation factored likelihoods given individual utterances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Ui,..., Un), n number utterances conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	3.2 Markov Modeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Ui1) (3) (k order Markov process describing U).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	n-gram-based discourse grammars used property.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	3.3 Dialogue Act Decoding.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Therefore, used method throughout.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Obviously, possible off-line processing, full conversation available.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	HMM terminology chosen mainly historical reasons..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 5 Perplexities DAs without turn information.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, incremental gain trigram small, higher-order models prove useful.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	4.2 Discourse Models.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Clearly, dialogue grammar adaptation needs research.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	structured tasks situation might different.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, found exploitable structure.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally, present results combination knowledge sources.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	4 5.1 Dialogue Act Classification Using Words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA classification using words based observation different DAs use distinctive word strings.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Similarly, find distinctive correlations certain phrases DA types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.1.1 Classification True Words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	4 frequency STATEMENTS across labeled data slightly different, cf.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 2..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	~ Ui ~ ....	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.1.2 Classification Recognized Words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	thorough use recognized speech derived follows.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Note another approximation modeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, different DAs common words may realized different word pronunciations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	According rationale, appropriate divideall score components ),.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also compared n-best DA classification approach straightforward 1-best approach.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2 Dialogue Act Classification Using Prosody.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prosody important DA recognition two reasons.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	First, saw earlier, word-based classification suffers recognition errors.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Second, utterances inherently ambiguous based words alone.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also touch briefly alternative machine learning models prosodic features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2.1 Prosodic Features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	node labeled majority class node, well posterior probabilities two classes.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	case utterance duration, measure correlates length words overall speaking rate.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	appropriate, included raw features values normalized utterance and/or conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2.2 Prosodic Decision Trees.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	chose second approach, downsampling training data equate DA proportions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2.3 Results Decision Trees.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	purpose model integration, likelihoods class assigned DA types comprised class.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 6).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2.4 Neural Network Classifiers.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Neural networks worth investigating since offer potential advantages decision trees.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	results summarized Table 8, along baseline result obtained decision tree model.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.2.5 Intonation Event Likelihoods.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.3 Using Multiple Knowledge Sources.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	mentioned earlier, expect improved performance combining word prosodic information.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	One important respect independence assumption violated modeling utterance length.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	need allow fact models combined Equation 8 give estimates differing qualities.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	reported results aggregate outcome two test set halves.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 9 Combined utterance classification accuracies (chance = 35%).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	first two columns correspond Tables 7 6, respectively.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Results summarized Table 9.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	5.3.2 Focused Classifications.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Chance performance experiments therefore 50%.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Results summarized Table 10.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	shown, combined classifier consistently accurate classifier using words alone.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	latter leveraged accurate speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	6.1 Integrating DA Modeling ASR.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	8 7 Note similarity Equations 10 1.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	identical except fact now.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	probabilities, extremely important practice.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach explained detail footnote 5 applies well.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	call mixture-of-LMs approach.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	6.2 Computational Structure Mixture Modeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	6.3 Experiments Results.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	thus approximation mixture approaches top DA considered.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA-specific LMs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	VERBMOBIL corpus consists two-party scheduling dialogues.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 13 shows commonly used versions tag sets three tasks.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	discussed earlier, domains differ Switchboard corpus task-oriented.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	tag sets also generally smaller, problems balance occur.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	results summarized table also illustrate differences inherent difficulty tasks.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	VERBMOBIL.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Examples Jekat et al. (1995).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"12 DAs ""move types"" used Map Task."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Examples Taylor et al. (1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	CHECK going Indian Country?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	QUERY-YN got graveyard written ? QUERY-W where?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	REPLY-N No, don't REPLY-W {And across to?}	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	pyramid.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	READY Okay ATR.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Examples English translations given Nagata (1992).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Word-based posterior probability estimators also possible, although less common.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	work mentioned far uses statistical models various kinds.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach presented differ various earlier models, particularly based HMMs?	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	made number significant simplifications arrive computationally statistically tractable formulation.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	research within framework characterized simplifications addressed.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	addition, model nonlocal aspects discourse structure, despite negative results far.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	study automatically extracted prosodic features DA modeling likewise infancy.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	reason skewed distribution nature task (or lack thereof) Switchboard.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	remains seen fine-grained DA distinctions made reliably corpus.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Hand-labeled DA categories may still serve important role initializing algorithm.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Despite shortcomings, Switchboard domain could serve purpose.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	grammar.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	components model automatically trained, thus applicable domains labeled data available.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	edu/ling/jurafsky/ws97/.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
"Conversational feedback mostly performedthrough short utterances yeah, mh, okaynot produced main speaker one ofthe participants conversation.</S><S sid =""2"" ssid = ""2"">Such utterances among frequent conversational data (Stolcke et al., 2000)."	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Email: stolcke@speech.sri.com.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DECLARATIVE-QUESTION You're a, you're senior now.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION Oh, good you.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B BACKCHANNEL Yeah.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	SIGNAL-NoN-UNDERSTANDING say?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	B STATEMENT N C State.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA represents meaning utterance level illocutionary force (Austin 1962).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 1 shows sample kind discourse structure interested.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	cases, DA labels would enrich available input higher-level processing spoken words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Another important role DA information could feedback lower-level processing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2 42 dialogue act labels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA frequencies given percentages total number utterances overall corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	36% Uh-huh.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	19% think it's great 13% So, -/ 6% That's exactly it.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5% imagine.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2% special training?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2% <Laughter>, < Throat_clearing> 2% Yes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% Well, it's nice talking you.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% wear work today?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% No. 1% Oh, okay.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% don't know I'm making sense not.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% afford get house?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% Well give break, know.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.5% is. .4% don't go first .4% aren't contributing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.2% I'm drawing blank.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.3% Well, .2% Uh, whole lot.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Excuse me? .1% don't know .1% you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% company?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Well, much that.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% goodness, Diane, get there.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	.1% Something like <.1% Right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% kind buff?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% I'm sorry.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	so, pull together previous approaches well new ideas.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results, besides validating methods described, interest several reasons.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	keep presentation interesting concrete, alternate description general methods empirical results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 2 describes task data detail.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 5 describe experiments DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Section 6 shows DA models used benefit speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prior related work summarized Section 7.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.1 Utterance Segmentation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	refer units segmentation utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance unit identified one DA, annotated single DA label.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.2 Tag Set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	began DAMSL markup system, modified several ways make relevant corpus task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	different tasks, techniques used study might prove useful others could greater importance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2 shows 42 categories examples relative frequencies.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2.3 Major Dialogue Act Types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	frequent DA types briefly characterized below.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Statements Opinions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	common types utterances STATEMENTS OPINIONS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	STATEMENT He's five months old OPINION Well, rabbits darling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	OPINION think would kind stressful.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	OPINIONS often include hedges think, believe, seems, mean.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Questions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Questions several types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	See Weber (1993) survey declarative questions various realizations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YEs-No-QUESTION doesn't eliminate it, it?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DECLARATIVE- QUESTION you're taking government course?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	WH-QUESTION Well, old you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Backchannels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	APPRECIATION Urn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Sounds good.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn Exits Abandoned Utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Abandoned utterances speaker breaks without finishing, followed restart.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Turn exits tend single words, often or.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, also coded NO-ANSWERS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Answers also semantically significant since likely contain new information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	common AGREEMENT/AccEPTS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	often yes yeah, look lot like ANSWERS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe mathematical computational framework used study.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Speaker labels introduced Section 4.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	likelihood usually much straightforward model posterior itself.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.1 Dialogue Act Likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	computation likelihoods P(EIU ) depends types evidence used.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	described later, involves considering multiple alternative recognized word sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ease reference, random variables used summarized Table 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	variables used subscripts refer individual utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, Wi word transcription ith utterance within conversation (not ith word).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	means likelihood given complete conversation factored likelihoods given individual utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Ui,..., Un), n number utterances conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.2 Markov Modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Ui1) (3) (k order Markov process describing U).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	n-gram-based discourse grammars used property.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3.3 Dialogue Act Decoding.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, used method throughout.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Obviously, possible off-line processing, full conversation available.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	HMM terminology chosen mainly historical reasons..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 5 Perplexities DAs without turn information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, incremental gain trigram small, higher-order models prove useful.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4.2 Discourse Models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Clearly, dialogue grammar adaptation needs research.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	structured tasks situation might different.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, found exploitable structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, present results combination knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4 5.1 Dialogue Act Classification Using Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA classification using words based observation different DAs use distinctive word strings.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, find distinctive correlations certain phrases DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.1.1 Classification True Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	4 frequency STATEMENTS across labeled data slightly different, cf.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 2..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Ui ~ ....	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.1.2 Classification Recognized Words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thorough use recognized speech derived follows.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note another approximation modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, different DAs common words may realized different word pronunciations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	According rationale, appropriate divideall score components ),.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also compared n-best DA classification approach straightforward 1-best approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2 Dialogue Act Classification Using Prosody.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosody important DA recognition two reasons.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	First, saw earlier, word-based classification suffers recognition errors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Second, utterances inherently ambiguous based words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also touch briefly alternative machine learning models prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.1 Prosodic Features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	node labeled majority class node, well posterior probabilities two classes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	case utterance duration, measure correlates length words overall speaking rate.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	appropriate, included raw features values normalized utterance and/or conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.2 Prosodic Decision Trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chose second approach, downsampling training data equate DA proportions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.3 Results Decision Trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	purpose model integration, likelihoods class assigned DA types comprised class.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 6).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.4 Neural Network Classifiers.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Neural networks worth investigating since offer potential advantages decision trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results summarized Table 8, along baseline result obtained decision tree model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.2.5 Intonation Event Likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.3 Using Multiple Knowledge Sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	mentioned earlier, expect improved performance combining word prosodic information.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	One important respect independence assumption violated modeling utterance length.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	need allow fact models combined Equation 8 give estimates differing qualities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	reported results aggregate outcome two test set halves.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 9 Combined utterance classification accuracies (chance = 35%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	first two columns correspond Tables 7 6, respectively.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Results summarized Table 9.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	5.3.2 Focused Classifications.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Chance performance experiments therefore 50%.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Results summarized Table 10.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown, combined classifier consistently accurate classifier using words alone.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	latter leveraged accurate speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.1 Integrating DA Modeling ASR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	8 7 Note similarity Equations 10 1.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	identical except fact now.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	probabilities, extremely important practice.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach explained detail footnote 5 applies well.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	call mixture-of-LMs approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.2 Computational Structure Mixture Modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	6.3 Experiments Results.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thus approximation mixture approaches top DA considered.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-specific LMs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	VERBMOBIL corpus consists two-party scheduling dialogues.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 13 shows commonly used versions tag sets three tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	discussed earlier, domains differ Switchboard corpus task-oriented.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	tag sets also generally smaller, problems balance occur.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	results summarized table also illustrate differences inherent difficulty tasks.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	VERBMOBIL.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples Jekat et al. (1995).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"12 DAs ""move types"" used Map Task."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples Taylor et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	CHECK going Indian Country?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	QUERY-YN got graveyard written ? QUERY-W where?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	REPLY-N No, don't REPLY-W {And across to?}	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	pyramid.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	READY Okay ATR.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Examples English translations given Nagata (1992).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Word-based posterior probability estimators also possible, although less common.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	work mentioned far uses statistical models various kinds.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach presented differ various earlier models, particularly based HMMs?	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	made number significant simplifications arrive computationally statistically tractable formulation.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	research within framework characterized simplifications addressed.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	addition, model nonlocal aspects discourse structure, despite negative results far.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	study automatically extracted prosodic features DA modeling likewise infancy.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	reason skewed distribution nature task (or lack thereof) Switchboard.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	remains seen fine-grained DA distinctions made reliably corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Hand-labeled DA categories may still serve important role initializing algorithm.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Despite shortcomings, Switchboard domain could serve purpose.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	grammar.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	components model automatically trained, thus applicable domains labeled data available.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	edu/ling/jurafsky/ws97/.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
Previous research leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) facial expressions (Boyer et al., 2011) automatic dialogue act classification, types nonverbal cues remain unexplored.	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Email: stolcke@speech.sri.com.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DECLARATIVE-QUESTION You're a, you're senior now.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	APPRECIATION Oh, good you.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	B BACKCHANNEL Yeah.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	SIGNAL-NoN-UNDERSTANDING say?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	B STATEMENT N C State.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA represents meaning utterance level illocutionary force (Austin 1962).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 1 shows sample kind discourse structure interested.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	cases, DA labels would enrich available input higher-level processing spoken words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Another important role DA information could feedback lower-level processing.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 2 42 dialogue act labels.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA frequencies given percentages total number utterances overall corpus.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	36% Uh-huh.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	19% think it's great 13% So, -/ 6% That's exactly it.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5% imagine.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2% special training?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2% <Laughter>, < Throat_clearing> 2% Yes.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% Well, it's nice talking you.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% wear work today?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% No. 1% Oh, okay.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% don't know I'm making sense not.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% afford get house?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% Well give break, know.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% right?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.5% is. .4% don't go first .4% aren't contributing.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.2% I'm drawing blank.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.3% Well, .2% Uh, whole lot.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% Excuse me? .1% don't know .1% you?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% company?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% Well, much that.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% goodness, Diane, get there.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	.1% Something like <.1% Right?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	<.1% kind buff?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	<.1% I'm sorry.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	so, pull together previous approaches well new ideas.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	results, besides validating methods described, interest several reasons.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	keep presentation interesting concrete, alternate description general methods empirical results.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Section 2 describes task data detail.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Section 5 describe experiments DA classification.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Section 6 shows DA models used benefit speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prior related work summarized Section 7.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2.1 Utterance Segmentation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	refer units segmentation utterances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	utterance unit identified one DA, annotated single DA label.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2.2 Tag Set.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	began DAMSL markup system, modified several ways make relevant corpus task.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	different tasks, techniques used study might prove useful others could greater importance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 2 shows 42 categories examples relative frequencies.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2.3 Major Dialogue Act Types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	frequent DA types briefly characterized below.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Statements Opinions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	common types utterances STATEMENTS OPINIONS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	STATEMENT He's five months old OPINION Well, rabbits darling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	OPINION think would kind stressful.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	OPINIONS often include hedges think, believe, seems, mean.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Questions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Questions several types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	See Weber (1993) survey declarative questions various realizations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	YEs-No-QUESTION doesn't eliminate it, it?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DECLARATIVE- QUESTION you're taking government course?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	WH-QUESTION Well, old you?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Backchannels.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	APPRECIATION Urn.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Sounds good.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Turn Exits Abandoned Utterances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Abandoned utterances speaker breaks without finishing, followed restart.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Turn exits tend single words, often or.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Similarly, also coded NO-ANSWERS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Answers also semantically significant since likely contain new information.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	common AGREEMENT/AccEPTS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	often yes yeah, look lot like ANSWERS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	describe mathematical computational framework used study.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	(Speaker labels introduced Section 4.)	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	likelihood usually much straightforward model posterior itself.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	3.1 Dialogue Act Likelihoods.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	computation likelihoods P(EIU ) depends types evidence used.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	described later, involves considering multiple alternative recognized word sequences.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ease reference, random variables used summarized Table 4.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	variables used subscripts refer individual utterances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, Wi word transcription ith utterance within conversation (not ith word).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	means likelihood given complete conversation factored likelihoods given individual utterances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Ui,..., Un), n number utterances conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	3.2 Markov Modeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Ui1) (3) (k order Markov process describing U).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	n-gram-based discourse grammars used property.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	3.3 Dialogue Act Decoding.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Therefore, used method throughout.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Obviously, possible off-line processing, full conversation available.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	HMM terminology chosen mainly historical reasons..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 5 Perplexities DAs without turn information.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, incremental gain trigram small, higher-order models prove useful.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	4.2 Discourse Models.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Clearly, dialogue grammar adaptation needs research.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	structured tasks situation might different.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, found exploitable structure.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally, present results combination knowledge sources.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	4 5.1 Dialogue Act Classification Using Words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA classification using words based observation different DAs use distinctive word strings.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Similarly, find distinctive correlations certain phrases DA types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.1.1 Classification True Words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	4 frequency STATEMENTS across labeled data slightly different, cf.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 2..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	~ Ui ~ ....	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.1.2 Classification Recognized Words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	thorough use recognized speech derived follows.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Note another approximation modeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, different DAs common words may realized different word pronunciations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	According rationale, appropriate divideall score components ),.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also compared n-best DA classification approach straightforward 1-best approach.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2 Dialogue Act Classification Using Prosody.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prosody important DA recognition two reasons.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	First, saw earlier, word-based classification suffers recognition errors.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Second, utterances inherently ambiguous based words alone.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also touch briefly alternative machine learning models prosodic features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2.1 Prosodic Features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	node labeled majority class node, well posterior probabilities two classes.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	case utterance duration, measure correlates length words overall speaking rate.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	appropriate, included raw features values normalized utterance and/or conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2.2 Prosodic Decision Trees.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	chose second approach, downsampling training data equate DA proportions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2.3 Results Decision Trees.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	purpose model integration, likelihoods class assigned DA types comprised class.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 6).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2.4 Neural Network Classifiers.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Neural networks worth investigating since offer potential advantages decision trees.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	results summarized Table 8, along baseline result obtained decision tree model.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.2.5 Intonation Event Likelihoods.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.3 Using Multiple Knowledge Sources.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	mentioned earlier, expect improved performance combining word prosodic information.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	One important respect independence assumption violated modeling utterance length.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	need allow fact models combined Equation 8 give estimates differing qualities.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	reported results aggregate outcome two test set halves.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 9 Combined utterance classification accuracies (chance = 35%).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	first two columns correspond Tables 7 6, respectively.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Results summarized Table 9.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	5.3.2 Focused Classifications.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Chance performance experiments therefore 50%.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Results summarized Table 10.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	shown, combined classifier consistently accurate classifier using words alone.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	latter leveraged accurate speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	6.1 Integrating DA Modeling ASR.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	8 7 Note similarity Equations 10 1.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	identical except fact now.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	probabilities, extremely important practice.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach explained detail footnote 5 applies well.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	call mixture-of-LMs approach.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	6.2 Computational Structure Mixture Modeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	6.3 Experiments Results.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	thus approximation mixture approaches top DA considered.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA-specific LMs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	VERBMOBIL corpus consists two-party scheduling dialogues.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 13 shows commonly used versions tag sets three tasks.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	discussed earlier, domains differ Switchboard corpus task-oriented.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	tag sets also generally smaller, problems balance occur.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	results summarized table also illustrate differences inherent difficulty tasks.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	VERBMOBIL.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Examples Jekat et al. (1995).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"12 DAs ""move types"" used Map Task."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Examples Taylor et al. (1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	CHECK going Indian Country?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	QUERY-YN got graveyard written ? QUERY-W where?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	REPLY-N No, don't REPLY-W {And across to?}	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	pyramid.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	READY Okay ATR.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Examples English translations given Nagata (1992).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Word-based posterior probability estimators also possible, although less common.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	work mentioned far uses statistical models various kinds.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach presented differ various earlier models, particularly based HMMs?	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	made number significant simplifications arrive computationally statistically tractable formulation.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	research within framework characterized simplifications addressed.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	addition, model nonlocal aspects discourse structure, despite negative results far.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	study automatically extracted prosodic features DA modeling likewise infancy.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	reason skewed distribution nature task (or lack thereof) Switchboard.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	remains seen fine-grained DA distinctions made reliably corpus.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Hand-labeled DA categories may still serve important role initializing algorithm.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Despite shortcomings, Switchboard domain could serve purpose.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	grammar.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	components model automatically trained, thus applicable domains labeled data available.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	edu/ling/jurafsky/ws97/.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
"Dialog act (DA) annotations tagging, inspiredby speech act theory Austin (1975) Searle(1976), used NLP community understand model dialog.</S><S sid =""18"" ssid = ""2"">Initial work done onspoken interactions (see example (Stolcke et al.,2000))."	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Email: stolcke@speech.sri.com.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DECLARATIVE-QUESTION You're a, you're senior now.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	APPRECIATION Oh, good you.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	B BACKCHANNEL Yeah.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	SIGNAL-NoN-UNDERSTANDING say?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	B STATEMENT N C State.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA represents meaning utterance level illocutionary force (Austin 1962).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 1 shows sample kind discourse structure interested.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	cases, DA labels would enrich available input higher-level processing spoken words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Another important role DA information could feedback lower-level processing.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 2 42 dialogue act labels.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA frequencies given percentages total number utterances overall corpus.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	36% Uh-huh.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	19% think it's great 13% So, -/ 6% That's exactly it.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5% imagine.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2% special training?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2% <Laughter>, < Throat_clearing> 2% Yes.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% Well, it's nice talking you.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% wear work today?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% No. 1% Oh, okay.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% don't know I'm making sense not.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% afford get house?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% Well give break, know.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% right?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.5% is. .4% don't go first .4% aren't contributing.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.2% I'm drawing blank.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.3% Well, .2% Uh, whole lot.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% Excuse me? .1% don't know .1% you?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% company?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% Well, much that.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% goodness, Diane, get there.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	.1% Something like <.1% Right?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	<.1% kind buff?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	<.1% I'm sorry.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	so, pull together previous approaches well new ideas.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	results, besides validating methods described, interest several reasons.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	keep presentation interesting concrete, alternate description general methods empirical results.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Section 2 describes task data detail.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Section 5 describe experiments DA classification.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Section 6 shows DA models used benefit speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prior related work summarized Section 7.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2.1 Utterance Segmentation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	refer units segmentation utterances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	utterance unit identified one DA, annotated single DA label.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2.2 Tag Set.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	began DAMSL markup system, modified several ways make relevant corpus task.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	different tasks, techniques used study might prove useful others could greater importance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 2 shows 42 categories examples relative frequencies.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2.3 Major Dialogue Act Types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	frequent DA types briefly characterized below.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Statements Opinions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	common types utterances STATEMENTS OPINIONS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	STATEMENT He's five months old OPINION Well, rabbits darling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	OPINION think would kind stressful.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	OPINIONS often include hedges think, believe, seems, mean.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Questions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Questions several types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	See Weber (1993) survey declarative questions various realizations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	YEs-No-QUESTION doesn't eliminate it, it?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DECLARATIVE- QUESTION you're taking government course?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	WH-QUESTION Well, old you?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Backchannels.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	APPRECIATION Urn.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Sounds good.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Turn Exits Abandoned Utterances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Abandoned utterances speaker breaks without finishing, followed restart.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Turn exits tend single words, often or.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Similarly, also coded NO-ANSWERS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Answers also semantically significant since likely contain new information.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	common AGREEMENT/AccEPTS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	often yes yeah, look lot like ANSWERS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	describe mathematical computational framework used study.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	(Speaker labels introduced Section 4.)	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	likelihood usually much straightforward model posterior itself.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	3.1 Dialogue Act Likelihoods.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	computation likelihoods P(EIU ) depends types evidence used.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	described later, involves considering multiple alternative recognized word sequences.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ease reference, random variables used summarized Table 4.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	variables used subscripts refer individual utterances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, Wi word transcription ith utterance within conversation (not ith word).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	means likelihood given complete conversation factored likelihoods given individual utterances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Ui,..., Un), n number utterances conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	3.2 Markov Modeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Ui1) (3) (k order Markov process describing U).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	n-gram-based discourse grammars used property.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	3.3 Dialogue Act Decoding.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Therefore, used method throughout.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Obviously, possible off-line processing, full conversation available.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	HMM terminology chosen mainly historical reasons..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 5 Perplexities DAs without turn information.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, incremental gain trigram small, higher-order models prove useful.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	4.2 Discourse Models.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Clearly, dialogue grammar adaptation needs research.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	structured tasks situation might different.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, found exploitable structure.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally, present results combination knowledge sources.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	4 5.1 Dialogue Act Classification Using Words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA classification using words based observation different DAs use distinctive word strings.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Similarly, find distinctive correlations certain phrases DA types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.1.1 Classification True Words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	4 frequency STATEMENTS across labeled data slightly different, cf.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 2..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	~ Ui ~ ....	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.1.2 Classification Recognized Words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	thorough use recognized speech derived follows.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Note another approximation modeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, different DAs common words may realized different word pronunciations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	According rationale, appropriate divideall score components ),.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also compared n-best DA classification approach straightforward 1-best approach.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2 Dialogue Act Classification Using Prosody.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prosody important DA recognition two reasons.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	First, saw earlier, word-based classification suffers recognition errors.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Second, utterances inherently ambiguous based words alone.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also touch briefly alternative machine learning models prosodic features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2.1 Prosodic Features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	node labeled majority class node, well posterior probabilities two classes.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	case utterance duration, measure correlates length words overall speaking rate.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	appropriate, included raw features values normalized utterance and/or conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2.2 Prosodic Decision Trees.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	chose second approach, downsampling training data equate DA proportions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2.3 Results Decision Trees.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	purpose model integration, likelihoods class assigned DA types comprised class.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 6).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2.4 Neural Network Classifiers.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Neural networks worth investigating since offer potential advantages decision trees.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	results summarized Table 8, along baseline result obtained decision tree model.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.2.5 Intonation Event Likelihoods.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.3 Using Multiple Knowledge Sources.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	mentioned earlier, expect improved performance combining word prosodic information.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	One important respect independence assumption violated modeling utterance length.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	need allow fact models combined Equation 8 give estimates differing qualities.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	reported results aggregate outcome two test set halves.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 9 Combined utterance classification accuracies (chance = 35%).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	first two columns correspond Tables 7 6, respectively.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Results summarized Table 9.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	5.3.2 Focused Classifications.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Chance performance experiments therefore 50%.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Results summarized Table 10.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	shown, combined classifier consistently accurate classifier using words alone.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	latter leveraged accurate speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	6.1 Integrating DA Modeling ASR.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	8 7 Note similarity Equations 10 1.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	identical except fact now.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	probabilities, extremely important practice.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach explained detail footnote 5 applies well.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	call mixture-of-LMs approach.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	6.2 Computational Structure Mixture Modeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	6.3 Experiments Results.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	thus approximation mixture approaches top DA considered.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA-specific LMs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	VERBMOBIL corpus consists two-party scheduling dialogues.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 13 shows commonly used versions tag sets three tasks.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	discussed earlier, domains differ Switchboard corpus task-oriented.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	tag sets also generally smaller, problems balance occur.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	results summarized table also illustrate differences inherent difficulty tasks.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	VERBMOBIL.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Examples Jekat et al. (1995).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"12 DAs ""move types"" used Map Task."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Examples Taylor et al. (1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	CHECK going Indian Country?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	QUERY-YN got graveyard written ? QUERY-W where?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	REPLY-N No, don't REPLY-W {And across to?}	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	pyramid.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	READY Okay ATR.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Examples English translations given Nagata (1992).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Word-based posterior probability estimators also possible, although less common.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	work mentioned far uses statistical models various kinds.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach presented differ various earlier models, particularly based HMMs?	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	made number significant simplifications arrive computationally statistically tractable formulation.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	research within framework characterized simplifications addressed.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	addition, model nonlocal aspects discourse structure, despite negative results far.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	study automatically extracted prosodic features DA modeling likewise infancy.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	reason skewed distribution nature task (or lack thereof) Switchboard.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	remains seen fine-grained DA distinctions made reliably corpus.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Hand-labeled DA categories may still serve important role initializing algorithm.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Despite shortcomings, Switchboard domain could serve purpose.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	grammar.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	components model automatically trained, thus applicable domains labeled data available.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	edu/ling/jurafsky/ws97/.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
"dialogue acts statements, questions, backchannels, ... detected using language model based detecÂ­tor trained Switchboard similar Stolcke et al.</S><S sid =""96"" ssid = ""15"">(2000"	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Email: stolcke@speech.sri.com.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DECLARATIVE-QUESTION You're a, you're senior now.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	APPRECIATION Oh, good you.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	B BACKCHANNEL Yeah.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	SIGNAL-NoN-UNDERSTANDING say?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	B STATEMENT N C State.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA represents meaning utterance level illocutionary force (Austin 1962).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 1 shows sample kind discourse structure interested.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	cases, DA labels would enrich available input higher-level processing spoken words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Another important role DA information could feedback lower-level processing.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 2 42 dialogue act labels.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA frequencies given percentages total number utterances overall corpus.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	36% Uh-huh.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	19% think it's great 13% So, -/ 6% That's exactly it.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5% imagine.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2% special training?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2% <Laughter>, < Throat_clearing> 2% Yes.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% Well, it's nice talking you.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% wear work today?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% No. 1% Oh, okay.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% don't know I'm making sense not.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% afford get house?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% Well give break, know.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% right?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.5% is. .4% don't go first .4% aren't contributing.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.2% I'm drawing blank.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.3% Well, .2% Uh, whole lot.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% Excuse me? .1% don't know .1% you?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% company?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% Well, much that.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% goodness, Diane, get there.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	.1% Something like <.1% Right?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	<.1% kind buff?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	<.1% I'm sorry.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	so, pull together previous approaches well new ideas.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	results, besides validating methods described, interest several reasons.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	keep presentation interesting concrete, alternate description general methods empirical results.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Section 2 describes task data detail.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Section 5 describe experiments DA classification.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Section 6 shows DA models used benefit speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prior related work summarized Section 7.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2.1 Utterance Segmentation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	refer units segmentation utterances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	utterance unit identified one DA, annotated single DA label.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2.2 Tag Set.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	began DAMSL markup system, modified several ways make relevant corpus task.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	different tasks, techniques used study might prove useful others could greater importance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 2 shows 42 categories examples relative frequencies.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2.3 Major Dialogue Act Types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	frequent DA types briefly characterized below.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Statements Opinions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	common types utterances STATEMENTS OPINIONS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	STATEMENT He's five months old OPINION Well, rabbits darling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	OPINION think would kind stressful.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	OPINIONS often include hedges think, believe, seems, mean.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Questions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Questions several types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	See Weber (1993) survey declarative questions various realizations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	YEs-No-QUESTION doesn't eliminate it, it?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DECLARATIVE- QUESTION you're taking government course?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	WH-QUESTION Well, old you?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Backchannels.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	APPRECIATION Urn.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Sounds good.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Turn Exits Abandoned Utterances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Abandoned utterances speaker breaks without finishing, followed restart.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Turn exits tend single words, often or.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Similarly, also coded NO-ANSWERS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Answers also semantically significant since likely contain new information.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	common AGREEMENT/AccEPTS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	often yes yeah, look lot like ANSWERS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	describe mathematical computational framework used study.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	(Speaker labels introduced Section 4.)	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	likelihood usually much straightforward model posterior itself.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	3.1 Dialogue Act Likelihoods.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	computation likelihoods P(EIU ) depends types evidence used.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	described later, involves considering multiple alternative recognized word sequences.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ease reference, random variables used summarized Table 4.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	variables used subscripts refer individual utterances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, Wi word transcription ith utterance within conversation (not ith word).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	means likelihood given complete conversation factored likelihoods given individual utterances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Ui,..., Un), n number utterances conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	3.2 Markov Modeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Ui1) (3) (k order Markov process describing U).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	n-gram-based discourse grammars used property.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	3.3 Dialogue Act Decoding.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Therefore, used method throughout.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Obviously, possible off-line processing, full conversation available.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	HMM terminology chosen mainly historical reasons..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 5 Perplexities DAs without turn information.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, incremental gain trigram small, higher-order models prove useful.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	4.2 Discourse Models.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Clearly, dialogue grammar adaptation needs research.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	structured tasks situation might different.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, found exploitable structure.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally, present results combination knowledge sources.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	4 5.1 Dialogue Act Classification Using Words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA classification using words based observation different DAs use distinctive word strings.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Similarly, find distinctive correlations certain phrases DA types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.1.1 Classification True Words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	4 frequency STATEMENTS across labeled data slightly different, cf.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 2..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	~ Ui ~ ....	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.1.2 Classification Recognized Words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	thorough use recognized speech derived follows.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Note another approximation modeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, different DAs common words may realized different word pronunciations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	According rationale, appropriate divideall score components ),.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also compared n-best DA classification approach straightforward 1-best approach.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2 Dialogue Act Classification Using Prosody.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prosody important DA recognition two reasons.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	First, saw earlier, word-based classification suffers recognition errors.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Second, utterances inherently ambiguous based words alone.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also touch briefly alternative machine learning models prosodic features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2.1 Prosodic Features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	node labeled majority class node, well posterior probabilities two classes.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	case utterance duration, measure correlates length words overall speaking rate.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	appropriate, included raw features values normalized utterance and/or conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2.2 Prosodic Decision Trees.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	chose second approach, downsampling training data equate DA proportions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2.3 Results Decision Trees.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	purpose model integration, likelihoods class assigned DA types comprised class.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 6).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2.4 Neural Network Classifiers.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Neural networks worth investigating since offer potential advantages decision trees.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	results summarized Table 8, along baseline result obtained decision tree model.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.2.5 Intonation Event Likelihoods.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.3 Using Multiple Knowledge Sources.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	mentioned earlier, expect improved performance combining word prosodic information.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	One important respect independence assumption violated modeling utterance length.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	need allow fact models combined Equation 8 give estimates differing qualities.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	reported results aggregate outcome two test set halves.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 9 Combined utterance classification accuracies (chance = 35%).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	first two columns correspond Tables 7 6, respectively.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Results summarized Table 9.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	5.3.2 Focused Classifications.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Chance performance experiments therefore 50%.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Results summarized Table 10.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	shown, combined classifier consistently accurate classifier using words alone.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	latter leveraged accurate speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	6.1 Integrating DA Modeling ASR.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	8 7 Note similarity Equations 10 1.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	identical except fact now.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	probabilities, extremely important practice.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach explained detail footnote 5 applies well.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	call mixture-of-LMs approach.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	6.2 Computational Structure Mixture Modeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	6.3 Experiments Results.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	thus approximation mixture approaches top DA considered.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA-specific LMs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	VERBMOBIL corpus consists two-party scheduling dialogues.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 13 shows commonly used versions tag sets three tasks.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	discussed earlier, domains differ Switchboard corpus task-oriented.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	tag sets also generally smaller, problems balance occur.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	results summarized table also illustrate differences inherent difficulty tasks.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	VERBMOBIL.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Examples Jekat et al. (1995).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"12 DAs ""move types"" used Map Task."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Examples Taylor et al. (1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	CHECK going Indian Country?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	QUERY-YN got graveyard written ? QUERY-W where?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	REPLY-N No, don't REPLY-W {And across to?}	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	pyramid.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	READY Okay ATR.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Examples English translations given Nagata (1992).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Word-based posterior probability estimators also possible, although less common.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	work mentioned far uses statistical models various kinds.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach presented differ various earlier models, particularly based HMMs?	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	made number significant simplifications arrive computationally statistically tractable formulation.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	research within framework characterized simplifications addressed.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	addition, model nonlocal aspects discourse structure, despite negative results far.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	study automatically extracted prosodic features DA modeling likewise infancy.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	reason skewed distribution nature task (or lack thereof) Switchboard.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	remains seen fine-grained DA distinctions made reliably corpus.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Hand-labeled DA categories may still serve important role initializing algorithm.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Despite shortcomings, Switchboard domain could serve purpose.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	grammar.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	components model automatically trained, thus applicable domains labeled data available.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	edu/ling/jurafsky/ws97/.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
"HMM widely used many tagging problems.</S><S sid =""65"" ssid = ""19"">Stolcke et al.</S><S sid =""66"" ssid = ""20"">(Stolcke et al., 2000) used dialog act classification, utterance (or dialog act) used observation."	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Dialogue Act Modeling Automatic Tagging Recognition Conversational Speech	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	describe statistical approach modeling dialogue acts conversational speech, i.e., speech- act-like units STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, APOLOGY.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	model detects predicts dialogue acts based lexical, collocational, prosodic cues, well discourse coherence dialogue act sequence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	dialogue model based treating discourse structure conversation hidden Markov model individual dialogue acts observations emanating model states.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Constraints likely sequence dialogue acts modeled via dialogue act n-gram.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	statistical dialogue grammar combined word n-grams, decision trees, neural networks modeling idiosyncratic lexical prosodic manifestations dialogue act.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	develop probabilistic integration speech recognition dialogue modeling, improve speech recognition dialogue act classification accuracy.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Models trained evaluated using large hand-labeled database 1,155 conversations Switchboard corpus spontaneous human-to-human telephone speech.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	achieved good dialogue act labeling accuracy (65% based errorful, automatically recognized words prosody, 71% based word transcripts, compared chance baseline accuracy 35% human accuracy 84%) small reduction word recognition error.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Â• Speech Technology Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Email: stolcke@speech.sri.com.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	@ 2000 Association Computational Linguistics Table 1 Fragment labeled conversation (from Switchboard corpus).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Speaker Dialogue Act Utterance YEs-No-QuESTION go college right now?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ABANDONED yo-, B YES- ANSWER Yeah, B STATEMENT it's last year [laughter].	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DECLARATIVE-QUESTION You're a, you're senior now.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	B YEs-ANSWER Yeah, B STATEMENT I'm working projects trying graduate [laughter].	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	APPRECIATION Oh, good you.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	B BACKCHANNEL Yeah.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	APPRECIATION That's great, YEs-No-QUESTION um, is, N C University that, uh, State, B STATEMENT N C State.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	SIGNAL-NoN-UNDERSTANDING say?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	B STATEMENT N C State.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ability model automatically detect discourse structure important step toward understanding spontaneous dialogue.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	hardly consensus exactly discourse structure described, agreement exists useful first level analysis involves identification dialogue acts (DAs).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA represents meaning utterance level illocutionary force (Austin 1962).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thus, DA approximately equivalent speech act Searle (1969), conversational game move Power (1979), adjacency pair part Schegloff (1968) Saks, Schegloff, Jefferson (1974).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 1 shows sample kind discourse structure interested.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	utterance assigned unique DA label (shown column 2), drawn well-defined set (shown Table 2).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thus, DAs thought tag set classifies utterances according combination pragmatic, semantic, syntactic criteria.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	computational community usually defined DA categories relevant particular application, although efforts way develop DA labeling systems domain-independent, Discourse Resource Initiative's DAMSL architecture (Core Allen 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	constituting dialogue understanding deep sense, DA tagging seems clearly useful range applications.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, meeting summarizer needs keep track said whom, conversational agent needs know whether asked question ordered something.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	related work DAs used first processing step infer dialogue games (Carlson 1983; Levin Moore 1977; Levin et al. 1999), slightly higher level unit comprises small number DAs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Interactional dominance (Linell 1990) might measured accurately using DA distributions simpler techniques, could serve indicator type genre discourse hand.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	cases, DA labels would enrich available input higher-level processing spoken words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Another important role DA information could feedback lower-level processing.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, speech recognizer could constrained expectations likely DAs given context, constraining potential recognition hypotheses improve accuracy.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 2 42 dialogue act labels.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA frequencies given percentages total number utterances overall corpus.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm legal department.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	36% Uh-huh.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	19% think it's great 13% So, -/ 6% That's exactly it.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5% imagine.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2% special training?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2% <Laughter>, < Throat_clearing> 2% Yes.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% Well, it's nice talking you.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% wear work today?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% No. 1% Oh, okay.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% don't know I'm making sense not.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% afford get house?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% Well give break, know.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% right?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1% can't pregnant cats .5% Oh, mean switched schools kids.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.5% is. .4% don't go first .4% aren't contributing.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.4% Oh, fajitas .3% ? .3% would steal newspaper?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.2% I'm drawing blank.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.3% Well, .2% Uh, whole lot.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% Excuse me? .1% don't know .1% you?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% company?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% Well, much that.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% goodness, Diane, get there.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% I'I1 check .1% What's word I'm looking .1% That's right.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	.1% Something like <.1% Right?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	<.1% kind buff?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	<.1% I'm sorry.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	<.1% Hey thanks lot <.1% goal article twofold: one hand, aim present comprehensive framework modeling automatic classification DAs, founded well-known statistical methods.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	so, pull together previous approaches well new ideas.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, model draws use DA n-grams hidden Markov models conversation present earlier work, Nagata Morimoto (1993, 1994) Woszczyna Waibel (1994) (see Section 7).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, framework generalizes earlier models, giving us clean probabilistic approach performing DA classification unreliable words nonlexical evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	speech recognition task, framework provides mathematically principled way condition speech recognizer conversation context dialogue structure, well nonlexical information correlated DA identity.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	present methods domain-independent framework part treats DA labels arbitrary formal tag set.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Throughout presentation, highlight simplifications assumptions made achieve tractable models, point might fall short reality.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Second, present results obtained approach large, widely available corpus spontaneous conversational speech.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	results, besides validating methods described, interest several reasons.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, unlike previous work DA labeling, corpus task-oriented nature, amount data used (198,000 utterances) exceeds previous studies least order magnitude (see Table 14).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	keep presentation interesting concrete, alternate description general methods empirical results.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Section 2 describes task data detail.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Section 3 presents probabilistic modeling framework; central component framework, discourse grammar, discussed Section 4.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Section 5 describe experiments DA classification.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Section 6 shows DA models used benefit speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prior related work summarized Section 7.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	issues open problems addressed Section 8, followed concluding remarks Section 9.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	domain chose model Switchboard corpus human-human conversational telephone speech (Godfrey, Holliman, McDaniel 1992) distributed Linguistic Data Consortium.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	conversation involved two randomly selected strangers charged talking informally one several, self- selected general-interest topics.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	train statistical models corpus, combined extensive effort human hand-coding DAs utterance, variety automatic semiautomatic tools.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	data consisted substantial portion Switchboard waveforms corresponding transcripts, totaling 1,155 conversations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2.1 Utterance Segmentation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	hand-labeling utterance corpus DA, needed choose utterance segmentation, raw Switchboard data segmented linguistically consistent way.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	expedite DA labeling task remain consistent Switchboard-based research efforts, made use version corpus hand-segmented sentence-level units prior work independently DA labeling system (Meteer et al. 1995).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	refer units segmentation utterances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	relation utterances speaker turns one-to-one: single turn contain multiple utterances, utterances span one turn (e.g., case backchanneling speaker midutterance).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	utterance unit identified one DA, annotated single DA label.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA labeling system special provisions rare cases utterances seemed combine aspects several DA types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Automatic segmentation spontaneous speech open research problem right (Mast et al. 1996; Stolcke Shriberg 1996).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	rough idea difficulty segmentation problem corpus using definition utterance units derived recent study (Shriberg et al. 2000).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	automatic labeling word boundaries either utterance nonboundaries using combination lexical prosodic cues, obtained 96% accuracy based correct word transcripts, 78% accuracy automatically recognized words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	fact segmentation labeling tasks interdependent (Warnke et al. 1997; Finke et al. 1998) complicates problem.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Based considerations, decided confound DA classification task additional problems introduced automatic segmentation assumed utterance-level segmentations given.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	important consequence decision expect utterance length acoustic properties utterance boundaries accurate, turn important features DAs (Shriberg et al. 1998; see also Section 5.2.1).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2.2 Tag Set.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	chose follow recent standard shallow discourse structure annotation, Dialog Act Markup Several Layers (DAMSL) tag set, designed natural language processing community auspices Discourse Resource Initiative (Core Allen 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	began DAMSL markup system, modified several ways make relevant corpus task.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DAMSL aims provide domain-independent framework dialogue annotation, reflected fact tag set mapped back DAMSL categories (Jurafsky, Shriberg, Biasca 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, labeling effort also showed content- task-related distinctions always play important role effective DA labeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"Switchboard domain essentially ""task-free,"" thus giving external constraints definition DA categories."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	primary purpose adapting tag set enable computational DA modeling conversational speech, possible improvements conversational speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	lack specific task, decided label categories seemed inherently interesting linguistically could identified reliably.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Also, focus conversational speech recognition led certain bias toward categories lexically syntactically distinct (recognition accuracy traditionally measured including lexical elements utterance).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	modeling techniques described paper formally independent corpus choice tag set, success particular task course crucially depend factors.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	different tasks, techniques used study might prove useful others could greater importance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, believe study represents fairly comprehensive application technology area serve point departure reference work.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	resulting SWBDDAMSL tag set multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could combined diacritics indicating orthogonal information, example, whether dialogue function utterance related Task-Management Communication-Management.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Approximately 220 many possible unique combinations codes used coders (Jurafsky, Shriberg, Biasca 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	obtain system somewhat higher interlabeler agreement, well enough data per class statistical modeling purposes, less fine-grained tag set devised.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tag set distinguishes 42 mutually exclusive utterance types used experiments reported here.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 2 shows 42 categories examples relative frequencies.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1 1 study focusing prosodic modeling DAs reported elsewhere (Shriberg et al. 1998), tag set reduced six categories..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	original infrequent classes collapsed, resulting DA type distribution still highly skewed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	occurs largely basis subdividing dominant DA categories according task-independent reliable criteria.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tag set incorporates traditional sociolinguistic discourse-theoretic notions, rhetorical relations adjacency pairs, well form- based labels.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Furthermore, tag set structured allow labelers annotate Switchboard conversation transcripts alone (i.e., without listening) 30 minutes.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Without constraints DA labels might included finer distinctions, felt drawback balanced ability cover large amount data.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2 Labeling carried three-month period 1997 eight linguistics graduate students CU Boulder.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Interlabeler agreement 421abel tag set used 84%, resulting Kappa statistic 0.80.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Kappa statistic measures agreement normalized chance (Siegel Castellan, Jr. 1988).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	argued Carletta (1996), Kappa values 0.8 higher desirable detecting associations several coded variables; thus satisfied level agreement achieved.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	(Note that, even though single variable, DA type, coded present study, goal is, among things, model associations several instances variable, e.g., adjacent DAs.)	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	total 1,155 Switchboard conversations labeled, comprising 205,000 utterances 1.4 million words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	data partitioned training set 1,115 conversations (1.4M words, 198K utterances), used estimating various components model, test set 19 conversations (29K words, 4K utterances).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Remaining conversations set aside future use (e.g., test set uncompromised tuning effects).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2.3 Major Dialogue Act Types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	frequent DA types briefly characterized below.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	discussed above, focus paper nature DAs, computational framework recognition; full details DA tag set numerous motivating examples found separate report (Jurafsky, Shriberg, Biasca 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Statements Opinions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	common types utterances STATEMENTS OPINIONS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"split distinguishes ""descriptive, narrative, personal"" statements (STATEMENT)from ""other-directed opinion statements"" (OPINION)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	distinction designed capture different kinds responses saw opinions (which often countered disagreed via opinions) statements (which often elicit continuers backchannels): Dialogue Act Example Utterance STATEMENT Well, cat, um, STATEMENT He's probably, oh, good two years old, big, old, fat sassy tabby.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	STATEMENT He's five months old OPINION Well, rabbits darling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	OPINION think would kind stressful.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	2 effect lacking acoustic information labeling accuracy assessed relabeling subset data listening, found fairly small (Shriberg et al. 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	conservative estimate based relabeling study that, DA types, 2% labels might changed based listening.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA types higher uncertainty BACKCHANNELS AGREEMENTS, easily confused without acoustic cues; rate change 10%..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	OPINIONS often include hedges think, believe, seems, mean.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	combined STATEMENT OPINION classes studies dimensions differ (Shriberg et al. 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Questions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Questions several types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	YES-No-QUESTION label includes utterances pragmatic force yes-no-question syntactic markings yes-no-question (i.e., subject-inversion sentence-final tags).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"DECLARATIVE- QUESTIONS utterances function pragmatically questions ""question form."""	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"mean declarative questions normally wh-word argument verb (except ""echo-question"" format), ""declarative"" word order subject precedes verb."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	See Weber (1993) survey declarative questions various realizations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Dialogue Act Example Utterance YEs-No-QUESTION special training?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	YEs-No-QUESTION doesn't eliminate it, it?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	YEs-No-QuESTION Uh, guess year ago you're probably watching C N N lot, right?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DECLARATIVE- QUESTION you're taking government course?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	WH-QUESTION Well, old you?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Backchannels.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	backchannel short utterance plays discourse-structuring roles, e.g., indicating speaker go talking.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"usually referred conversation analysis literature ""continuers"" studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	expect recognition backchannels useful discourse-structuring role (knowing hearer expects speaker go talking tells us something course narrative) seem occur certain kinds syntactic boundaries; detecting backchannel may thus help predicting utterance boundaries surrounding lexical material.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	intuition backchannels look like, Table 3 shows common realizations approximately 300 types (35,827 tokens) backchannel Switchboard subset.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	following table shows examples backchannels context Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're point financial income enough consider putting away - BACKCHANNEL Uh-huh.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	/ B STATEMENT -for college, / B STATEMENT going starting regular payroll deduction - BACKCHANNEL Urn.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	/ B STATEMENT -- fall / B STATEMENT money making summer we'll putting away college fund.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	APPRECIATION Urn.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Sounds good.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Turn Exits Abandoned Utterances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Abandoned utterances speaker breaks without finishing, followed restart.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Turn exits resemble abandoned utterances often syntactically broken off, used Table 3 common realizations backchannels Switchboard.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly way passing speakership speaker.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Turn exits tend single words, often or.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Speaker Dialogue Act Utterance STATEMENT we're from, uh, I'm Ohio / STATEMENT wife's Florida / TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ HEDGE so, don't know, / ABANDONED it's Klipsmack>, -/ STATEMENT I'm glad it's kind problem come answer it's - Answers Agreements.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	YES-ANSWERS include yes, yeah, yep, uh-huh, variations yes, acting answer YES-NO-QUESTION DECLARATWE-0UESTION.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Similarly, also coded NO-ANSWERS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Detecting ANSWERS help tell us previous utterance YES-NO-QUESTION.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Answers also semantically significant since likely contain new information.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	AGREEMENT/ACCEPT, REJECT, MAYBE/ACCEPT-PARTall mark degree speaker accepts previous proposal, plan, opinion, statement.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	common AGREEMENT/AccEPTS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	often yes yeah, look lot like ANSWERS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ANSWERS follow questions, AGREEMENTS often follow opinions proposals, distinguishing important discourse.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	describe mathematical computational framework used study.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	goal perform DA classification tasks using probabilistic formulation, giving us principled approach combining multiple knowledge sources (using laws probability), well ability derive model parameters automatically corpus, using statistical inference techniques.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Given available evidence E conversation, goal find DA sequence U highest posterior probability P(UIE ) given evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Applying Bayes' rule get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U P(U) represents prior probability DA sequence, P(EIU ) like- Table 4 Summary random variables used dialogue modeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	(Speaker labels introduced Section 4.)	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Symbol Meaning U sequence DA labels E evidence (complete speech signal) F prosodic evidence acoustic evidence (spectral features used ASR) W sequence words speakers labels lihood U given evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	likelihood usually much straightforward model posterior itself.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	fact models generative causal nature, i.e., describe evidence produced underlying DA sequence U. Estimating P (U) requires building probabilistic discourse grammar, i.e., statistical model DA sequences.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	done using familiar techniques language modeling speech recognition, although sequenced objects case DA labels rather words; discourse grammars discussed detail Section 4.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	3.1 Dialogue Act Likelihoods.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	computation likelihoods P(EIU ) depends types evidence used.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	experiments used following sources evidence, either alone combination: Transcribed words: likelihoods used Equation 1 P(WIU ), W refers true (hand-transcribed) words spoken conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Recognized words: evidence consists recognizer acoustics A, seek compute P(A U).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	described later, involves considering multiple alternative recognized word sequences.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prosodic features-Evidence given acoustic features F capturing various aspects pitch, duration, energy, etc., speech signal; associated likelihoods P(F U).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ease reference, random variables used summarized Table 4.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	variables used subscripts refer individual utterances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, Wi word transcription ith utterance within conversation (not ith word).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	make modeling search best DA sequence feasible, require likelihood models decomposable utterance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	means likelihood given complete conversation factored likelihoods given individual utterances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	use Ui ith DA label sequence U, i.e., U = (U1 .....	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Ui,..., Un), n number utterances conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	addition, use Ei portion evidence corresponds ith utterance, e.g., words prosody ith utterance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Decomposability likelihood means P(EIU) = P(E11 U1).....	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	P(En [Un) (2) Applied separately three types evidence Ai, Wi, Fi mentioned above, clear assumption strictly true.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, speakers tend reuse E1 Ei E. <start> , U1 , ...	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	~ Ui ) ...---* Un <end> Figure 1 discourse HMM Bayes network.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	words found earlier conversation (Fowler Housum 1987) answer might actually relevant question it, violating independence P(WilUi).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Similarly, speakers adjust pitch volume time, e.g., conversation partner structure discourse (Menn Boyce 1982), violating independence P(FilUi).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	areas statistical modeling, count fact violations small compared properties actually modeled, namely, dependence Ei Ui.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	3.2 Markov Modeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Returning prior distribution DA sequences P(U), convenient make certain independence assumptions here, too.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	particular, assume prior distribution U Markovian, i.e., Ui depends fixed number k preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k .....	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Ui1) (3) (k order Markov process describing U).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	n-gram-based discourse grammars used property.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	described later, k = 1 good choice, i.e., conditioning DA types one removed current one improve quality model much, least amount data available experiments.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	importance Markov assumption discourse grammar view whole system discourse grammar local utterance-based likelihoods kth-order hidden Markov model (HMM) (Rabiner Juang 1986).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	HMM states correspond DAs, observations correspond utterances, transition probabilities given discourse grammar (see Section 4), observation probabilities given local likelihoods P(Eil Ui).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	represent dependency structure (as well implied conditional independences) special case Bayesian belief network (Pearl 1988).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Figure 1 shows variables resulting HMM directed edges representing conditional dependence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	keep things simple, first-order HMM (bigram discourse grammar) assumed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	3.3 Dialogue Act Decoding.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	HMM representation allows us use efficient dynamic programming algorithms compute relevant aspects model, Â• probable DA sequence (the Viterbi algorithm) Â• posterior probability various DAs given utterance, considering evidence (the forward-backward algorithm) Viterbi algorithm HMMs (Viterbi 1967) finds globally probable state sequence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	applied discourse model locally decomposable likelihoods Markovian discourse grammar, therefore find precisely DA sequence highest posterior probability: U* = argmaxP(UIE ) (4) u combination likelihood prior modeling, HMMs, Viterbi decoding fundamentally standard probabilistic approaches speech recognition (Bahl, Jelinek, Mercer 1983) tagging (Church 1988).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	maximizes probability getting entire DA sequence correct, necessarily find DA sequence DA labels correct (Dermatas Kokkinakis 1995).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	minimize total number utterance labeling errors, need maximize probability getting DA label correct individually, i.e., need maximize P(UilE) = 1 ..... n. compute per-utterance posterior DA probabilities summing: P(u[E) = E P(UIE) (5) U: Ui=u summation sequences U whose ith element matches label question.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	summation efficiently carried forward-backward algorithm HMMs (Baum et al. 1970).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	3 zeroth-order (unigram) discourse grammars, Viterbi decoding forward- backward decoding necessarily yield results.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, higher-order discourse grammars found forward-backward decoding consistently gives slightly (up 1% absolute) better accuracies, expected.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Therefore, used method throughout.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	formulation presented here, well experiments, uses entire conversation evidence DA classification.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Obviously, possible off-line processing, full conversation available.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	paradigm thus follows historical practice Switchboard domain, goal typically off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) entire previously recorded conversations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, HMM formulation used also supports computing posterior DA probabilities based partial evidence, e.g., using utterances preceding current one, would required online processing.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	statistical discourse grammar models prior probabilities P(U) DA sequences.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	case conversations identities speakers known (as Switchboard), discourse grammar also model turn-taking behavior.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	straightforward approach model sequences pairs (Ui, Ti) Ui DA label Ti represents speaker.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	trying model speaker idiosyncrasies, conversants arbitrarily identified B, model made symmetric respect choice sides (e.g., replicating training sequences sides switched).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	discourse grammars thus vocabulary 42 x 2 = 84 labels, plus tags beginning end conversations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, second DA tag Table 1 would predicted trigram discourse grammar using fact speaker previously uttered YES-NO-QUESTION, turn preceded start-of-conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	3 note passing Viterbi Baum algorithms equivalent formulations Bayes network framework (Pearl 1988).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	HMM terminology chosen mainly historical reasons..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 5 Perplexities DAs without turn information.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models computationally convenient type discourse grammar n-gram model based DA tags, allows efficient decoding HMM framework.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	trained standard backoff n-gram models (Katz 1987), using frequency smoothing approach Witten Bell (1991).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Models various orders compared perplexities, i.e., average number choices model predicts tag, conditioned preceding tags.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 5 shows perplexities three types models: P(U), DAs alone; P(U, T), combined DA/speaker ID sequence; P(UIT ), DAs conditioned known speaker IDs (appropriate Switchboard task).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	expected, see improvement (decreasing perplexities) increasing n-gram order.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, incremental gain trigram small, higher-order models prove useful.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	(This observation, initially based perplexity, confirmed DA tagging experiments reported Section 5.)	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Comparing P(U) P(U[T),we see speaker identity adds substantial information, especially higher-order models.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	relatively small improvements higher-order models could result lack training data, inherent independence DAs DAs removed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	near-optimality bigram discourse grammar plausible given conversation analysis accounts discourse structure terms adjacency pairs (Schegloff 1968; Sacks, Schegloff, Jefferson 1974).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Inspection bigram probabilities estimated data revealed conventional adjacency pairs receive high probabilities, expected.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, 30% YES-NO-QUESTIONS followed YES-ANSWERS, 14% NO-ANSWERS (confirming latter dispreferred).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	COMMANDS followed AGREEMENTS 23% cases, STATEMENTS elicit BACKCHANNELS 26% cases.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	4.2 Discourse Models.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also investigated non-n-gram discourse models, based various language modeling techniques known speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	One motivation alternative models n-grams enforce one-dimensional representation DA sequences, whereas saw event space really multidimensional (DA label speaker labels).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Another motivation n-grams fail model long-distance dependencies, fact speakers may tend repeat certain DAs patterns throughout conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	first alternative approach standard cache model (Kuhn de Mori 1990), boosts probabilities previously observed unigrams bigrams, theory tokens tend repeat longer distances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, seem true DA sequences corpus, cache model showed improvement standard N-gram.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	result somewhat surprising since unigram dialogue grammars able detect speaker gender 63% accuracy (over 50% baseline) Switchboard (Ries 1999b), indicating global variables DA distribution could potentially exploited cache dialogue grammar.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Clearly, dialogue grammar adaptation needs research.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Second, built discourse grammar incorporated constraints DA sequences nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, Della Pietra 1996).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	choice features informed similar ones commonly used statistical language models, well general intuitions potentially information-bearing elements discourse context.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thus, model designed current DA label constrained features unigram statistics, previous DA DA removed, DAs occurring within window past, whether previous utterance speaker.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	found, however, model using n-gram constraints performed slightly better corresponding backoff n-gram.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Additional constraints DA triggers, distance-1 bigrams, separate encoding speaker change bigrams last DA same/other channel improve relative trigram model.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	model thus confirms adequacy backoff n-gram approach, leads us conclude DA sequences, least Switchboard domain, mostly characterized local interactions, thus modeled well low-order n-gram statistics task.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	structured tasks situation might different.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, found exploitable structure.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	describe detail knowledge sources words prosody modeled, automatic DA labeling results obtained using knowledge sources turn.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally, present results combination knowledge sources.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA labeling accuracy results compared baseline (chance) accuracy 35%, relative frequency frequent DA type (STATEMENT)in test set.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	4 5.1 Dialogue Act Classification Using Words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA classification using words based observation different DAs use distinctive word strings.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	known certain cue words phrases (Hirschberg Litman 1993) serve explicit indicators discourse structure.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Similarly, find distinctive correlations certain phrases DA types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"example, 92.4% uh-huh's occur BACKCHANNELS,and 88.4% trigrams ""<start> you"" occur YES-NO-QUESTIONS."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	leverage information source, without hand-coding knowledge words indicative DAs, use statistical language models model full word sequences associated DA type.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.1.1 Classification True Words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Assuming true (hand-transcribed) words utterances given evidence, compute word-based likelihoods P(WIU ) straightforward way, building statistical language model 42 DAs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DAs particular type found training corpus pooled, DA-specific trigram model estimated using standard techniques (Katz backoff [Katz 1987] Witten-Bell discounting [Witten Bell 1991]).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	4 frequency STATEMENTS across labeled data slightly different, cf.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 2..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	A1 wl Ai wi w. <start> ~ U1 ~ ....	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	~ Ui ~ ....	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Un ~ <end> Figure 2 Modified Bayes network including word hypotheses recognizer acoustics.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.1.2 Classification Recognized Words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	fully automatic DA classification, approach partial solution, since yet able recognize words spontaneous speech perfect accuracy.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	standard approach use 1-best hypothesis speech recognizer place true word transcripts.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	conceptually simple convenient, method make optimal use information recognizer, fact maintains multiple hypotheses well relative plausibilities.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	thorough use recognized speech derived follows.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	classification framework modified recognizer's acoustic information (spectral features) appear evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	compute P(A[U) decomposing acoustic likelihood P(A]W) word-based likelihood P(W[ U), summing word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w second line justified assumption recognizer acoustics (typically, cepstral coefficients) invariant DA type words fixed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Note another approximation modeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, different DAs common words may realized different word pronunciations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Figure 2 shows Bayes network resulting modeling recognizer acoustics word hypotheses independence assumption; note added Wi variables (that summed over) comparison Figure 1.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	acoustic likelihoods P(A[W) correspond acoustic scores recognizer outputs every hypothesized word sequence W. summation W must approximated; experiments summed (up to) 2,500 best hypotheses generated recognizer utterance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Care must taken scale recognizer acoustic scores properly, i.e., exponentiate recognizer acoustic scores 1/~, language model weight recognizer, 5 standard recognizer total log score hypothesis Wi computed as.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	logP(AdWi ) + )~ logP(Wi) - I~]Wi], [Wi]is number words hypothesis, and/~ parameters optimized minimize word error rate.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	word insertion penalty/~ represents correction language model allows balancing insertion deletion errors.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	language model weight ,~ compensates acoustic score variances effectively large due severe independence assumptions recognizer acoustic model.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	According rationale, appropriate divideall score components ),.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thus, experiments, computed summand Equation 6 whose Table 6 DA classification accuracies (in %) transcribed recognized words (chance = 35%).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 6 shows DA classification accuracies obtained combining word- recognizer-based likelihoods n-gram discourse grammars described earlier.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	best accuracy obtained transcribed words, 71%, encouraging given comparable human performance 84% (the interlabeler agreement, see Section 2.2).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	observe 21% relative increase classification error using recognizer words; remarkably small considering speech recognizer used word error rate 41% test set.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also compared n-best DA classification approach straightforward 1-best approach.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	experiment, single best recognizer hypothesis used, effectively treating true word string.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1-best method increased classification error 7% relative n-best algorithm (61.5% accuracy bigram discourse grammar).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2 Dialogue Act Classification Using Prosody.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also investigated prosodic information, i.e., information independent words well standard recognizer acoustics.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prosody important DA recognition two reasons.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	First, saw earlier, word-based classification suffers recognition errors.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Second, utterances inherently ambiguous based words alone.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, YES-NO-QUESTiONS word sequences identical STATEMENTS, often distinguished final F0 rise.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	detailed study aimed automatic prosodic classification DAs Switchboard domain available companion paper (Shriberg et al. 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	investigate interaction prosodic models dialogue grammar word-based DA models discussed above.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also touch briefly alternative machine learning models prosodic features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2.1 Prosodic Features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prosodic DA classification based large set features computed automatically waveform, without reference word phone information.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	features broadly grouped referring duration (e.g., utterance duration, without pauses), pauses (e.g., total mean nonspeech regions exceeding 100 ms), pitch (e.g., mean range F0 utterance, slope F0 regression line), energy (e.g., mean range RMS energy, signal-to- logarithm -d1 logP(Ai]Wi) + logP(WilUi) -~lWil.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	found approach give better results standard multiplication logP(W) ,L Note selecting best hypothesis recognizer relative magnitudes score weights matter; however, summation Equation 6 absolute values become important.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	parameter values )~ # used standard recognizer; specifically optimized DA classification task.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"~ 23.403 utt < 0.3""/~U >= 0.3""/17 ~ Figure 3 Decision tree classification BACKCHANNELS (B) AGREEMENTS (A)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	node labeled majority class node, well posterior probabilities two classes.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	following features queried tree: number frames continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), mean signal-to-noise ratio (snr_mean_utt ).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"noise ratio [SNR]), speaking rate (based ""enrate"" measure Morgan, Fosler, Mirghafori [1997]), gender (of speaker listener)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	case utterance duration, measure correlates length words overall speaking rate.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	gender feature classified speakers either male female used test potential inadequacies F0 normalizations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	appropriate, included raw features values normalized utterance and/or conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also included features output pitch accent boundary tone event detector Taylor (2000) (e.g., number pitch accents utterance).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	complete description prosodic features analysis usage models found Shriberg et al. (1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2.2 Prosodic Decision Trees.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prosodic classifiers, used CART-style decision trees (Breiman et al. 1984).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Decision trees allow combination discrete continuous features, inspected help understanding role different features feature combinations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	illustrate one area prosody could aid classification task, applied trees DA classifications known ambiguous words alone.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	One frequent example corpus distinction BACKCHANNELS AGREEMENTS (see Table 2), share terms right yeah.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	shown Figure 3, prosodic tree trained task revealed agreements consistently longer durations greater energy (as reflected SNR measure) backchannels.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 7 DA classification using prosodic decision trees (chance = 35%).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 HMM framework requires compute prosodic likelihoods form P(FilUi) utterance Ui associated prosodic feature values Fi.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	apparent difficulty decision trees (as well classifiers, neural networks) give estimates posterior probabilities, P(Ui[Fi).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	problem overcome applying Bayes' rule locally: (7) true) P(Ui) Note P(Fi) depend Ui treated constant purpose DA classification.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	quantity proportional required likelihood therefore obtained either dividing posterior tree probability prior P(Ui), 6 training tree uniform prior distribution DA types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	chose second approach, downsampling training data equate DA proportions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also counteracts common problem tree classifiers trained skewed distributions target classes, i.e., low-frequency classes modeled sufficient detail majority class dominates tree-growing objective hznction.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2.3 Results Decision Trees.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	preliminary experiment test integration prosody knowledge sources, trained single tree discriminate among five frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, AGREEMENT,totaling 79% data) category comprising remaining DA types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	decision tree trained downsampled training subset containing equal proportions six DA classes.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tree achieved classification accuracy 45.4% independent test set uniform six-class distribution.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	chance accuracy set 16.6%, tree clearly extracts useful information prosodic features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	used decision tree posteriors scaled DA likelihoods dialogue model HMM, combining various n-gram dialogue grammars testing full standard test set.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	purpose model integration, likelihoods class assigned DA types comprised class.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	shown Table 7, tree dialogue grammar performs significantly better chance raw DA distribution, although well word-based methods (cf.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 6).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2.4 Neural Network Classifiers.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Although chose use decision trees prosodic classifiers relative ease inspection, might used suitable probabilistic classifier, i.e., model estimates posterior probabilities DAs given prosodic features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	conducted preliminary experiments assess neural networks compare decision trees type data studied here.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Neural networks worth investigating since offer potential advantages decision trees.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	learn decision surfaces lie angle axes input feature space, unlike standard CART trees, always split continuous features one dimension time.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	response function neural networks continuous (smooth) decision boundaries, allowing avoid hard decisions complete fragmentation data associated decision tree questions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	important, however, related work (Ries 1999a) indicated similarly structured networks superior classifiers input features words therefore plugin replacement language model classifiers described paper.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Neural networks therefore good candidate jointly optimized classifier prosodic word-level information since one show generalization integration approach used here.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tested various neural network models six-class downsampled data used decision tree training, using variety network architectures output layer functions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	results summarized Table 8, along baseline result obtained decision tree model.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Based experiments, softmax network (Bridle 1990) without hidden units resulted slight improvement decision tree.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	network hidden units afford additional advantage, even optimized number hidden units, indicating complex combinations features (as far network could learn them) predict DAs better linear combinations input features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	believe alternative classifier architectures investigated prosodic models, results far seem confirm choice decision trees model class gives close optimal performance task.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.2.5 Intonation Event Likelihoods.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	alternative way compute prosodically based DA likelihoods uses pitch accents boundary phrases (Taylor et al. 1997).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"approach relies intuition different utterance types characterized different intonational ""tunes"" (Kowtko 1996), successfully applied classification move types DCIEM Map Task corpus (Wright Taylor 1997)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	system detects sequences distinctive pitch patterns training one continuous- density HMM DA type.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Unfortunately, event classification accuracy Switchboard corpus considerably poorer Map Task domain, DA recognition results coupled discourse grammar substantially worse decision trees.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach could prove valuable future, however, intonation event detector made robust corpora like OURS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"A1 Ai 1 1 Wl Wi W,, <start> -, 0""1 , ...---* U/ , ..."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network discourse HMM incorporating word recognition prosodic features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.3 Using Multiple Knowledge Sources.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	mentioned earlier, expect improved performance combining word prosodic information.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Combining knowledge sources requires estimating combined likelihood P(Ai, Fi[Ui) utterance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	simplest approach assume two types acoustic observations (recognizer acoustics prosodic features) approximately conditionally independent Ui given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since recognizer acoustics modeled way dependence words, particularly important avoid using prosodic features directly correlated word identities, features also modeled discourse grammars, utterance position relative turn changes.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Figure 4 depicts Bayes network incorporating evidence word recognition prosodic features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	One important respect independence assumption violated modeling utterance length.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	utterance length prosodic feature, important feature condition examining prosodic characteristics utterances, thus best included decision tree.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Utterance length captured directly tree using various duration measures, DA-specific LMs encode average number words per utterance indirectly n-gram parameters, still accurately enough violate independence significant way (Finke et al. 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	discussed Section 8, problem best addressed joint lexical-prosodic models.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	need allow fact models combined Equation 8 give estimates differing qualities.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Therefore, introduce exponential weight P(Fi[Ui) controls contribution prosodic likelihood overall likelihood.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally, second exponential weight fl combined likelihood controls dynamic range relative discourse grammar scores, partially compensating correlation two likelihoods.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) experiments, parameters fl optimized using twofold jackknifing.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	test data split roughly half (without speaker overlap), half used separately optimize parameters, best values tested respective half.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	reported results aggregate outcome two test set halves.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 9 Combined utterance classification accuracies (chance = 35%).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	first two columns correspond Tables 7 6, respectively.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) individual combined models two subtasks, using uniform priors (chance = 50%).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody 76.0 76.0 words 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody 72.9 72.9 words 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	experiment combined acoustic n-best likelihoods based recognized words Top-5 tree classifier mentioned Section 5.2.3.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Results summarized Table 9.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	shown, combined classifier presents slight improvement recognizer-based classifier, experiment without discourse grammar indicates combined evidence considerably stronger either knowledge source alone, yet improvement seems made largely redundant use priors discourse grammar.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, definition DECLARATIVE-QUESTIONS marked syntax (e.g., subject-auxiliary inversion) thus confusable STATEMENTS OPINIONS.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	prosody expected help disambiguate cases, ambiguity also removed examining context utterance, e.g., noticing following utterance YEs-ANswER NO-ANSWER.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	5.3.2 Focused Classifications.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	gain better understanding potential prosodic DA classification independent effects discourse grammar skewed DA distribution Switchboard, examined several binary DA classification tasks.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	choice tasks motivated analysis confusions committed purely word-based DA detector, tends mistake QUESTIONS STATEMENTS, BACKCHANNELS AGREEMENTS (and vice versa).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tested prosodic classifier, word-based classifier (with transcribed recognized words), combined classifier two tasks, downsampling DA distribution equate class sizes case.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Chance performance experiments therefore 50%.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Results summarized Table 10.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	shown, combined classifier consistently accurate classifier using words alone.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Although gain accuracy statistically significant small recognizer test set lack power, replication larger hand-transcribed test set showed gain highly significant subtasks Sign test, p < .001 p < .0001 (one-tailed), respectively.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Across these, well additional subtasks, relative advantage adding prosody larger recognized true words, suggesting prosody particularly helpful word information perfect.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	consider ways use DA modeling enhance automatic speech recognition (ASR).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	intuition behind approach discourse context constrains choice DAs given utterance, DA type turn constrains choice words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	latter leveraged accurate speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	6.1 Integrating DA Modeling ASR.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Constraints word sequences hypothesized recognizer expressed probabilistically recognizer language model (LM).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	provides prior distribution P(Wi) finding posteriori probable hypothesized words utterance, given acoustic evidence Ai (Bahl, Jelinek, Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi likelihoods P(AilWi) estimated recognizer's acoustic model.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	standard recognizer language model P(Wi) utterances; idea obtain better-quality LMs conditioning DA type Ui, since presumably word distributions differ depending DA type.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi DA classification model, tacitly assume words Wi depend DA current utterance, also acoustics independent DA type words fixed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA-conditioned language models P(Wil Ui) readily trained DA-specific training data, much DA classification words.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	8 7 Note similarity Equations 10 1.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	identical except fact now.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	operating level individual utterance, evidence given acoustics, targets word hypotheses instead DA hypotheses.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	8 Equation 11 elsewhere section gloss issue proper weighting model.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	probabilities, extremely important practice.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach explained detail footnote 5 applies well.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	problem applying Equation 11, course, DA type Ui generally known (except maybe applications user interface engineered allow one kind DA given utterance).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Therefore, need infer likely DA types utterance, using available evidence E entire conversation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	leads following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ last step Equation 12 justified because, shown Figures 1 4, evidence E (acoustics, prosody, words) pertaining utterances affect current utterance DA type Ui.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	call mixture-of-posteriorsapproach, amounts mixture posterior distributions obtained DA-specific speech recognizers (Equation 11), using DA posteriors weights.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach quite expensive, however, requires multiple full recognizer rescoring passes input, one DA type.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	efficient, though mathematically less accurate, solution obtained combining guesses correct DA types directly level LM.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	estimate distribution likely DA types given utterance using entire conversation E evidence, use sentence-level mixture (Iyer, Ostendorf, Rohlicek 1994) DA-specific LMs single recognizer run.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	words, replace P(WilUi) Equation 11 ~_~ P(WilUi)P(Ui]E), ui weighted mixture DA-specific LMs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	call mixture-of-LMs approach.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	practice, would first estimate DA posteriors utterance, using forward-backward algorithm models described Section 5, rerecognize conversation rescore recognizer output, using new posterior- weighted mixture LM.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Fortunately, shown next section, mixture-of-LMs approach seems give results almost identical (and good as) mixture- of-posteriors approach.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	6.2 Computational Structure Mixture Modeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	instructive compare expanded scoring formulas two DA mixture modeling approaches ASK mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates LM perplexities.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 see second equation reduces first crude approximation P(Ai] Ui) ~ P(Ai).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	practice, denominators computed summing numerators finite number word hypotheses Wi, difference translates normalizing either summing DAs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	normalization takes place final step omitted score maximization purposes; shows mixture-of-LMs approach less computationally expensive.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	6.3 Experiments Results.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tested mixture-of-posteriors mixture-of-LMs approaches Switchboard test set 19 conversations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Instead decoding data scratch using modified models, manipulated n-best lists consisting 2,500 best hypotheses utterance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach also convenient since approaches require access full word string hypothesis scoring; overall model longer Markovian, therefore inconvenient use first decoding stage, even lattice rescoring.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	baseline experiments obtained standard backoff trigram language model estimated available training data.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA-specific language models trained word transcripts training utterances given type, smoothed interpolating baseline LM.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA- specific LM used interpolation weight, obtained minimizing perplexity interpolated model held-out DA-specific training data.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Note smoothing step helpful using DA-specific LMs word recognition, DA classification, since renders DA-specific LMs less discriminative.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	9 Table 11 summarizes word error rates achieved various models perplexities corresponding LMs used rescoring (note perplexity meaningful mixture-of-posteriors approach).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"comparison, also included two additional models: 'q-best LM"" refers always using DA- specific LM corresponding probable DA type utterance."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	thus approximation mixture approaches top DA considered.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"Second, included ""oracle LM,"" i.e., always using LM corresponds hand-labeled DA utterance."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	purpose experiment give us upper bound effectiveness mixture approaches, assuming perfect DA recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	somewhat disappointing word error rate (WER) improvement oracle experiment small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according Sign test matched utterance pairs).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	9 Indeed, DA classification experiments, observed smoothed DA-specific LMs yield lower classification accuracy..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA-specific LMs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	detailed analysis effect DA modeling speech recognition errors found elsewhere (Van EssDykema Ries 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	summary, experiments confirmed DA modeling improve word recognition accuracy quite substantially principle, least certain DA types, skewed distribution DAs (especially terms number words per type) limits usefulness approach Switchboard corpus.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	benefits DA modeling might therefore pronounced corpora even DA distribution, typically case task-oriented dialogues.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Task-oriented dialogues might also feature specific subtypes general DA categories might constrained discourse.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prior research task-oriented dialogues summarized next section, however, also found small reductions WER (on order 1%).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	suggests even task-oriented domains research needed realize potential DA modeling ASR.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	indicated introduction, work builds number previous efforts computational discourse modeling automatic discourse processing, occurred last half-decade.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"generally possible directly compare quantitative results vast differences methodology, tag set, type amount training data, and, principally, assumptions made information available ""free"" (e.g., hand-transcribed versus automatically recognized words, segmented versus unsegmented utterances)."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thus, focus conceptual aspects previous research efforts, offer summary previous quantitative results, interpreted informative datapoints only, fair comparisons algorithms.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Previous research DA modeling generally focused task-oriented dialogue, three tasks particular garnering much research effort.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists conversations two speakers slightly different maps imaginary territory.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	task help one speaker reproduce route drawn speaker's map, without able see other's maps.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA modeling algorithms described below, Taylor et al. (1998) Wright (1998) based Map Task.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	VERBMOBIL corpus consists two-party scheduling dialogues.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	number DA m6deling algorithms described developed VERBMOBIL, including Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger Klesen (1997), Samuel, Carberry, VijayShanker (1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ATR Conference corpus subset larger ATR Dialogue database consisting simulated dialogues secretary questioner international conferences.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Researchers using corpus include Nagata (1992), Nagata Morimoto (1993, 1994), Kita et al. (1996).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 13 shows commonly used versions tag sets three tasks.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	discussed earlier, domains differ Switchboard corpus task-oriented.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	tag sets also generally smaller, problems balance occur.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, Map Task domain, 33% words occur 1 12 DAs 0NSTRUCT).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 14 shows approximate size corpora, tag set, tag estimation accuracy rates various recent models DA prediction.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	results summarized table also illustrate differences inherent difficulty tasks.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, task Warnke et al. (1997) simultaneously segment tag DAs, whereas results rely prior manual segmentation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Similarly, task Wright (1998) study determine DA types speech input, whereas work others based hand-transcribed textual input.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Table 13 Dialogue act tag sets used three extensively studied corpora.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	VERBMOBIL.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	18 high-level DAs used VERBMOBIL1 abstracted total 43 specific DAs; experiments VERBMOBIL DAs use set 18 rather 43.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Examples Jekat et al. (1995).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's BYE Alright bye REQUEST~COMMENT look?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	SUGGEST thirteenth seventeenth June REJECT Friday I'm booked day ACCEPT Saturday sounds fine, REQUEST-SUGGEST good day week you?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	INIT wanted make appointment GIVE_REASON meetings afternoon FEEDBACK Okay DELIBERATE Let check calendar CONFIRM Okay, would wonderful CLARIFY Okay, mean Tuesday 23rd?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DIGRESS [we could meet lunch] eat lots ice cream MOTIVATE go visit subsidiary Munich GARBAGE Oops, I- Maptask.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"12 DAs ""move types"" used Map Task."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Examples Taylor et al. (1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN don't ravine ALIGN Okay?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	CHECK going Indian Country?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	QUERY-YN got graveyard written ? QUERY-W where?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ACKNOWLEDGE Okay CLARIFY {you want go... diagonally} Diagonally REPLY-Y do.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	REPLY-N No, don't REPLY-W {And across to?}	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	pyramid.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	READY Okay ATR.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"9 DAs (""illocutionary force types"") used ATR Dialogue database task; later models used extended set 15 DAs."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Examples English translations given Nagata (1992).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Tag Example PHATIC Hello EXPRESSIVE Thank RESPONSE That's right PROMISE send registration form REQUEST Please go Kitaooji station subway INFORM giving discount time QUESTIONIP announcement conference ? QUESTIONREF do?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	QUESTIONCONF already transferred registration fee, right ? C ~'~ % .~ > .~~ ~ ~ Â°Â°ll ~.~ ~~ g,..l ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', Â¢~ ,-~ ,.0 NS~ use n-grams model probabilities DA sequences, predict upcoming DAs online, proposed many authors.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	seems first employed Nagata (1992), follow-up papers Nagata Morimoto (1993, 1994) ATR Dialogue database.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	model predicted upcoming DAs using bigrams trigrams conditioned preceding DAs, trained corpus 2,722 DAs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Many others subsequently relied enhanced n-grams-of-DAs approach, often applying standard techniques statistical language modeling.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Reithinger et al. (1996), example, used deleted interpolation smooth dialogue n-grams.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	ChuCarroll (1998) uses knowledge subdialogue structure selectively skip previous DAs choosing conditioning DA prediction.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Nagata Morimoto (1993, 1994) may also first use word n-grams miniature grammar DAs, used improving speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	idea caught quickly: Suhm Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger Klesen (1997), Taylor et al. (1998) use variants backoff, interpolated, class n-gram language models estimate DA likelihoods.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	kind sufficiently powerful, trainable language model could perform function, course, indeed Alexandersson Reithinger (1997) propose using automatically learned stochastic context-free grammars.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Jurafsky, Shriberg, Fox, Curl (1998) show grammar DAs, appreciations, captured finite-state automata part-of-speech tags.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	N-gram models likelihood models DAs, i.e., compute conditional probabilities word sequence given DA type.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Word-based posterior probability estimators also possible, although less common.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Mast et al. (1996) propose use semantic classification trees, kind decision tree conditioned word patterns features.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally, Ries (1999a) shows neural networks using unigram features superior higher-order n-gram DA models.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Warnke et al. (1999) Ohler, Harbeck, Niemann (1999) use related discriminative training algorithms language models.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Woszczyna Waibel (1994) Suhm Waibel (1994), followed ChuCarroll (1998), seem first note combination word dialogue n-grams could viewed dialogue HMM word strings observations.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	(Indeed, exception Samuel, Carberry, VijayShanker (1998), models listed Table 14 rely version HMM metaphor.)	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	researchers explicitly used HMM induction techniques infer dialogue grammars.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Woszczyna Waibel (1994), example, trained ergodic HMM using expectation-maximization model speech act sequencing.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Kita et al. (1996) made one attempts unsupervised discovery dialogue structure, finite-state grammar induction algorithm used find topology dialogue grammar.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Computational approaches prosodic modeling DAs aimed automatically extract various prosodic parameters--such duration, pitch, energy patterns--from speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approaches model F0 patterns techniques vector quantization Gaussian classifiers help disambiguate utterance types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	extensive comparison prosodic DA modeling literature work found Shriberg et al. (1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA modeling mostly geared toward automatic DA classification, much less work done applying DA models automatic speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Nagata Morimoto (1994) suggest conditioning word language models DAs lower perplexity.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Suhm Waibel (1994) Eckert, Gallwitz, Niemann (1996) condition recognizer LM left-to-right DA predictions able show reductions word error rate 1% task-oriented corpora.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	similar work, still task-oriented domain, work Taylor et al. (1998) combines DA likelihoods prosodic models 1-best recognition output condition recognizer LM, achieving absolute reduction word error rate 1%, disappointing 0.3% improvement experiments.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Related computational tasks beyond DA classification speech recognition received even less attention date.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	already mentioned Warnke et al. (1997) Finke et al. (1998), showed utterance segmentation classification integrated single search process.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	"Fukada et al. (1998) investigate augmenting DA tagging detailed semantic ""concept"" tags, preliminary step toward interlingua-based dialogue translation system."	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Levin et al. (1999) couple DA classification dialogue game classification; dialogue games units DA level, i.e., short DA sequences question-answer pairs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	work mentioned far uses statistical models various kinds.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	shown here, models offer fundamental advantages, modularity composability (e.g., discourse grammars DA models) ability deal noisy input (e.g., speech recognizer) principled way.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, many classifier architectures applicable tasks discussed, particular DA classification.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	nonprobabilistic approach DA labeling proposed Samuel, Car- berry, VijayShanker (1998) transformation-based learning (Brill 1993).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally noted tasks mathematical structure similar DA tagging, shallow parsing natural language processing (Munk 1999) DNA classification tasks (Ohler, Harbeck, Niemann 1999), techniques could borrowed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach presented differ various earlier models, particularly based HMMs?	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Apart corpus tag set differences, approach differs primarily generalizes simple HMM approach cope new kinds problems, based Bayes network representations depicted Figures 2 4.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA classification task, framework allows us classification given unreliable words (by marginalizing possible word strings corresponding acoustic input) given nonlexical (e.g., prosodic) evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	speech recognition task, generalized model gives clean probabilistic framework conditioning word probabilities conversation context via underlying DA structure.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Unlike previous models address speech recognition relied intuitive 1-best approximation, model allows computation optimum word sequence effectively summing possible DA sequences well recognition hypotheses throughout conversation, using evidence past future.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach dialogue modeling two major components: statistical dialogue grammars modeling sequencing DAs, DA likelihood models expressing local cues (both lexical prosodic) DAs.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	made number significant simplifications arrive computationally statistically tractable formulation.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	formulation, DAs serve hinges join various model components, also decouple components statistical independence assumptions.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Conditional DAs, observations across utterances assumed independent, evidence different kinds utterance (e.g., lexical prosodic) assumed independent.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally, DA types assumed independent beyond short span (corresponding order dialogue n-gram).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	research within framework characterized simplifications addressed.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Dialogue grammars conversational speech need made aware temporal properties utterances.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, currently modeling fact utterances conversants may actually overlap (e.g., backchannels interrupting ongoing utterance).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	addition, model nonlocal aspects discourse structure, despite negative results far.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	example, context-free discourse grammar could potentially account nested structures proposed Grosz Sidner (1986).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	1Â° standard n-gram models DA discrimination lexical cues probably suboptimal task, simply trained maximum likelihood framework, without explicitly optimizing discrimination DA types.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	may overcome using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, Niemann 1999).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Training neural networks directly posterior probability (Ries 1999a) seems principled approach also offers much easier integration knowledge sources.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Prosodic features, example, simply added lexical features, allowing model capture dependencies redundancies across knowledge sources.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Keyword-based techniques field message classification also applicable (Rose, Chang, Lippmann 1991).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Eventually, desirable integrate dialogue grammar, lexical, prosodic cues single model, e.g., one predicts next DA based DA history local evidence.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	study automatically extracted prosodic features DA modeling likewise infancy.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	preliminary experiments neural networks shown small gains obtainable improved statistical modeling techniques.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, believe progress made improving underlying features themselves, terms better understanding speakers use them, ways reliably extract data.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Regarding data itself, saw distribution DAs corpus limits benefit DA modeling lower-level processing, particular speech recognition.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	reason skewed distribution nature task (or lack thereof) Switchboard.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	remains seen fine-grained DA distinctions made reliably corpus.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, noted DA definitions really arbitrary far tasks DA labeling concerned.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	suggests using unsupervised, self-organizing learning schemes choose DA definitions process optimizing primary task, whatever may be.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Hand-labeled DA categories may still serve important role initializing algorithm.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	believe dialogue-related tasks much benefit corpus-driven, automatic learning techniques.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	enable research, need fairly large, standardized corpora allow comparisons time across approaches.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Despite shortcomings, Switchboard domain could serve purpose.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	developed integrated probabilistic approach dialogue act modeling conversational speech, tested large speech corpus.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	approach combines models lexical prosodic realizations DAs, well statistical discourse 10 inadequacy n-gram models nested discourse structures pointed ChuCarroll (1998), although suggested solution modified n-gram approach..	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	grammar.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	components model automatically trained, thus applicable domains labeled data available.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Classification accuracies achieved far highly encouraging, relative inherent difficulty task measured human labeler performance.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	investigated several modeling alternatives components model (backoff n-grams maximum entropy models discourse grammars, decision trees neural networks prosodic classification) found performance largely independent choices.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Finally, developed principled way incorporating DA modeling probability model continuous speech recognizer, constraining word hypotheses using discourse context.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	However, approach gives small reduction word error corpus, attributed preponderance single dialogue act type (statements).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Note research described based project 1997 Workshop Innovative Techniques LVCSR Center Speech Language Processing Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	DA-labeled Switchboard transcripts well project-related publications available http://www.colorado.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	edu/ling/jurafsky/ws97/.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	thank funders, researchers, support staff 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, Chuck Wooters.	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Additional support came NSF via grants IRI9619921 IRI9314967, UK Engineering Physical Science Research Council (grant GR/J55106).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	Thanks Mitch Weintraub, Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, Marilyn Walker advice design SWBDDAMSL tag set, discourse labelers CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, Amma Oduro) intonation labelers University Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, Matthew Bull).	0
representing higher level intention utterancesduring human conversation, dialogue act labels arebeing used enrich information provided byspoken words (Stolcke et al., 2000).	also thank Andy Kehler anonymous reviewers valuable comments draft paper.	0
