approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	model advantage pre-tagged training corpus required.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	structure state chains based analysis errors linguistic knowledge.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Examples show word dependency across phrases modeled.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Application areas include speech recognition/synthesis information retrieval.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Several workers addressed problem tagging text.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	statistical methods described terms Markov models.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	States model represent categories {cl...c=} (n number different categories used).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	word sequence considered generated underlying sequence categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	possible category sequences given word sequence generated, one maximizes probability words used.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	great advantage eliminating pre-tagged corpus.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	work described also makes use hidden Markov model.	1
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	One aim work investigate quality performance models minimal parameter descriptions.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	regard, word equivalence classes used (Kupiec, 1989).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Obviously, trade-off involved.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	However members equivalence class noun-or-verb, considered behave identically.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	local word context (embodied transition probabilities) must aid disambiguation word.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Sing.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"interesting note model performs well, despite ""noisy"" training data."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	training sentence-based, model trained using 6,000 sentences corpus.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Eight iterations Baum-Welch algorithm used.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	(If desired, tagging finer category replaced).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	basic model punctuation except sentence boundaries ignored.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	interesting observation worth noting regard words act auxiliary main verbs.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	indicates modal auxiliaries recognized natural class via pattern usage.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Extending Basic Model basic model used benchmark successive improvements.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	first addition correct treatment non-words text.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	includes hyphenation, punctuation, numbers abbreviations.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	New categories added number, abbreviation, comma.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	punctuation collapsed single new punctuation category.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Refinement Basic Categories verb states basic model found coarse.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	leaves 50% corpus training equivalence classes.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	limited context adequately model constraint present local word context.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Transition probabilities form P(Ci [ Ci1, Ci2).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	n category model requires n 3 transition probabilities.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Increasing order conditioning requires exponentially parameters.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	conditioning described uniform- possible two-category contexts modeled.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	alternative uniformly increasing order conditioning extend selectively.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Mixed higher- order context modeled introducing explicit state sequences.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	basic network augmented extra state sequences model certain category sequences detail.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	example, may consider systematic error made basic model.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	model context necessary correct error, two extra states used, shown Figure 1.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Training hidden Markov model topology corrected nine instances error test data.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	important point note improving model detail manner forcibly correct error.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	actual patterns category usage must distinct language.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	state generally used several places (E.g. Figure 1.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	two noun states, two adjective states: one augmented network, basic network).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	distributions P(Eqvi Ci) considered every instance state.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	exemplified following sentence fragments: 1.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Temperatures upper mantle range apparently from.....	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	2.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	The velocity seismic waves rises to....	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	model dependency across phrase, networks shown Figure 2 used.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	previous section, corrections programmed model.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	(Approximately 19,000 sentences used train networks used example).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	disadvantage cannot share training data.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	problem could resolved tying corresponding transitions together.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	document, 142 words tagged unknown (their possible categories known).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	total 1,526 words ambiguous categories (i.e. 40% document).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"example, 9 errors 3 instances ""... well ..."" arise text."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	1 easy construct counterexamples sentences presented here, tagging would correct.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	He ran big hill.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	Extra information required assign correct tagging.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	stochastic method assigning part-of-speech categories unrestricted English text described.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	minimizes resources required high performance automatic tagging.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	pre-tagged training corpus required, tagger cope words found training text.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
approach similar spirit iterative computational approaches Hidden Markov Models (Kupiec, 1989	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	model advantage pre-tagged training corpus required.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	structure state chains based analysis errors linguistic knowledge.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Examples show word dependency across phrases modeled.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Application areas include speech recognition/synthesis information retrieval.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Several workers addressed problem tagging text.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	statistical methods described terms Markov models.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	States model represent categories {cl...c=} (n number different categories used).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	word sequence considered generated underlying sequence categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	possible category sequences given word sequence generated, one maximizes probability words used.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	great advantage eliminating pre-tagged corpus.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	work described also makes use hidden Markov model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	One aim work investigate quality performance models minimal parameter descriptions.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	regard, word equivalence classes used (Kupiec, 1989).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Obviously, trade-off involved.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	However members equivalence class noun-or-verb, considered behave identically.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	local word context (embodied transition probabilities) must aid disambiguation word.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Sing.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"interesting note model performs well, despite ""noisy"" training data."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	training sentence-based, model trained using 6,000 sentences corpus.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Eight iterations Baum-Welch algorithm used.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	(If desired, tagging finer category replaced).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	basic model punctuation except sentence boundaries ignored.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	interesting observation worth noting regard words act auxiliary main verbs.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	indicates modal auxiliaries recognized natural class via pattern usage.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Extending Basic Model basic model used benchmark successive improvements.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	first addition correct treatment non-words text.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	includes hyphenation, punctuation, numbers abbreviations.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	New categories added number, abbreviation, comma.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	punctuation collapsed single new punctuation category.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Refinement Basic Categories verb states basic model found coarse.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	leaves 50% corpus training equivalence classes.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	limited context adequately model constraint present local word context.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Transition probabilities form P(Ci [ Ci1, Ci2).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	n category model requires n 3 transition probabilities.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Increasing order conditioning requires exponentially parameters.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	conditioning described uniform- possible two-category contexts modeled.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	alternative uniformly increasing order conditioning extend selectively.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Mixed higher- order context modeled introducing explicit state sequences.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	basic network augmented extra state sequences model certain category sequences detail.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	example, may consider systematic error made basic model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	model context necessary correct error, two extra states used, shown Figure 1.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Training hidden Markov model topology corrected nine instances error test data.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	important point note improving model detail manner forcibly correct error.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	actual patterns category usage must distinct language.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	state generally used several places (E.g. Figure 1.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	two noun states, two adjective states: one augmented network, basic network).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	distributions P(Eqvi Ci) considered every instance state.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	exemplified following sentence fragments: 1.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Temperatures upper mantle range apparently from.....	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	2.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	The velocity seismic waves rises to....	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	model dependency across phrase, networks shown Figure 2 used.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	previous section, corrections programmed model.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	(Approximately 19,000 sentences used train networks used example).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	disadvantage cannot share training data.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	problem could resolved tying corresponding transitions together.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	document, 142 words tagged unknown (their possible categories known).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	total 1,526 words ambiguous categories (i.e. 40% document).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"example, 9 errors 3 instances ""... well ..."" arise text."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	1 easy construct counterexamples sentences presented here, tagging would correct.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	He ran big hill.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	Extra information required assign correct tagging.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	stochastic method assigning part-of-speech categories unrestricted English text described.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	minimizes resources required high performance automatic tagging.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	pre-tagged training corpus required, tagger cope words found training text.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
[Kupiec, 1989a], networks used selectively augment context basic first- order model, rather using uniformly second-order dependencies.	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	model advantage pre-tagged training corpus required.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	structure state chains based analysis errors linguistic knowledge.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Examples show word dependency across phrases modeled.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Application areas include speech recognition/synthesis information retrieval.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Several workers addressed problem tagging text.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	statistical methods described terms Markov models.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	States model represent categories {cl...c=} (n number different categories used).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	word sequence considered generated underlying sequence categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	possible category sequences given word sequence generated, one maximizes probability words used.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	great advantage eliminating pre-tagged corpus.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	work described also makes use hidden Markov model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	One aim work investigate quality performance models minimal parameter descriptions.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	regard, word equivalence classes used (Kupiec, 1989).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Obviously, trade-off involved.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	However members equivalence class noun-or-verb, considered behave identically.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	local word context (embodied transition probabilities) must aid disambiguation word.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Sing.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"interesting note model performs well, despite ""noisy"" training data."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	training sentence-based, model trained using 6,000 sentences corpus.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Eight iterations Baum-Welch algorithm used.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	(If desired, tagging finer category replaced).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	basic model punctuation except sentence boundaries ignored.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	interesting observation worth noting regard words act auxiliary main verbs.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	indicates modal auxiliaries recognized natural class via pattern usage.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Extending Basic Model basic model used benchmark successive improvements.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	first addition correct treatment non-words text.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	includes hyphenation, punctuation, numbers abbreviations.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	New categories added number, abbreviation, comma.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	punctuation collapsed single new punctuation category.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Refinement Basic Categories verb states basic model found coarse.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	leaves 50% corpus training equivalence classes.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	limited context adequately model constraint present local word context.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Transition probabilities form P(Ci [ Ci1, Ci2).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	n category model requires n 3 transition probabilities.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Increasing order conditioning requires exponentially parameters.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	conditioning described uniform- possible two-category contexts modeled.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	alternative uniformly increasing order conditioning extend selectively.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Mixed higher- order context modeled introducing explicit state sequences.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	basic network augmented extra state sequences model certain category sequences detail.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	example, may consider systematic error made basic model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	model context necessary correct error, two extra states used, shown Figure 1.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Training hidden Markov model topology corrected nine instances error test data.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	important point note improving model detail manner forcibly correct error.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	actual patterns category usage must distinct language.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	state generally used several places (E.g. Figure 1.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	two noun states, two adjective states: one augmented network, basic network).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	distributions P(Eqvi Ci) considered every instance state.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	exemplified following sentence fragments: 1.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Temperatures upper mantle range apparently from.....	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	2.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	The velocity seismic waves rises to....	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	model dependency across phrase, networks shown Figure 2 used.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	previous section, corrections programmed model.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	(Approximately 19,000 sentences used train networks used example).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	disadvantage cannot share training data.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	problem could resolved tying corresponding transitions together.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	document, 142 words tagged unknown (their possible categories known).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	total 1,526 words ambiguous categories (i.e. 40% document).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"example, 9 errors 3 instances ""... well ..."" arise text."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	1 easy construct counterexamples sentences presented here, tagging would correct.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	He ran big hill.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	Extra information required assign correct tagging.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	stochastic method assigning part-of-speech categories unrestricted English text described.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	minimizes resources required high performance automatic tagging.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	pre-tagged training corpus required, tagger cope words found training text.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
adequate training requires processing tens thousands hundreds thousands tokens [Kupiec, 1989a].	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	1
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	model advantage pre-tagged training corpus required.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	structure state chains based analysis errors linguistic knowledge.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Examples show word dependency across phrases modeled.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Application areas include speech recognition/synthesis information retrieval.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Several workers addressed problem tagging text.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	statistical methods described terms Markov models.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	States model represent categories {cl...c=} (n number different categories used).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	word sequence considered generated underlying sequence categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	possible category sequences given word sequence generated, one maximizes probability words used.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	great advantage eliminating pre-tagged corpus.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	work described also makes use hidden Markov model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	One aim work investigate quality performance models minimal parameter descriptions.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	regard, word equivalence classes used (Kupiec, 1989).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Obviously, trade-off involved.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	However members equivalence class noun-or-verb, considered behave identically.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	local word context (embodied transition probabilities) must aid disambiguation word.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Sing.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"interesting note model performs well, despite ""noisy"" training data."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	training sentence-based, model trained using 6,000 sentences corpus.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Eight iterations Baum-Welch algorithm used.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	(If desired, tagging finer category replaced).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	basic model punctuation except sentence boundaries ignored.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	interesting observation worth noting regard words act auxiliary main verbs.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	indicates modal auxiliaries recognized natural class via pattern usage.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Extending Basic Model basic model used benchmark successive improvements.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	first addition correct treatment non-words text.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	includes hyphenation, punctuation, numbers abbreviations.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	New categories added number, abbreviation, comma.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	punctuation collapsed single new punctuation category.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Refinement Basic Categories verb states basic model found coarse.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	leaves 50% corpus training equivalence classes.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	limited context adequately model constraint present local word context.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Transition probabilities form P(Ci [ Ci1, Ci2).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	n category model requires n 3 transition probabilities.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Increasing order conditioning requires exponentially parameters.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	conditioning described uniform- possible two-category contexts modeled.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	alternative uniformly increasing order conditioning extend selectively.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Mixed higher- order context modeled introducing explicit state sequences.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	basic network augmented extra state sequences model certain category sequences detail.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	example, may consider systematic error made basic model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	model context necessary correct error, two extra states used, shown Figure 1.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Training hidden Markov model topology corrected nine instances error test data.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	important point note improving model detail manner forcibly correct error.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	actual patterns category usage must distinct language.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	state generally used several places (E.g. Figure 1.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	two noun states, two adjective states: one augmented network, basic network).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	distributions P(Eqvi Ci) considered every instance state.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	exemplified following sentence fragments: 1.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Temperatures upper mantle range apparently from.....	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	2.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	The velocity seismic waves rises to....	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	model dependency across phrase, networks shown Figure 2 used.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	previous section, corrections programmed model.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	(Approximately 19,000 sentences used train networks used example).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	disadvantage cannot share training data.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	problem could resolved tying corresponding transitions together.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	document, 142 words tagged unknown (their possible categories known).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	total 1,526 words ambiguous categories (i.e. 40% document).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"example, 9 errors 3 instances ""... well ..."" arise text."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	1 easy construct counterexamples sentences presented here, tagging would correct.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	He ran big hill.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	Extra information required assign correct tagging.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	stochastic method assigning part-of-speech categories unrestricted English text described.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	minimizes resources required high performance automatic tagging.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	pre-tagged training corpus required, tagger cope words found training text.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
"report Section 2 experiments assignment part speech words text.</S><S sid =""41"" ssid = ""41"">The effectiveness models well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985)"	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	model advantage pre-tagged training corpus required.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	structure state chains based analysis errors linguistic knowledge.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Examples show word dependency across phrases modeled.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Application areas include speech recognition/synthesis information retrieval.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Several workers addressed problem tagging text.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	statistical methods described terms Markov models.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	States model represent categories {cl...c=} (n number different categories used).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	word sequence considered generated underlying sequence categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	possible category sequences given word sequence generated, one maximizes probability words used.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	great advantage eliminating pre-tagged corpus.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	work described also makes use hidden Markov model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	One aim work investigate quality performance models minimal parameter descriptions.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	regard, word equivalence classes used (Kupiec, 1989).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Obviously, trade-off involved.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	However members equivalence class noun-or-verb, considered behave identically.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	local word context (embodied transition probabilities) must aid disambiguation word.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Sing.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"interesting note model performs well, despite ""noisy"" training data."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	training sentence-based, model trained using 6,000 sentences corpus.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Eight iterations Baum-Welch algorithm used.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	(If desired, tagging finer category replaced).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	basic model punctuation except sentence boundaries ignored.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	interesting observation worth noting regard words act auxiliary main verbs.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	indicates modal auxiliaries recognized natural class via pattern usage.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Extending Basic Model basic model used benchmark successive improvements.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	first addition correct treatment non-words text.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	includes hyphenation, punctuation, numbers abbreviations.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	New categories added number, abbreviation, comma.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	punctuation collapsed single new punctuation category.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Refinement Basic Categories verb states basic model found coarse.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	leaves 50% corpus training equivalence classes.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	limited context adequately model constraint present local word context.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Transition probabilities form P(Ci [ Ci1, Ci2).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	n category model requires n 3 transition probabilities.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Increasing order conditioning requires exponentially parameters.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	conditioning described uniform- possible two-category contexts modeled.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	alternative uniformly increasing order conditioning extend selectively.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Mixed higher- order context modeled introducing explicit state sequences.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	basic network augmented extra state sequences model certain category sequences detail.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	example, may consider systematic error made basic model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	model context necessary correct error, two extra states used, shown Figure 1.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Training hidden Markov model topology corrected nine instances error test data.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	important point note improving model detail manner forcibly correct error.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	actual patterns category usage must distinct language.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	state generally used several places (E.g. Figure 1.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	two noun states, two adjective states: one augmented network, basic network).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	distributions P(Eqvi Ci) considered every instance state.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	exemplified following sentence fragments: 1.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Temperatures upper mantle range apparently from.....	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	2.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	The velocity seismic waves rises to....	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	model dependency across phrase, networks shown Figure 2 used.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	previous section, corrections programmed model.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	(Approximately 19,000 sentences used train networks used example).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	disadvantage cannot share training data.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	problem could resolved tying corresponding transitions together.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	document, 142 words tagged unknown (their possible categories known).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	total 1,526 words ambiguous categories (i.e. 40% document).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"example, 9 errors 3 instances ""... well ..."" arise text."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	1 easy construct counterexamples sentences presented here, tagging would correct.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	He ran big hill.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	Extra information required assign correct tagging.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	stochastic method assigning part-of-speech categories unrestricted English text described.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	minimizes resources required high performance automatic tagging.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	pre-tagged training corpus required, tagger cope words found training text.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
Kupiec (1989) experimented inclusion networks model mixed-order dependencies.	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	model advantage pre-tagged training corpus required.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	structure state chains based analysis errors linguistic knowledge.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Examples show word dependency across phrases modeled.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Application areas include speech recognition/synthesis information retrieval.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Several workers addressed problem tagging text.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	statistical methods described terms Markov models.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	States model represent categories {cl...c=} (n number different categories used).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	word sequence considered generated underlying sequence categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	possible category sequences given word sequence generated, one maximizes probability words used.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	great advantage eliminating pre-tagged corpus.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	work described also makes use hidden Markov model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	One aim work investigate quality performance models minimal parameter descriptions.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	regard, word equivalence classes used (Kupiec, 1989).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Obviously, trade-off involved.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	However members equivalence class noun-or-verb, considered behave identically.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	local word context (embodied transition probabilities) must aid disambiguation word.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Sing.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"interesting note model performs well, despite ""noisy"" training data."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	training sentence-based, model trained using 6,000 sentences corpus.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Eight iterations Baum-Welch algorithm used.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	(If desired, tagging finer category replaced).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	basic model punctuation except sentence boundaries ignored.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	interesting observation worth noting regard words act auxiliary main verbs.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	indicates modal auxiliaries recognized natural class via pattern usage.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Extending Basic Model basic model used benchmark successive improvements.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	first addition correct treatment non-words text.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	includes hyphenation, punctuation, numbers abbreviations.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	New categories added number, abbreviation, comma.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	punctuation collapsed single new punctuation category.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Refinement Basic Categories verb states basic model found coarse.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	leaves 50% corpus training equivalence classes.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	limited context adequately model constraint present local word context.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Transition probabilities form P(Ci [ Ci1, Ci2).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	n category model requires n 3 transition probabilities.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Increasing order conditioning requires exponentially parameters.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	conditioning described uniform- possible two-category contexts modeled.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	alternative uniformly increasing order conditioning extend selectively.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Mixed higher- order context modeled introducing explicit state sequences.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	basic network augmented extra state sequences model certain category sequences detail.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	example, may consider systematic error made basic model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	model context necessary correct error, two extra states used, shown Figure 1.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Training hidden Markov model topology corrected nine instances error test data.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	important point note improving model detail manner forcibly correct error.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	actual patterns category usage must distinct language.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	state generally used several places (E.g. Figure 1.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	two noun states, two adjective states: one augmented network, basic network).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	distributions P(Eqvi Ci) considered every instance state.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	exemplified following sentence fragments: 1.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Temperatures upper mantle range apparently from.....	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	2.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	The velocity seismic waves rises to....	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	model dependency across phrase, networks shown Figure 2 used.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	previous section, corrections programmed model.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	(Approximately 19,000 sentences used train networks used example).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	disadvantage cannot share training data.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	problem could resolved tying corresponding transitions together.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	document, 142 words tagged unknown (their possible categories known).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	total 1,526 words ambiguous categories (i.e. 40% document).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"example, 9 errors 3 instances ""... well ..."" arise text."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	1 easy construct counterexamples sentences presented here, tagging would correct.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	He ran big hill.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	Extra information required assign correct tagging.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	stochastic method assigning part-of-speech categories unrestricted English text described.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	minimizes resources required high performance automatic tagging.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	pre-tagged training corpus required, tagger cope words found training text.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
vocabulary entry may word equivalence class based categories (Kupiec, 1989).	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	model advantage pre-tagged training corpus required.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	structure state chains based analysis errors linguistic knowledge.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Examples show word dependency across phrases modeled.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Application areas include speech recognition/synthesis information retrieval.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Several workers addressed problem tagging text.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	statistical methods described terms Markov models.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	States model represent categories {cl...c=} (n number different categories used).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	word sequence considered generated underlying sequence categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	possible category sequences given word sequence generated, one maximizes probability words used.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	great advantage eliminating pre-tagged corpus.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	work described also makes use hidden Markov model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	One aim work investigate quality performance models minimal parameter descriptions.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	regard, word equivalence classes used (Kupiec, 1989).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Obviously, trade-off involved.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	However members equivalence class noun-or-verb, considered behave identically.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	local word context (embodied transition probabilities) must aid disambiguation word.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Sing.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"interesting note model performs well, despite ""noisy"" training data."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	training sentence-based, model trained using 6,000 sentences corpus.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Eight iterations Baum-Welch algorithm used.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	(If desired, tagging finer category replaced).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	basic model punctuation except sentence boundaries ignored.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	interesting observation worth noting regard words act auxiliary main verbs.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	indicates modal auxiliaries recognized natural class via pattern usage.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Extending Basic Model basic model used benchmark successive improvements.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	first addition correct treatment non-words text.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	includes hyphenation, punctuation, numbers abbreviations.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	New categories added number, abbreviation, comma.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	punctuation collapsed single new punctuation category.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Refinement Basic Categories verb states basic model found coarse.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	leaves 50% corpus training equivalence classes.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	limited context adequately model constraint present local word context.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Transition probabilities form P(Ci [ Ci1, Ci2).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	n category model requires n 3 transition probabilities.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Increasing order conditioning requires exponentially parameters.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	conditioning described uniform- possible two-category contexts modeled.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	alternative uniformly increasing order conditioning extend selectively.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Mixed higher- order context modeled introducing explicit state sequences.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	basic network augmented extra state sequences model certain category sequences detail.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	example, may consider systematic error made basic model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	model context necessary correct error, two extra states used, shown Figure 1.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Training hidden Markov model topology corrected nine instances error test data.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	important point note improving model detail manner forcibly correct error.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	actual patterns category usage must distinct language.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	state generally used several places (E.g. Figure 1.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	two noun states, two adjective states: one augmented network, basic network).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	distributions P(Eqvi Ci) considered every instance state.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	exemplified following sentence fragments: 1.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Temperatures upper mantle range apparently from.....	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	2.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	The velocity seismic waves rises to....	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	model dependency across phrase, networks shown Figure 2 used.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	previous section, corrections programmed model.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	(Approximately 19,000 sentences used train networks used example).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	disadvantage cannot share training data.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	problem could resolved tying corresponding transitions together.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	document, 142 words tagged unknown (their possible categories known).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	total 1,526 words ambiguous categories (i.e. 40% document).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"example, 9 errors 3 instances ""... well ..."" arise text."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	1 easy construct counterexamples sentences presented here, tagging would correct.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	He ran big hill.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	Extra information required assign correct tagging.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	stochastic method assigning part-of-speech categories unrestricted English text described.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	minimizes resources required high performance automatic tagging.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	pre-tagged training corpus required, tagger cope words found training text.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
practical tagger (Kupiec, 1989), frequent 100 words lexicalized.	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	model advantage pre-tagged training corpus required.	1
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	structure state chains based analysis errors linguistic knowledge.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Examples show word dependency across phrases modeled.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Application areas include speech recognition/synthesis information retrieval.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Several workers addressed problem tagging text.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	statistical methods described terms Markov models.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	States model represent categories {cl...c=} (n number different categories used).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	word sequence considered generated underlying sequence categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	possible category sequences given word sequence generated, one maximizes probability words used.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	great advantage eliminating pre-tagged corpus.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	work described also makes use hidden Markov model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	One aim work investigate quality performance models minimal parameter descriptions.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	regard, word equivalence classes used (Kupiec, 1989).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Obviously, trade-off involved.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	However members equivalence class noun-or-verb, considered behave identically.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	local word context (embodied transition probabilities) must aid disambiguation word.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Sing.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"interesting note model performs well, despite ""noisy"" training data."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	training sentence-based, model trained using 6,000 sentences corpus.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Eight iterations Baum-Welch algorithm used.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	(If desired, tagging finer category replaced).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	basic model punctuation except sentence boundaries ignored.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	interesting observation worth noting regard words act auxiliary main verbs.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	indicates modal auxiliaries recognized natural class via pattern usage.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Extending Basic Model basic model used benchmark successive improvements.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	first addition correct treatment non-words text.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	includes hyphenation, punctuation, numbers abbreviations.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	New categories added number, abbreviation, comma.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	punctuation collapsed single new punctuation category.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Refinement Basic Categories verb states basic model found coarse.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	leaves 50% corpus training equivalence classes.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	limited context adequately model constraint present local word context.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Transition probabilities form P(Ci [ Ci1, Ci2).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	n category model requires n 3 transition probabilities.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Increasing order conditioning requires exponentially parameters.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	conditioning described uniform- possible two-category contexts modeled.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	alternative uniformly increasing order conditioning extend selectively.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Mixed higher- order context modeled introducing explicit state sequences.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	basic network augmented extra state sequences model certain category sequences detail.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	example, may consider systematic error made basic model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	model context necessary correct error, two extra states used, shown Figure 1.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Training hidden Markov model topology corrected nine instances error test data.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	important point note improving model detail manner forcibly correct error.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	actual patterns category usage must distinct language.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	state generally used several places (E.g. Figure 1.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	two noun states, two adjective states: one augmented network, basic network).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	distributions P(Eqvi Ci) considered every instance state.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	exemplified following sentence fragments: 1.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Temperatures upper mantle range apparently from.....	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	2.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	The velocity seismic waves rises to....	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	model dependency across phrase, networks shown Figure 2 used.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	previous section, corrections programmed model.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	(Approximately 19,000 sentences used train networks used example).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	disadvantage cannot share training data.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	problem could resolved tying corresponding transitions together.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	document, 142 words tagged unknown (their possible categories known).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	total 1,526 words ambiguous categories (i.e. 40% document).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"example, 9 errors 3 instances ""... well ..."" arise text."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	1 easy construct counterexamples sentences presented here, tagging would correct.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	He ran big hill.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	Extra information required assign correct tagging.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	stochastic method assigning part-of-speech categories unrestricted English text described.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	minimizes resources required high performance automatic tagging.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	pre-tagged training corpus required, tagger cope words found training text.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
parameters model estimated tagged (1, 3, 4, 6, 12] untagged [2, 9, 11] text.	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	model advantage pre-tagged training corpus required.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	structure state chains based analysis errors linguistic knowledge.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Examples show word dependency across phrases modeled.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Application areas include speech recognition/synthesis information retrieval.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Several workers addressed problem tagging text.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	statistical methods described terms Markov models.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	States model represent categories {cl...c=} (n number different categories used).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	word sequence considered generated underlying sequence categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	possible category sequences given word sequence generated, one maximizes probability words used.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	great advantage eliminating pre-tagged corpus.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	work described also makes use hidden Markov model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	One aim work investigate quality performance models minimal parameter descriptions.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	regard, word equivalence classes used (Kupiec, 1989).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Obviously, trade-off involved.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	However members equivalence class noun-or-verb, considered behave identically.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	local word context (embodied transition probabilities) must aid disambiguation word.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Sing.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"interesting note model performs well, despite ""noisy"" training data."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	training sentence-based, model trained using 6,000 sentences corpus.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Eight iterations Baum-Welch algorithm used.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	(If desired, tagging finer category replaced).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	basic model punctuation except sentence boundaries ignored.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	interesting observation worth noting regard words act auxiliary main verbs.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	indicates modal auxiliaries recognized natural class via pattern usage.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Extending Basic Model basic model used benchmark successive improvements.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	first addition correct treatment non-words text.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	includes hyphenation, punctuation, numbers abbreviations.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	New categories added number, abbreviation, comma.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	punctuation collapsed single new punctuation category.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Refinement Basic Categories verb states basic model found coarse.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	leaves 50% corpus training equivalence classes.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	limited context adequately model constraint present local word context.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Transition probabilities form P(Ci [ Ci1, Ci2).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	n category model requires n 3 transition probabilities.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Increasing order conditioning requires exponentially parameters.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	conditioning described uniform- possible two-category contexts modeled.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	alternative uniformly increasing order conditioning extend selectively.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Mixed higher- order context modeled introducing explicit state sequences.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	basic network augmented extra state sequences model certain category sequences detail.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	example, may consider systematic error made basic model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	model context necessary correct error, two extra states used, shown Figure 1.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Training hidden Markov model topology corrected nine instances error test data.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	important point note improving model detail manner forcibly correct error.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	actual patterns category usage must distinct language.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	state generally used several places (E.g. Figure 1.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	two noun states, two adjective states: one augmented network, basic network).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	distributions P(Eqvi Ci) considered every instance state.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	exemplified following sentence fragments: 1.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Temperatures upper mantle range apparently from.....	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	2.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	The velocity seismic waves rises to....	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	model dependency across phrase, networks shown Figure 2 used.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	previous section, corrections programmed model.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	(Approximately 19,000 sentences used train networks used example).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	disadvantage cannot share training data.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	problem could resolved tying corresponding transitions together.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	document, 142 words tagged unknown (their possible categories known).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	total 1,526 words ambiguous categories (i.e. 40% document).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"example, 9 errors 3 instances ""... well ..."" arise text."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	1 easy construct counterexamples sentences presented here, tagging would correct.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	He ran big hill.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	Extra information required assign correct tagging.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	stochastic method assigning part-of-speech categories unrestricted English text described.	1
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	minimizes resources required high performance automatic tagging.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	pre-tagged training corpus required, tagger cope words found training text.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
One area statistical approach done par­ ticularly well automatic part speech tagging, as­ signing word input sentence proper part speech (1, 2, 3, 4, 6, 9, 11, 12].	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	model advantage pre-tagged training corpus required.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	structure state chains based analysis errors linguistic knowledge.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Examples show word dependency across phrases modeled.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Application areas include speech recognition/synthesis information retrieval.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Several workers addressed problem tagging text.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	statistical methods described terms Markov models.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	States model represent categories {cl...c=} (n number different categories used).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	word sequence considered generated underlying sequence categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	possible category sequences given word sequence generated, one maximizes probability words used.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	great advantage eliminating pre-tagged corpus.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	work described also makes use hidden Markov model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	One aim work investigate quality performance models minimal parameter descriptions.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	regard, word equivalence classes used (Kupiec, 1989).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Obviously, trade-off involved.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	However members equivalence class noun-or-verb, considered behave identically.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	local word context (embodied transition probabilities) must aid disambiguation word.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Sing.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"interesting note model performs well, despite ""noisy"" training data."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	training sentence-based, model trained using 6,000 sentences corpus.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Eight iterations Baum-Welch algorithm used.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	(If desired, tagging finer category replaced).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	basic model punctuation except sentence boundaries ignored.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	interesting observation worth noting regard words act auxiliary main verbs.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	indicates modal auxiliaries recognized natural class via pattern usage.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Extending Basic Model basic model used benchmark successive improvements.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	first addition correct treatment non-words text.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	includes hyphenation, punctuation, numbers abbreviations.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	New categories added number, abbreviation, comma.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	punctuation collapsed single new punctuation category.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Refinement Basic Categories verb states basic model found coarse.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	leaves 50% corpus training equivalence classes.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	limited context adequately model constraint present local word context.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Transition probabilities form P(Ci [ Ci1, Ci2).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	n category model requires n 3 transition probabilities.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Increasing order conditioning requires exponentially parameters.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	conditioning described uniform- possible two-category contexts modeled.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	alternative uniformly increasing order conditioning extend selectively.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Mixed higher- order context modeled introducing explicit state sequences.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	basic network augmented extra state sequences model certain category sequences detail.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	example, may consider systematic error made basic model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	model context necessary correct error, two extra states used, shown Figure 1.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Training hidden Markov model topology corrected nine instances error test data.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	important point note improving model detail manner forcibly correct error.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	actual patterns category usage must distinct language.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	state generally used several places (E.g. Figure 1.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	two noun states, two adjective states: one augmented network, basic network).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	distributions P(Eqvi Ci) considered every instance state.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	exemplified following sentence fragments: 1.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Temperatures upper mantle range apparently from.....	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	2.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	The velocity seismic waves rises to....	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	model dependency across phrase, networks shown Figure 2 used.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	previous section, corrections programmed model.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	(Approximately 19,000 sentences used train networks used example).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	disadvantage cannot share training data.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	problem could resolved tying corresponding transitions together.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	document, 142 words tagged unknown (their possible categories known).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	total 1,526 words ambiguous categories (i.e. 40% document).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"example, 9 errors 3 instances ""... well ..."" arise text."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	1 easy construct counterexamples sentences presented here, tagging would correct.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	He ran big hill.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	Extra information required assign correct tagging.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	stochastic method assigning part-of-speech categories unrestricted English text described.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	minimizes resources required high performance automatic tagging.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	pre-tagged training corpus required, tagger cope words found training text.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
Instead, common words represented individually; rest words dictionary partitioned word equivalence classes (Kupiec, 1989)	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Augmenting Hidden Markov Model Phrase-Dependent Word Tagging	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	paper describes refinements currently investigated model part-of-speech assignment words unrestricted text.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	model advantage pre-tagged training corpus required.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Words represented equivalence classes reduce number parameters required provide essentially vocabulary-independent model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	State chains used model selective higher-order conditioning model, obviates proliferation parameters attendant uniformly higher-order models.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	structure state chains based analysis errors linguistic knowledge.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Examples show word dependency across phrases modeled.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"determination part-of-speech categories words important problem language modeling, syntactic semantic roles words depend part-of-speech category (henceforth simply termed ""category"")."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Application areas include speech recognition/synthesis information retrieval.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Several workers addressed problem tagging text.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Methods ranged locally-operating rules (Greene Rubin, 1971), statistical methods (Church, 1989; DeRose, 1988; Garside, Leech Sampson, 1987; Jelinek, 1985) back-propagation (Benello, Mackie Anderson, 1989; Nakamura Shikano, 1989).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	statistical methods described terms Markov models.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	States model represent categories {cl...c=} (n number different categories used).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	first order model, Ci Ci_l random variables denoting categories words position (i - 1) text.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	transition probability P(Ci = cz ] Ci_~ = %) linking two states cz cy, represents probability category cx following category %.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	word position represented random variable Wi, ranges vocabulary {w~ ...wv} (v number words vocabulary).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	State-dependent probabilities form P(Wi = Wa ] Ci = cz) represent probability word Wa seen, given category c~.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"instance, word ""dog"" seen states noun verb, nonzero probability states."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	word sequence considered generated underlying sequence categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	possible category sequences given word sequence generated, one maximizes probability words used.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Viterbi algorithm (Viterbi, 1967) find category sequence.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	systems previously mentioned require pre-tagged training corpus order collect word counts perform back-propagation.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Brown Corpus (Francis Kucera, 1982) notable example corpus, used many systems cited above.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"alternative approach taken Jelinek, (Jelinek, 1985) view training problem terms ""hidden"" Markov model: is, words training text available, corresponding categories known."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	situation, Baum-Welch algorithm (Baum, 1972) used estimate model parameters.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	great advantage eliminating pre-tagged corpus.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	minimizes resources required, facilitates experimentation different word categories, easily adapted use languages.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	work described also makes use hidden Markov model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	One aim work investigate quality performance models minimal parameter descriptions.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	regard, word equivalence classes used (Kupiec, 1989).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	assumed distribution use word depends set categories assume, words partitioned accordingly.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"Thus words ""play"" ""touch"" considered behave identically, members class noun-or-verb, ""clay"" ""zinc""are members class noun."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	partitioning drastically reduces number parameters required model, aids reliable estimation using moderate amounts training data.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Equivalence classes {Eqvl ...Eqvm} replace words {wl...Wv} (m << v) P(Eqvi Ci) replace parameters P(Wi Ci).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	21 category model reported Kupiec (1989) 129 equivalence classes required cover 30,000 word dictionary.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	fact, number equivalence classes essentially independent size dictionary, enabling new words added without modification model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Obviously, trade-off involved.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"example, ""dog"" likely noun verb ""see"" likely verb noun."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	However members equivalence class noun-or-verb, considered behave identically.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	local word context (embodied transition probabilities) must aid disambiguation word.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	practice, word context provides significant constraint, trade-off appears remarkably favorable one.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Basic Model development model guided evaluation simple basic model (much development model prompted analysis errors hehaviour).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	basic model contained states representing following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating subordinating Preposition Adjective Including comparative superlative Verb Uninflected Verb 3rd Pers.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Sing.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could found dictionary.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Lisp Used tag common symbols Lisp programming language (see below:) To-inf.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	To acting infinitive marker Sentence Boundary states arranged first-order, fully connected network, state transition every state, allowing possible sequences categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	training corpus collection electronic mail messages concerning design Common-Lisp programming language -a somewhat less ideal representation English.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Many Lisp-specific words vocabulary, thus tagged unknown, however lisp category nevertheless created frequently occurring Lisp symbols attempt reduce bias estimation.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"interesting note model performs well, despite ""noisy"" training data."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	training sentence-based, model trained using 6,000 sentences corpus.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Eight iterations Baum-Welch algorithm used.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	implementation hidden Markov model based Rabiner, Levinson Sondhi (1983).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	exploiting fact matrix probabilities P(Eqvi Ci) sparse, considerable improvement gained basic training algorithm iterations made states.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	initial values model parameters calculated word occurrence probabilities, words initially assumed function equally probably possible categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Superlative comparative adjectives collapsed single adjective category, economize overall number categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	(If desired, tagging finer category replaced).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	basic model punctuation except sentence boundaries ignored.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	interesting observation worth noting regard words act auxiliary main verbs.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"Modal auxiliaries consistently tagged auxiliary whereas tagging auxiliaries (e.g. ""is .... have"" etc.) variable."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	indicates modal auxiliaries recognized natural class via pattern usage.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Extending Basic Model basic model used benchmark successive improvements.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	first addition correct treatment non-words text.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	includes hyphenation, punctuation, numbers abbreviations.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	New categories added number, abbreviation, comma.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	punctuation collapsed single new punctuation category.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Refinement Basic Categories verb states basic model found coarse.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	example, many noun/verb ambiguities front past participles incorrectly tagged verbs.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	replacement auxiliary category following categories greatly improved this: Category Name Words included Category Have* has, have, had, be* is, am, are, was, do* do, does, modal Modal auxiliaries Unique Equivalence Classes Common Words Common words occur often enough estimated reliably.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	ranked list words corpus frequent 100 words account approximately 50% total tokens corpus, thus data available estimate reliably.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	frequent 100 words corpus assigned individually model, thereby enabling different distributions categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	leaves 50% corpus training equivalence classes.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"Editing Transition Structure common error basic model assignment word ""to"" to-infcategory (""to"" acting infinitive marker) instead preposition noun phrases."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"surprising, ""to"" member to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"contrast, P(Wi = ""to"" Ci = preposition) = 0.086, many words share preposition state."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Unless transition probabilities highly constraining, higher probability paths tend go to-infstate.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	situation may addressed several ways, simplest initially assign zero transition probabilities to-infstate states verbs adverb state.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"ADJECTIVE DETERMINER states NOUN Basic Network ""Transitions  states states Basic Network Basic Network except NOUN ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING STATES EXCEPT DETERMINER Figure 1: Extending Basic Model Augmenting Model Use Networks basic model consists first-order fully connected network."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	lexical context available modeling word's category solely category preceding word (expressed via transition probabilities P(Ci [ Ci1).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	limited context adequately model constraint present local word context.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	straightforward method extending context use second-order conditioning takes account previous two word categories.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Transition probabilities form P(Ci [ Ci1, Ci2).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	n category model requires n 3 transition probabilities.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Increasing order conditioning requires exponentially parameters.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	practice, models limited second-order, smoothing methods normally required deal problem estimation limited data.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	conditioning described uniform- possible two-category contexts modeled.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Many neither contribute performance model, occur frequently enough estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	alternative uniformly increasing order conditioning extend selectively.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Mixed higher- order context modeled introducing explicit state sequences.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	arrangement basic first-order network remains, permitting possible category sequences, modeling first-order dependency.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	basic network augmented extra state sequences model certain category sequences detail.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	design augmented network based linguistic considerations also upon analysis tagging errors made basic network.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	example, may consider systematic error made basic model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	concerns disambiguation equivalence class adjective-or-noun following determiner.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"error exemplified sentence fragment ""The period of..."", ""period"" tagged adjective."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	model context necessary correct error, two extra states used, shown Figure 1.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	augmented network uniquely models second-order dependencies type determiner -noun - X, determiner -adjective -X (X ranges {cl...cn}).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Training hidden Markov model topology corrected nine instances error test data.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	important point note improving model detail manner forcibly correct error.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	actual patterns category usage must distinct language.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	95 complete description augmented model necessary mention tying model states (Jelinek Mercer, 1980).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Whenever transition made state, state-dependent probability distribution P(Eqvi Ci) used obtain probability observed equivalence class.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	state generally used several places (E.g. Figure 1.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	two noun states, two adjective states: one augmented network, basic network).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	distributions P(Eqvi Ci) considered every instance state.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	estimates pooled reassigned identically iteration Baum-Welch algorithm.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Modeling Dependencies across Phrases Linguistic considerations used correct errors made model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	section two illustrations given, concerning simple subject/verb agreement across intermediate prepositional phrase.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	exemplified following sentence fragments: 1.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Temperatures upper mantle range apparently from.....	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	2.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	The velocity seismic waves rises to....	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"basic model tagged sentences correctly, except for- ""range"" ""rises"" tagged noun plural-noun respectively 1."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	basic network cannot model dependency number verb subject, precedes prepositional phrase.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	model dependency across phrase, networks shown Figure 2 used.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	seen simple forms prepositional phrase modeled networks; single noun may optionally preceded single adjective and/or determiner.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	final transitions networks serve discriminate correct incorrect category assignment given selected preceding context.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	previous section, corrections programmed model.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	context supplied aid training procedure, latter responsible deciding alternative likely, based training data.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	(Approximately 19,000 sentences used train networks used example).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Discussion Results Figure 2, two copies prepositional phrase trained separate contexts (preceding singu- lax/plural nouns).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	disadvantage cannot share training data.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	problem could resolved tying corresponding transitions together.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Alternatively, investigation trainable grammar (Baker, 1979; Fujisaki et al., 1989) may fruitful way develop model terms grammatical components.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	model containing refinements described, tested using magazine article containing 146 sentences (3,822 words).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	30,000 word dictionary used, supplemented inflectional analysis words found directly dictionary.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	document, 142 words tagged unknown (their possible categories known).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	total 1,526 words ambiguous categories (i.e. 40% document).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Critical examination tagging provided augmented model showed 168 word tagging errors, whereas basic model gave 215 erroneous word tags.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	former represents 95.6% correct word tagging text whole (ignoring unknown words), 89% ambiguous words.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	performance tagging program depends choice number categories used, correct tag assignment words always obvious.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	cases choice tag unclear (as often occurs idioms), tag ruled incorrect.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"example, 9 errors 3 instances ""... well ..."" arise text."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	would appropriate deal idioms separately, done Gaxside, Leech Sampson (1987).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Typical errors beyond scope model described exemplified incorrect adverbial prepositional assignment.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	1 easy construct counterexamples sentences presented here, tagging would correct.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	However, training procedure affirms counterexamples occur less frequently corpus cases shown here..	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	96 NOUN PREPOSITION ADJECTIVE UN~ PLURAL NOUN PLURAL NOUN PREPOSITION E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"SINGULAR STATES BASIC NETWORK SHOWN Figure 2: Augmented Networks Example Subject/Verb Agreement example, consider word ""up"" following sentences: ""He ran big bill""."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	He ran big hill.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	Extra information required assign correct tagging.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	"examples worth noting even model based individual words, trained pre-tagged corpus, association ""up"" (as adverb) ""bill"" would captured trigrams."	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	(Work phrasal verbs, using mutual information estimates (Church et ai., 1989b) directly relevant problem).	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	tagger could extended category refinements (e.g. inclusion gerund category), single pronoun category currently causes erroneous tags adjacent words.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	respect problem unknown words, alternative category assignments could made using context embodied transition probabilities.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	stochastic method assigning part-of-speech categories unrestricted English text described.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	minimizes resources required high performance automatic tagging.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	pre-tagged training corpus required, tagger cope words found training text.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	trained reliably moderate amounts training text, use selectively augmented networks model high-order dependencies without requiring excessive number parameters.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	would like thank Meg Withgott Lanri Karttunen Xerox PARC, helpful contributions work.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	also indebted Sheldon Nicholl Univ. Illinois, comments valuable insight.	0
empirical approach adopted almost contemporary part-of-speech programs: Bahl Mer­ cer (1976), Leech, Garside, Atwell (1983), Jelinek (1985), Deroualt Merialdo (1986), Garside, Leech, Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.	work sponsored part Defense Advanced Research Projects Agency (DOD), Information Science Technology Office, contract #N0014086-C-8996.	0
