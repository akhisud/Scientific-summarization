"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Simple Type-Level Unsupervised POS Tagging	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, existing systems, expansion come steep increase model complexity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	— similar results observed across multiple languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design, readily capture regularities token-level.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Across languages, high performance attained selecting single tag per word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	token-level HMM reflect lexicon sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	two key benefits model architecture.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	several languages, report performance exceeding state-of-the art systems.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	extent constraint enforced varies greatly across existing methods.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	clusters computed using SVD variant without relying transitional structure.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	approaches encode sparsity soft constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design guarantee “structural zeros,” biases towards sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast approaches, method directly incorporates constraints structure model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design leads significant reduction computational complexity training inference.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	standard, use fixed constant K number tagging states.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Conditioned , features word types W drawn.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	parameters depend single hyperparameter α.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	total O(K 2) parameters associated transition parameters.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3.1 Lexicon Component.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	purpose 3 follows since θt St − 1 parameters and.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	set fixed constants.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	explore well induce POS tags using one-tag-per-word constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model, associate features type-level lexicon.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	i=1 (f,v)∈Wi	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	inference, interested posterior probability latent variables model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Performance typically stabilizes across languages number iterations.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	t(i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	use w erations sampling (see Figure 2 depiction).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	language investigate contribution component model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	languages make use tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 62.6 45.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 61.7 37.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 56.2 32.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 53.8 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 53.7 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 61.0 44.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 62.2 39.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 68.4 49.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 68.4 48.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 68.1 34.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 54.4 33.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	36.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 55.3 34.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 50.2 +P RI st dia n 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 65.5 46.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 64.7 42.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 58.3 40.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 57.3 51.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 65.9 48.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 60.7 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	41.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	7 68.3 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 70.7 52.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 70.9 42.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	37.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 55.8 38.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	36.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 57.3 +F EA TS st dia n 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 66.4 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 66.4 52.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 61.2 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 60.7 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 69.0 51.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 67.3 55.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 70.4 46.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 61.7 64.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 74.5 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 70.1 58.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 68.9 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 57.2 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 61.7 38.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second row represents performance median hyperparameter setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	See Section 5.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5.1 Data Sets.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	train test CoNLL-X training set.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Statistics data sets shown Table 2.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5.2 Setup.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	final model tions.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	perform five runs different random initialization sampling state.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	report results best median hyperparameter settings obtained way.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, settings report results median run setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	See Table 2 tag set size languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	exception Dutch data set, processing performed annotated tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	6 Results Analysis.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	system Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) reports best unsupervised results English.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	consider two variants Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	instance, Spanish, absolute gap median performance 10%.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second point comparison Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	compare Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009) Portuguese (Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, full model takes advantage word features present Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	novel element model ability capture type-level tag frequencies.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Similar behavior observed adding features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 1 0.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 2 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 1 2.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 1 8.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	7 Conclusion Future Work.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	resulting model compact, efficiently learnable linguistically expressive.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	paper, make simplifying assumption one-tag-per-word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	assumption, however, inherent type-based tagging models.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	promising direction future work explicitly model distribution tags word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	thank members MIT NLP group suggestions comments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Simple Type-Level Unsupervised POS Tagging	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, existing systems, expansion come steep increase model complexity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	— similar results observed across multiple languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design, readily capture regularities token-level.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Across languages, high performance attained selecting single tag per word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	token-level HMM reflect lexicon sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	two key benefits model architecture.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	several languages, report performance exceeding state-of-the art systems.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	extent constraint enforced varies greatly across existing methods.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	clusters computed using SVD variant without relying transitional structure.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	approaches encode sparsity soft constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design guarantee “structural zeros,” biases towards sparsity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast approaches, method directly incorporates constraints structure model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	design leads significant reduction computational complexity training inference.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	standard, use fixed constant K number tagging states.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Conditioned , features word types W drawn.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	parameters depend single hyperparameter α.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	total O(K 2) parameters associated transition parameters.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3.1 Lexicon Component.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	purpose 3 follows since θt St − 1 parameters and.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	set fixed constants.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	explore well induce POS tags using one-tag-per-word constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model, associate features type-level lexicon.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	i=1 (f,v)∈Wi	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	inference, interested posterior probability latent variables model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Performance typically stabilizes across languages number iterations.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	t(i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	use w erations sampling (see Figure 2 depiction).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	language investigate contribution component model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	languages make use tagging dictionary.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 62.6 45.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 61.7 37.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 56.2 32.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 53.8 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 53.7 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 61.0 44.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 62.2 39.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 68.4 49.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 68.4 48.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 68.1 34.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 54.4 33.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	36.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 55.3 34.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 50.2 +P RI st dia n 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 65.5 46.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 64.7 42.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 58.3 40.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 57.3 51.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 65.9 48.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 60.7 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	41.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	7 68.3 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 70.7 52.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 70.9 42.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	37.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 55.8 38.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	36.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 57.3 +F EA TS st dia n 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	9 66.4 47.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 66.4 52.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 61.2 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 60.7 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 69.0 51.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 67.3 55.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 70.4 46.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	2 61.7 64.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 74.5 56.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 70.1 58.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 68.9 50.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	0 57.2 43.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	3 61.7 38.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second row represents performance median hyperparameter setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	See Section 5.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5.1 Data Sets.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	train test CoNLL-X training set.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Statistics data sets shown Table 2.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5.2 Setup.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	final model tions.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	perform five runs different random initialization sampling state.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	report results best median hyperparameter settings obtained way.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Specifically, settings report results median run setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	See Table 2 tag set size languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	exception Dutch data set, processing performed annotated tags.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	6 Results Analysis.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	system Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) reports best unsupervised results English.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	consider two variants Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	instance, Spanish, absolute gap median performance 10%.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	second point comparison Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	compare Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009) Portuguese (Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	However, full model takes advantage word features present Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	(2009).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	novel element model ability capture type-level tag frequencies.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Similar behavior observed adding features.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 1 0.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	1 2 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 1 2.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	8 1 8.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	7 Conclusion Future Work.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	resulting model compact, efficiently learnable linguistically expressive.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	paper, make simplifying assumption one-tag-per-word.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	assumption, however, inherent type-based tagging models.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	promising direction future work explicitly model distribution tags word type.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	thank members MIT NLP group suggestions comments.	0
"Following Lee et al.</S><S sid =""264"" ssid = ""45"">(2010), report best median settings hyperparameters based F- score, addition inferred values."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	— similar results observed across multiple languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design, readily capture regularities token-level.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	two key benefits model architecture.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approaches encode sparsity soft constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Conditioned , features word types W drawn.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	parameters depend single hyperparameter α.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3.1 Lexicon Component.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	set fixed constants.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model, associate features type-level lexicon.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	i=1 (f,v)∈Wi	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	t(i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language investigate contribution component model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages make use tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.6 45.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.7 37.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 56.2 32.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 53.8 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 53.7 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 61.0 44.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.2 39.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.4 49.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 68.4 48.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 68.1 34.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 54.4 33.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 55.3 34.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 65.5 46.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 64.7 42.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 58.3 40.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.3 51.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 65.9 48.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 60.7 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	41.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 68.3 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 70.7 52.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 70.9 42.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	37.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 55.8 38.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 66.4 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 66.4 52.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.2 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 60.7 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 69.0 51.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 67.3 55.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 70.4 46.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 61.7 64.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 74.5 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 70.1 58.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.9 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.2 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 61.7 38.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Section 5.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.1 Data Sets.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	train test CoNLL-X training set.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Statistics data sets shown Table 2.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.2 Setup.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	final model tions.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	perform five runs different random initialization sampling state.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, settings report results median run setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Table 2 tag set size languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	6 Results Analysis.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) reports best unsupervised results English.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second point comparison Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	compare Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Similar behavior observed adding features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 1 0.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 2 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 2.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 8.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 Conclusion Future Work.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	1
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	— similar results observed across multiple languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design, readily capture regularities token-level.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	two key benefits model architecture.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approaches encode sparsity soft constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Conditioned , features word types W drawn.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	parameters depend single hyperparameter α.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3.1 Lexicon Component.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	set fixed constants.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model, associate features type-level lexicon.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	i=1 (f,v)∈Wi	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	t(i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language investigate contribution component model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages make use tagging dictionary.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.6 45.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.7 37.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 56.2 32.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 53.8 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 53.7 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 61.0 44.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.2 39.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.4 49.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 68.4 48.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 68.1 34.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 54.4 33.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 55.3 34.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 65.5 46.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 64.7 42.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 58.3 40.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.3 51.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 65.9 48.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 60.7 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	41.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 68.3 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 70.7 52.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 70.9 42.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	37.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 55.8 38.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 66.4 47.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 66.4 52.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.2 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 60.7 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 69.0 51.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 67.3 55.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 70.4 46.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 61.7 64.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 74.5 56.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 70.1 58.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.9 50.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.2 43.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 61.7 38.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Section 5.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.1 Data Sets.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	train test CoNLL-X training set.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Statistics data sets shown Table 2.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.2 Setup.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	final model tions.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	perform five runs different random initialization sampling state.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, settings report results median run setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Table 2 tag set size languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	6 Results Analysis.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) reports best unsupervised results English.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Berg-Kirkpatrick et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second point comparison Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	compare Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Similar behavior observed adding features.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 1 0.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 2 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 2.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 8.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 Conclusion Future Work.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	1
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
However, despite recent proliferation syntactic class induction systems (Biemann, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Simple Type-Level Unsupervised POS Tagging	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, existing systems, expansion come steep increase model complexity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	several languages, report performance exceeding complex state-of-the art systems.1	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	— similar results observed across multiple languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design, readily capture regularities token-level.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work, take direct approach treat word type allowed POS tags primary element model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	1
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Across languages, high performance attained selecting single tag per word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	token-level HMM reflect lexicon sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	two key benefits model architecture.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	several languages, report performance exceeding state-of-the art systems.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	extent constraint enforced varies greatly across existing methods.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	clusters computed using SVD variant without relying transitional structure.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	approaches encode sparsity soft constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design guarantee “structural zeros,” biases towards sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	use ILP learning desired grammar significantly increases computational complexity method.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast approaches, method directly incorporates constraints structure model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design leads significant reduction computational complexity training inference.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	consider unsupervised POS induction problem without use tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	graphical depiction model well summary random variables parameters found Figure 1.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	standard, use fixed constant K number tagging states.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Conditioned , features word types W drawn.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	parameters depend single hyperparameter α.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	total O(K 2) parameters associated transition parameters.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3.1 Lexicon Component.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	purpose 3 follows since θt St − 1 parameters and.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	set fixed constants.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	explore well induce POS tags using one-tag-per-word constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model, associate features type-level lexicon.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	i=1 (f,v)∈Wi	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	inference, interested posterior probability latent variables model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Performance typically stabilizes across languages number iterations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	t(i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	use w erations sampling (see Figure 2 depiction).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	language investigate contribution component model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	languages make use tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 62.6 45.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 61.7 37.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 56.2 32.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 53.8 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 53.7 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 61.0 44.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 62.2 39.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 68.4 49.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 68.4 48.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 68.1 34.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 54.4 33.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	36.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 55.3 34.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 50.2 +P RI st dia n 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 65.5 46.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 64.7 42.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 58.3 40.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 57.3 51.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 65.9 48.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 60.7 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	41.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	7 68.3 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 70.7 52.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 70.9 42.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	37.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 55.8 38.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	36.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 57.3 +F EA TS st dia n 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 66.4 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 66.4 52.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 61.2 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 60.7 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 69.0 51.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 67.3 55.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 70.4 46.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 61.7 64.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 74.5 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 70.1 58.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 68.9 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 57.2 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 61.7 38.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second row represents performance median hyperparameter setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	See Section 5.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5.1 Data Sets.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	train test CoNLL-X training set.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Statistics data sets shown Table 2.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5.2 Setup.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	encodes one tag per word constraint uni form type-level tag assignments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	final model tions.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	perform five runs different random initialization sampling state.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	report results best median hyperparameter settings obtained way.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, settings report results median run setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	tokenize MWUs POS tags; reduces tag set size 12.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	See Table 2 tag set size languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	exception Dutch data set, processing performed annotated tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	6 Results Analysis.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	report token- type-level accuracy Table 3 6 languages system settings.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	system Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) reports best unsupervised results English.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	consider two variants Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	instance, Spanish, absolute gap median performance 10%.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second point comparison Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	compare Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009) Portuguese (Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, full model takes advantage word features present Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	novel element model ability capture type-level tag frequencies.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Similar behavior observed adding features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 1 0.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 2 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 1 2.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 1 8.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	7 Conclusion Future Work.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	resulting model compact, efficiently learnable linguistically expressive.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	paper, make simplifying assumption one-tag-per-word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	assumption, however, inherent type-based tagging models.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	promising direction future work explicitly model distribution tags word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	thank members MIT NLP group suggestions comments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Simple Type-Level Unsupervised POS Tagging	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, existing systems, expansion come steep increase model complexity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	several languages, report performance exceeding complex state-of-the art systems.1	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	— similar results observed across multiple languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design, readily capture regularities token-level.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work, take direct approach treat word type allowed POS tags primary element model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	1
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Across languages, high performance attained selecting single tag per word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	token-level HMM reflect lexicon sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	two key benefits model architecture.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	several languages, report performance exceeding state-of-the art systems.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	extent constraint enforced varies greatly across existing methods.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	clusters computed using SVD variant without relying transitional structure.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	approaches encode sparsity soft constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design guarantee “structural zeros,” biases towards sparsity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	use ILP learning desired grammar significantly increases computational complexity method.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast approaches, method directly incorporates constraints structure model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	design leads significant reduction computational complexity training inference.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	consider unsupervised POS induction problem without use tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	graphical depiction model well summary random variables parameters found Figure 1.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	standard, use fixed constant K number tagging states.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Conditioned , features word types W drawn.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	parameters depend single hyperparameter α.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	total O(K 2) parameters associated transition parameters.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3.1 Lexicon Component.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	purpose 3 follows since θt St − 1 parameters and.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	set fixed constants.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	explore well induce POS tags using one-tag-per-word constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model, associate features type-level lexicon.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	i=1 (f,v)∈Wi	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	inference, interested posterior probability latent variables model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Performance typically stabilizes across languages number iterations.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	t(i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	use w erations sampling (see Figure 2 depiction).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	language investigate contribution component model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	languages make use tagging dictionary.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 62.6 45.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 61.7 37.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 56.2 32.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 53.8 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 53.7 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 61.0 44.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 62.2 39.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 68.4 49.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 68.4 48.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 68.1 34.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 54.4 33.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	36.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 55.3 34.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 50.2 +P RI st dia n 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 65.5 46.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 64.7 42.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 58.3 40.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 57.3 51.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 65.9 48.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 60.7 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	41.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	7 68.3 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 70.7 52.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 70.9 42.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	37.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 55.8 38.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	36.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 57.3 +F EA TS st dia n 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	9 66.4 47.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 66.4 52.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 61.2 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 60.7 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 69.0 51.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 67.3 55.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 70.4 46.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	2 61.7 64.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 74.5 56.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 70.1 58.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 68.9 50.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	0 57.2 43.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	3 61.7 38.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second row represents performance median hyperparameter setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	See Section 5.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5.1 Data Sets.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	train test CoNLL-X training set.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Statistics data sets shown Table 2.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5.2 Setup.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	encodes one tag per word constraint uni form type-level tag assignments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	final model tions.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	perform five runs different random initialization sampling state.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	report results best median hyperparameter settings obtained way.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Specifically, settings report results median run setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	tokenize MWUs POS tags; reduces tag set size 12.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	See Table 2 tag set size languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	exception Dutch data set, processing performed annotated tags.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	6 Results Analysis.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	report token- type-level accuracy Table 3 6 languages system settings.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	system Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) reports best unsupervised results English.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	consider two variants Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Berg-Kirkpatrick et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	instance, Spanish, absolute gap median performance 10%.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	second point comparison Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	compare Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009) Portuguese (Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	However, full model takes advantage word features present Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	(2009).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	novel element model ability capture type-level tag frequencies.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Similar behavior observed adding features.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 1 0.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	1 2 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 1 2.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	8 1 8.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	7 Conclusion Future Work.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	resulting model compact, efficiently learnable linguistically expressive.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	paper, make simplifying assumption one-tag-per-word.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	assumption, however, inherent type-based tagging models.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	promising direction future work explicitly model distribution tags word type.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	thank members MIT NLP group suggestions comments.	0
"property strictly true linguistic data, good approximation: Lee et al.</S><S sid =""11"" ssid = ""11"">(2010) note, assigning word type frequent part speech yields upper bound accuracy 93% languages."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Simple Type-Level Unsupervised POS Tagging	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, existing systems, expansion come steep increase model complexity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	several languages, report performance exceeding complex state-of-the art systems.1	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	— similar results observed across multiple languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design, readily capture regularities token-level.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work, take direct approach treat word type allowed POS tags primary element model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Across languages, high performance attained selecting single tag per word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	token-level HMM reflect lexicon sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	two key benefits model architecture.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	several languages, report performance exceeding state-of-the art systems.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	extent constraint enforced varies greatly across existing methods.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	clusters computed using SVD variant without relying transitional structure.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	approaches encode sparsity soft constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design guarantee “structural zeros,” biases towards sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	use ILP learning desired grammar significantly increases computational complexity method.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast approaches, method directly incorporates constraints structure model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design leads significant reduction computational complexity training inference.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	consider unsupervised POS induction problem without use tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	graphical depiction model well summary random variables parameters found Figure 1.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	standard, use fixed constant K number tagging states.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Conditioned , features word types W drawn.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	parameters depend single hyperparameter α.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	total O(K 2) parameters associated transition parameters.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3.1 Lexicon Component.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	purpose 3 follows since θt St − 1 parameters and.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	set fixed constants.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	explore well induce POS tags using one-tag-per-word constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model, associate features type-level lexicon.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	i=1 (f,v)∈Wi	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	inference, interested posterior probability latent variables model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Performance typically stabilizes across languages number iterations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	t(i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	use w erations sampling (see Figure 2 depiction).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	language investigate contribution component model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	languages make use tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 62.6 45.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 61.7 37.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 56.2 32.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 53.8 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 53.7 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 61.0 44.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 62.2 39.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 68.4 49.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 68.4 48.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 68.1 34.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 54.4 33.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	36.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 55.3 34.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 50.2 +P RI st dia n 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 65.5 46.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 64.7 42.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 58.3 40.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 57.3 51.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 65.9 48.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 60.7 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	41.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	7 68.3 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 70.7 52.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 70.9 42.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	37.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 55.8 38.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	36.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 57.3 +F EA TS st dia n 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 66.4 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 66.4 52.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 61.2 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 60.7 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 69.0 51.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 67.3 55.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 70.4 46.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 61.7 64.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 74.5 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 70.1 58.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 68.9 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 57.2 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 61.7 38.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second row represents performance median hyperparameter setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	See Section 5.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5.1 Data Sets.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	train test CoNLL-X training set.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Statistics data sets shown Table 2.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5.2 Setup.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	encodes one tag per word constraint uni form type-level tag assignments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	final model tions.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	perform five runs different random initialization sampling state.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	report results best median hyperparameter settings obtained way.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, settings report results median run setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	tokenize MWUs POS tags; reduces tag set size 12.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	See Table 2 tag set size languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	exception Dutch data set, processing performed annotated tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	6 Results Analysis.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	report token- type-level accuracy Table 3 6 languages system settings.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	system Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) reports best unsupervised results English.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	consider two variants Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	instance, Spanish, absolute gap median performance 10%.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second point comparison Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	compare Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009) Portuguese (Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, full model takes advantage word features present Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	novel element model ability capture type-level tag frequencies.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Similar behavior observed adding features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 1 0.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 2 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 1 2.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 1 8.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	7 Conclusion Future Work.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	resulting model compact, efficiently learnable linguistically expressive.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	1
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	paper, make simplifying assumption one-tag-per-word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	assumption, however, inherent type-based tagging models.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	promising direction future work explicitly model distribution tags word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	thank members MIT NLP group suggestions comments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Simple Type-Level Unsupervised POS Tagging	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, existing systems, expansion come steep increase model complexity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	several languages, report performance exceeding complex state-of-the art systems.1	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	— similar results observed across multiple languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design, readily capture regularities token-level.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work, take direct approach treat word type allowed POS tags primary element model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Across languages, high performance attained selecting single tag per word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	token-level HMM reflect lexicon sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	two key benefits model architecture.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	several languages, report performance exceeding state-of-the art systems.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	extent constraint enforced varies greatly across existing methods.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	clusters computed using SVD variant without relying transitional structure.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	approaches encode sparsity soft constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design guarantee “structural zeros,” biases towards sparsity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	use ILP learning desired grammar significantly increases computational complexity method.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast approaches, method directly incorporates constraints structure model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	design leads significant reduction computational complexity training inference.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	consider unsupervised POS induction problem without use tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	graphical depiction model well summary random variables parameters found Figure 1.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	standard, use fixed constant K number tagging states.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Conditioned , features word types W drawn.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	parameters depend single hyperparameter α.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	total O(K 2) parameters associated transition parameters.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3.1 Lexicon Component.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	purpose 3 follows since θt St − 1 parameters and.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	set fixed constants.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	explore well induce POS tags using one-tag-per-word constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model, associate features type-level lexicon.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	i=1 (f,v)∈Wi	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	inference, interested posterior probability latent variables model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Performance typically stabilizes across languages number iterations.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	t(i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	use w erations sampling (see Figure 2 depiction).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	language investigate contribution component model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	languages make use tagging dictionary.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 62.6 45.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 61.7 37.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 56.2 32.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 53.8 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 53.7 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 61.0 44.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 62.2 39.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 68.4 49.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 68.4 48.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 68.1 34.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 54.4 33.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	36.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 55.3 34.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 50.2 +P RI st dia n 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 65.5 46.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 64.7 42.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 58.3 40.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 57.3 51.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 65.9 48.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 60.7 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	41.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	7 68.3 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 70.7 52.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 70.9 42.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	37.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 55.8 38.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	36.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 57.3 +F EA TS st dia n 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	9 66.4 47.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 66.4 52.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 61.2 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 60.7 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 69.0 51.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 67.3 55.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 70.4 46.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	2 61.7 64.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 74.5 56.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 70.1 58.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 68.9 50.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	0 57.2 43.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	3 61.7 38.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second row represents performance median hyperparameter setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	See Section 5.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5.1 Data Sets.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	train test CoNLL-X training set.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Statistics data sets shown Table 2.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5.2 Setup.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	encodes one tag per word constraint uni form type-level tag assignments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	final model tions.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	perform five runs different random initialization sampling state.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	report results best median hyperparameter settings obtained way.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Specifically, settings report results median run setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	tokenize MWUs POS tags; reduces tag set size 12.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	See Table 2 tag set size languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	exception Dutch data set, processing performed annotated tags.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	6 Results Analysis.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	report token- type-level accuracy Table 3 6 languages system settings.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	system Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) reports best unsupervised results English.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	consider two variants Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Berg-Kirkpatrick et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	instance, Spanish, absolute gap median performance 10%.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	second point comparison Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	compare Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009) Portuguese (Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	However, full model takes advantage word features present Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	(2009).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	novel element model ability capture type-level tag frequencies.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Similar behavior observed adding features.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 1 0.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	1 2 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 1 2.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	8 1 8.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	7 Conclusion Future Work.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	resulting model compact, efficiently learnable linguistically expressive.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	1
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	paper, make simplifying assumption one-tag-per-word.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	assumption, however, inherent type-based tagging models.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	promising direction future work explicitly model distribution tags word type.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	thank members MIT NLP group suggestions comments.	0
"recently, Lee et al.</S><S sid =""17"" ssid = ""17"">(2010) presented new type-based model, also reported good results."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	— similar results observed across multiple languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design, readily capture regularities token-level.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	two key benefits model architecture.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	1
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Conditioned , features word types W drawn.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	parameters depend single hyperparameter α.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3.1 Lexicon Component.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	set fixed constants.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model, associate features type-level lexicon.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	i=1 (f,v)∈Wi	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	t(i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	language investigate contribution component model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	languages make use tagging dictionary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 62.6 45.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 61.7 37.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 56.2 32.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 53.8 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 53.7 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 61.0 44.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 62.2 39.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 68.4 49.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 68.4 48.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 68.1 34.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 54.4 33.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	36.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 55.3 34.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 65.5 46.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 64.7 42.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 58.3 40.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 57.3 51.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 65.9 48.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 60.7 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	41.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	7 68.3 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 70.7 52.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 70.9 42.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	37.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 55.8 38.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	36.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 66.4 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 66.4 52.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 61.2 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 60.7 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 69.0 51.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 67.3 55.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 70.4 46.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 61.7 64.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 74.5 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 70.1 58.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 68.9 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 57.2 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 61.7 38.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	See Section 5.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5.1 Data Sets.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	train test CoNLL-X training set.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Statistics data sets shown Table 2.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5.2 Setup.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	final model tions.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, settings report results median run setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	See Table 2 tag set size languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	6 Results Analysis.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second point comparison Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	compare Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Similar behavior observed adding features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 1 0.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 2 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 1 2.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 1 8.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	7 Conclusion Future Work.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	— similar results observed across multiple languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design, readily capture regularities token-level.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	two key benefits model architecture.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	1
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Conditioned , features word types W drawn.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	parameters depend single hyperparameter α.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3.1 Lexicon Component.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	set fixed constants.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model, associate features type-level lexicon.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	i=1 (f,v)∈Wi	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	t(i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	language investigate contribution component model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	languages make use tagging dictionary.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 62.6 45.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 61.7 37.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 56.2 32.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 53.8 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 53.7 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 61.0 44.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 62.2 39.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 68.4 49.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 68.4 48.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 68.1 34.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 54.4 33.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	36.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 55.3 34.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 65.5 46.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 64.7 42.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 58.3 40.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 57.3 51.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 65.9 48.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 60.7 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	41.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	7 68.3 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 70.7 52.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 70.9 42.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	37.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 55.8 38.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	36.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	9 66.4 47.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 66.4 52.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 61.2 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 60.7 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 69.0 51.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 67.3 55.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 70.4 46.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	2 61.7 64.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 74.5 56.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 70.1 58.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 68.9 50.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	0 57.2 43.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	3 61.7 38.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	See Section 5.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5.1 Data Sets.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	train test CoNLL-X training set.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Statistics data sets shown Table 2.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5.2 Setup.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	final model tions.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Specifically, settings report results median run setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	See Table 2 tag set size languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	6 Results Analysis.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	second point comparison Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	compare Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	(2009).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Similar behavior observed adding features.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 1 0.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	1 2 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 1 2.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	8 1 8.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	7 Conclusion Future Work.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Sequence models far common method supervised part- of-speech tagging, also widely used unsupervised part-of-speech tagging without dictionary (Smith Eisner, 2005; Haghighi Klein, 2006; Goldwater Griffiths, 2007; Johnson, 2007; Ravi Knight, 2009; Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Simple Type-Level Unsupervised POS Tagging	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, existing systems, expansion come steep increase model complexity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	several languages, report performance exceeding complex state-of-the art systems.1	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	— similar results observed across multiple languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design, readily capture regularities token-level.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work, take direct approach treat word type allowed POS tags primary element model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Across languages, high performance attained selecting single tag per word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	token-level HMM reflect lexicon sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	two key benefits model architecture.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	evaluate model seven languages exhibiting substantial syntactic variation.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	several languages, report performance exceeding state-of-the art systems.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	extent constraint enforced varies greatly across existing methods.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	clusters computed using SVD variant without relying transitional structure.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	approaches encode sparsity soft constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design guarantee “structural zeros,” biases towards sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	use ILP learning desired grammar significantly increases computational complexity method.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast approaches, method directly incorporates constraints structure model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design leads significant reduction computational complexity training inference.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	consider unsupervised POS induction problem without use tagging dictionary.	1
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	graphical depiction model well summary random variables parameters found Figure 1.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	standard, use fixed constant K number tagging states.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Conditioned , features word types W drawn.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	parameters depend single hyperparameter α.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	total O(K 2) parameters associated transition parameters.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast Bayesian HMM, θt drawn distribution support n word types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3.1 Lexicon Component.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	purpose 3 follows since θt St − 1 parameters and.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	tokens w generated token-level tags HMM parameterized lexicon structure.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	set fixed constants.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	explore well induce POS tags using one-tag-per-word constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model, associate features type-level lexicon.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast, NNP (proper nouns) form large portion vocabulary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, assume word type W consists feature-value pairs (f, v).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	i=1 (f,v)∈Wi	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	inference, interested posterior probability latent variables model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Performance typically stabilizes across languages number iterations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	t(i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	terms right-hand-side denote type-level token-level probability terms respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	crucial difference number parameters greatly reduced number variables sampled iteration.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	token-level term similar standard HMM sampling equations found Johnson (2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	use w erations sampling (see Figure 2 depiction).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	language investigate contribution component model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	languages make use tagging dictionary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 62.6 45.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 61.7 37.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 56.2 32.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 53.8 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 53.7 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 61.0 44.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 62.2 39.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 68.4 49.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 68.4 48.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 68.1 34.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 54.4 33.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	36.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 55.3 34.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 50.2 +P RI st dia n 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 65.5 46.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 64.7 42.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 58.3 40.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 57.3 51.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 65.9 48.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 60.7 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	41.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	7 68.3 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 70.7 52.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 70.9 42.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	37.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 55.8 38.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	36.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 57.3 +F EA TS st dia n 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 66.4 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 66.4 52.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 61.2 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 60.7 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 69.0 51.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 67.3 55.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 70.4 46.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 61.7 64.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 74.5 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 70.1 58.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 68.9 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 57.2 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 61.7 38.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second row represents performance median hyperparameter setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	See Section 5.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5.1 Data Sets.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	train test CoNLL-X training set.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Statistics data sets shown Table 2.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5.2 Setup.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	β shared hyperparameter tag assignment prior word feature multinomials.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Evaluation Metrics report three metrics evaluate tagging performance.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	encodes one tag per word constraint uni form type-level tag assignments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	final model tions.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	perform five runs different random initialization sampling state.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Hyperparameter settings sorted according median one-to-one metric runs.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	report results best median hyperparameter settings obtained way.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, settings report results median run setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	tokenize MWUs POS tags; reduces tag set size 12.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	See Table 2 tag set size languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	exception Dutch data set, processing performed annotated tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	6 Results Analysis.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	report token- type-level accuracy Table 3 6 languages system settings.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) posterior regular- ization HMM Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	system Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) reports best unsupervised results English.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	consider two variants Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) consistently outperforms English, obtain substantial gains across languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	instance, Spanish, absolute gap median performance 10%.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second point comparison Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	compare Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009) Portuguese (Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009) also report results English, reduced 17 tag set, comparable ours).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, full model takes advantage word features present Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	novel element model ability capture type-level tag frequencies.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Similar behavior observed adding features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 1 0.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 2 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 1 2.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 1 8.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	table shows lexicon tag frequency predicated full model closest gold standard.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	7 Conclusion Future Work.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	resulting model compact, efficiently learnable linguistically expressive.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	paper, make simplifying assumption one-tag-per-word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	assumption, however, inherent type-based tagging models.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	promising direction future work explicitly model distribution tags word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	thank members MIT NLP group suggestions comments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Simple Type-Level Unsupervised POS Tagging	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, existing systems, expansion come steep increase model complexity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	several languages, report performance exceeding complex state-of-the art systems.1	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	— similar results observed across multiple languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design, readily capture regularities token-level.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work, take direct approach treat word type allowed POS tags primary element model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Across languages, high performance attained selecting single tag per word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	token-level HMM reflect lexicon sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	two key benefits model architecture.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	evaluate model seven languages exhibiting substantial syntactic variation.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	several languages, report performance exceeding state-of-the art systems.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	extent constraint enforced varies greatly across existing methods.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	clusters computed using SVD variant without relying transitional structure.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	approaches encode sparsity soft constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design guarantee “structural zeros,” biases towards sparsity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	use ILP learning desired grammar significantly increases computational complexity method.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast approaches, method directly incorporates constraints structure model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	design leads significant reduction computational complexity training inference.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	consider unsupervised POS induction problem without use tagging dictionary.	1
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	graphical depiction model well summary random variables parameters found Figure 1.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	standard, use fixed constant K number tagging states.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Conditioned , features word types W drawn.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	parameters depend single hyperparameter α.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	total O(K 2) parameters associated transition parameters.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast Bayesian HMM, θt drawn distribution support n word types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3.1 Lexicon Component.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	purpose 3 follows since θt St − 1 parameters and.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	tokens w generated token-level tags HMM parameterized lexicon structure.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	set fixed constants.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	explore well induce POS tags using one-tag-per-word constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model, associate features type-level lexicon.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast, NNP (proper nouns) form large portion vocabulary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, assume word type W consists feature-value pairs (f, v).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	i=1 (f,v)∈Wi	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	inference, interested posterior probability latent variables model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Performance typically stabilizes across languages number iterations.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	t(i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	terms right-hand-side denote type-level token-level probability terms respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	crucial difference number parameters greatly reduced number variables sampled iteration.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	token-level term similar standard HMM sampling equations found Johnson (2007).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	use w erations sampling (see Figure 2 depiction).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	language investigate contribution component model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	languages make use tagging dictionary.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 62.6 45.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 61.7 37.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 56.2 32.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 53.8 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 53.7 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 61.0 44.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 62.2 39.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 68.4 49.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 68.4 48.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 68.1 34.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 54.4 33.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	36.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 55.3 34.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 50.2 +P RI st dia n 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 65.5 46.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 64.7 42.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 58.3 40.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 57.3 51.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 65.9 48.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 60.7 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	41.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	7 68.3 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 70.7 52.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 70.9 42.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	37.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 55.8 38.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	36.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 57.3 +F EA TS st dia n 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	9 66.4 47.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 66.4 52.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 61.2 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 60.7 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 69.0 51.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 67.3 55.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 70.4 46.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	2 61.7 64.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 74.5 56.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 70.1 58.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 68.9 50.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	0 57.2 43.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	3 61.7 38.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second row represents performance median hyperparameter setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	See Section 5.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5.1 Data Sets.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	train test CoNLL-X training set.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Statistics data sets shown Table 2.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5.2 Setup.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	β shared hyperparameter tag assignment prior word feature multinomials.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Evaluation Metrics report three metrics evaluate tagging performance.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	encodes one tag per word constraint uni form type-level tag assignments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	final model tions.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	perform five runs different random initialization sampling state.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Hyperparameter settings sorted according median one-to-one metric runs.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	report results best median hyperparameter settings obtained way.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Specifically, settings report results median run setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	tokenize MWUs POS tags; reduces tag set size 12.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	See Table 2 tag set size languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	exception Dutch data set, processing performed annotated tags.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	6 Results Analysis.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	report token- type-level accuracy Table 3 6 languages system settings.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) posterior regular- ization HMM Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	system Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) reports best unsupervised results English.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	consider two variants Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Berg-Kirkpatrick et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2010) consistently outperforms English, obtain substantial gains across languages.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	instance, Spanish, absolute gap median performance 10%.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	second point comparison Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	compare Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009) Portuguese (Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009) also report results English, reduced 17 tag set, comparable ours).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	However, full model takes advantage word features present Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	(2009).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	novel element model ability capture type-level tag frequencies.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Similar behavior observed adding features.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 1 0.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	1 2 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 1 2.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	8 1 8.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	table shows lexicon tag frequency predicated full model closest gold standard.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	7 Conclusion Future Work.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	resulting model compact, efficiently learnable linguistically expressive.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	paper, make simplifying assumption one-tag-per-word.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	assumption, however, inherent type-based tagging models.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	promising direction future work explicitly model distribution tags word type.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	thank members MIT NLP group suggestions comments.	0
previous work (Lee et al., 2010), find one-class-per-type restriction boosts performance considerably comparable token- based model yields results comparable state-of-the-art even without use morphology alignment features.	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Simple Type-Level Unsupervised POS Tagging	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, existing systems, expansion come steep increase model complexity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	several languages, report performance exceeding complex state-of-the art systems.1	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	— similar results observed across multiple languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design, readily capture regularities token-level.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work, take direct approach treat word type allowed POS tags primary element model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Across languages, high performance attained selecting single tag per word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	token-level HMM reflect lexicon sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	two key benefits model architecture.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	several languages, report performance exceeding state-of-the art systems.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	extent constraint enforced varies greatly across existing methods.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	clusters computed using SVD variant without relying transitional structure.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	approaches encode sparsity soft constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design guarantee “structural zeros,” biases towards sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	use ILP learning desired grammar significantly increases computational complexity method.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast approaches, method directly incorporates constraints structure model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design leads significant reduction computational complexity training inference.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	consider unsupervised POS induction problem without use tagging dictionary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	graphical depiction model well summary random variables parameters found Figure 1.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	standard, use fixed constant K number tagging states.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Conditioned , features word types W drawn.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	parameters depend single hyperparameter α.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	total O(K 2) parameters associated transition parameters.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3.1 Lexicon Component.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	purpose 3 follows since θt St − 1 parameters and.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	set fixed constants.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	explore well induce POS tags using one-tag-per-word constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model, associate features type-level lexicon.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	i=1 (f,v)∈Wi	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	inference, interested posterior probability latent variables model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Performance typically stabilizes across languages number iterations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	t(i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	use w erations sampling (see Figure 2 depiction).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	language investigate contribution component model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	languages make use tagging dictionary.	1
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 62.6 45.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 61.7 37.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 56.2 32.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 53.8 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 53.7 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 61.0 44.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 62.2 39.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 68.4 49.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 68.4 48.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 68.1 34.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 54.4 33.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	36.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 55.3 34.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 50.2 +P RI st dia n 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 65.5 46.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 64.7 42.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 58.3 40.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 57.3 51.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 65.9 48.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 60.7 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	41.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	7 68.3 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 70.7 52.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 70.9 42.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	37.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 55.8 38.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	36.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 57.3 +F EA TS st dia n 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 66.4 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 66.4 52.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 61.2 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 60.7 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 69.0 51.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 67.3 55.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 70.4 46.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 61.7 64.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 74.5 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 70.1 58.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 68.9 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 57.2 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 61.7 38.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second row represents performance median hyperparameter setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	See Section 5.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5.1 Data Sets.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	train test CoNLL-X training set.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Statistics data sets shown Table 2.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5.2 Setup.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	encodes one tag per word constraint uni form type-level tag assignments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	final model tions.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	perform five runs different random initialization sampling state.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	report results best median hyperparameter settings obtained way.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, settings report results median run setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	tokenize MWUs POS tags; reduces tag set size 12.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	See Table 2 tag set size languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	exception Dutch data set, processing performed annotated tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	6 Results Analysis.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	report token- type-level accuracy Table 3 6 languages system settings.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	system Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) reports best unsupervised results English.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	consider two variants Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	instance, Spanish, absolute gap median performance 10%.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second point comparison Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	compare Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009) Portuguese (Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, full model takes advantage word features present Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	novel element model ability capture type-level tag frequencies.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Similar behavior observed adding features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 1 0.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 2 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 1 2.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 1 8.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	7 Conclusion Future Work.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	resulting model compact, efficiently learnable linguistically expressive.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	paper, make simplifying assumption one-tag-per-word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	assumption, however, inherent type-based tagging models.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	promising direction future work explicitly model distribution tags word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	thank members MIT NLP group suggestions comments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Simple Type-Level Unsupervised POS Tagging	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, existing systems, expansion come steep increase model complexity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	several languages, report performance exceeding complex state-of-the art systems.1	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	— similar results observed across multiple languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design, readily capture regularities token-level.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work, take direct approach treat word type allowed POS tags primary element model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Across languages, high performance attained selecting single tag per word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	token-level HMM reflect lexicon sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	two key benefits model architecture.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	several languages, report performance exceeding state-of-the art systems.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	extent constraint enforced varies greatly across existing methods.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	clusters computed using SVD variant without relying transitional structure.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	approaches encode sparsity soft constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design guarantee “structural zeros,” biases towards sparsity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	use ILP learning desired grammar significantly increases computational complexity method.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast approaches, method directly incorporates constraints structure model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	design leads significant reduction computational complexity training inference.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	consider unsupervised POS induction problem without use tagging dictionary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	graphical depiction model well summary random variables parameters found Figure 1.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	standard, use fixed constant K number tagging states.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Conditioned , features word types W drawn.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	parameters depend single hyperparameter α.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	total O(K 2) parameters associated transition parameters.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3.1 Lexicon Component.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	purpose 3 follows since θt St − 1 parameters and.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	set fixed constants.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	explore well induce POS tags using one-tag-per-word constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model, associate features type-level lexicon.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	i=1 (f,v)∈Wi	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	inference, interested posterior probability latent variables model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Performance typically stabilizes across languages number iterations.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	t(i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	use w erations sampling (see Figure 2 depiction).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	language investigate contribution component model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	languages make use tagging dictionary.	1
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 62.6 45.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 61.7 37.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 56.2 32.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 53.8 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 53.7 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 61.0 44.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 62.2 39.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 68.4 49.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 68.4 48.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 68.1 34.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 54.4 33.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	36.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 55.3 34.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 50.2 +P RI st dia n 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 65.5 46.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 64.7 42.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 58.3 40.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 57.3 51.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 65.9 48.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 60.7 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	41.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	7 68.3 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 70.7 52.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 70.9 42.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	37.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 55.8 38.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	36.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 57.3 +F EA TS st dia n 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	9 66.4 47.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 66.4 52.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 61.2 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 60.7 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 69.0 51.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 67.3 55.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 70.4 46.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	2 61.7 64.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 74.5 56.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 70.1 58.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 68.9 50.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	0 57.2 43.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	3 61.7 38.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second row represents performance median hyperparameter setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	See Section 5.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5.1 Data Sets.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	train test CoNLL-X training set.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Statistics data sets shown Table 2.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5.2 Setup.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	encodes one tag per word constraint uni form type-level tag assignments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	final model tions.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	perform five runs different random initialization sampling state.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	report results best median hyperparameter settings obtained way.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Specifically, settings report results median run setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	tokenize MWUs POS tags; reduces tag set size 12.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	See Table 2 tag set size languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	exception Dutch data set, processing performed annotated tags.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	6 Results Analysis.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	report token- type-level accuracy Table 3 6 languages system settings.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	system Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) reports best unsupervised results English.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	consider two variants Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Berg-Kirkpatrick et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	instance, Spanish, absolute gap median performance 10%.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	second point comparison Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	compare Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009) Portuguese (Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	However, full model takes advantage word features present Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	(2009).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	novel element model ability capture type-level tag frequencies.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Similar behavior observed adding features.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 1 0.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	1 2 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 1 2.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	8 1 8.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	7 Conclusion Future Work.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	resulting model compact, efficiently learnable linguistically expressive.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	paper, make simplifying assumption one-tag-per-word.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	assumption, however, inherent type-based tagging models.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	promising direction future work explicitly model distribution tags word type.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	thank members MIT NLP group suggestions comments.	0
"2 One could approximate likelihood term assuming independence nj feature tokens word type j. approach taken Lee et al.</S><S sid =""91"" ssid = ""58"">(2010)."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Simple Type-Level Unsupervised POS Tagging	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, existing systems, expansion come steep increase model complexity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	1
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	— similar results observed across multiple languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design, readily capture regularities token-level.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Across languages, high performance attained selecting single tag per word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	token-level HMM reflect lexicon sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	two key benefits model architecture.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	several languages, report performance exceeding state-of-the art systems.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	extent constraint enforced varies greatly across existing methods.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	clusters computed using SVD variant without relying transitional structure.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	approaches encode sparsity soft constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design guarantee “structural zeros,” biases towards sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast approaches, method directly incorporates constraints structure model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design leads significant reduction computational complexity training inference.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	standard, use fixed constant K number tagging states.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Conditioned , features word types W drawn.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	parameters depend single hyperparameter α.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	total O(K 2) parameters associated transition parameters.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3.1 Lexicon Component.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	purpose 3 follows since θt St − 1 parameters and.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	set fixed constants.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	explore well induce POS tags using one-tag-per-word constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model, associate features type-level lexicon.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	i=1 (f,v)∈Wi	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	inference, interested posterior probability latent variables model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Performance typically stabilizes across languages number iterations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	t(i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	use w erations sampling (see Figure 2 depiction).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	language investigate contribution component model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	languages make use tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 62.6 45.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 61.7 37.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 56.2 32.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 53.8 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 53.7 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 61.0 44.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 62.2 39.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 68.4 49.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 68.4 48.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 68.1 34.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 54.4 33.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	36.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 55.3 34.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 50.2 +P RI st dia n 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 65.5 46.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 64.7 42.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 58.3 40.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 57.3 51.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 65.9 48.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 60.7 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	41.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	7 68.3 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 70.7 52.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 70.9 42.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	37.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 55.8 38.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	36.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 57.3 +F EA TS st dia n 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 66.4 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 66.4 52.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 61.2 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 60.7 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 69.0 51.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 67.3 55.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 70.4 46.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 61.7 64.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 74.5 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 70.1 58.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 68.9 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 57.2 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 61.7 38.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second row represents performance median hyperparameter setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	See Section 5.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5.1 Data Sets.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	train test CoNLL-X training set.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Statistics data sets shown Table 2.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5.2 Setup.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	final model tions.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	perform five runs different random initialization sampling state.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	report results best median hyperparameter settings obtained way.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, settings report results median run setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	See Table 2 tag set size languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	exception Dutch data set, processing performed annotated tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	6 Results Analysis.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	system Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) reports best unsupervised results English.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	consider two variants Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	instance, Spanish, absolute gap median performance 10%.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second point comparison Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	compare Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009) Portuguese (Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, full model takes advantage word features present Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	novel element model ability capture type-level tag frequencies.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Similar behavior observed adding features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 1 0.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 2 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 1 2.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 1 8.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	7 Conclusion Future Work.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	resulting model compact, efficiently learnable linguistically expressive.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	paper, make simplifying assumption one-tag-per-word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	assumption, however, inherent type-based tagging models.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	promising direction future work explicitly model distribution tags word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	thank members MIT NLP group suggestions comments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Simple Type-Level Unsupervised POS Tagging	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, existing systems, expansion come steep increase model complexity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	1
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	— similar results observed across multiple languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design, readily capture regularities token-level.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Across languages, high performance attained selecting single tag per word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	token-level HMM reflect lexicon sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	two key benefits model architecture.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	several languages, report performance exceeding state-of-the art systems.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	extent constraint enforced varies greatly across existing methods.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	clusters computed using SVD variant without relying transitional structure.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	approaches encode sparsity soft constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design guarantee “structural zeros,” biases towards sparsity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast approaches, method directly incorporates constraints structure model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	design leads significant reduction computational complexity training inference.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	standard, use fixed constant K number tagging states.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Conditioned , features word types W drawn.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	parameters depend single hyperparameter α.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	total O(K 2) parameters associated transition parameters.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3.1 Lexicon Component.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	purpose 3 follows since θt St − 1 parameters and.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	set fixed constants.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	explore well induce POS tags using one-tag-per-word constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model, associate features type-level lexicon.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	i=1 (f,v)∈Wi	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	inference, interested posterior probability latent variables model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Performance typically stabilizes across languages number iterations.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	t(i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	use w erations sampling (see Figure 2 depiction).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	language investigate contribution component model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	languages make use tagging dictionary.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 62.6 45.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 61.7 37.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 56.2 32.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 53.8 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 53.7 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 61.0 44.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 62.2 39.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 68.4 49.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 68.4 48.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 68.1 34.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 54.4 33.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	36.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 55.3 34.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 50.2 +P RI st dia n 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 65.5 46.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 64.7 42.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 58.3 40.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 57.3 51.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 65.9 48.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 60.7 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	41.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	7 68.3 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 70.7 52.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 70.9 42.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	37.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 55.8 38.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	36.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 57.3 +F EA TS st dia n 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	9 66.4 47.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 66.4 52.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 61.2 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 60.7 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 69.0 51.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 67.3 55.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 70.4 46.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	2 61.7 64.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 74.5 56.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 70.1 58.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 68.9 50.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	0 57.2 43.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	3 61.7 38.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second row represents performance median hyperparameter setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	See Section 5.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5.1 Data Sets.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	train test CoNLL-X training set.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Statistics data sets shown Table 2.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5.2 Setup.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	final model tions.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	perform five runs different random initialization sampling state.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	report results best median hyperparameter settings obtained way.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Specifically, settings report results median run setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	See Table 2 tag set size languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	exception Dutch data set, processing performed annotated tags.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	6 Results Analysis.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	system Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) reports best unsupervised results English.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	consider two variants Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Berg-Kirkpatrick et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	instance, Spanish, absolute gap median performance 10%.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	second point comparison Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	compare Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009) Portuguese (Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	However, full model takes advantage word features present Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	(2009).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	novel element model ability capture type-level tag frequencies.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Similar behavior observed adding features.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 1 0.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	1 2 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 1 2.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	8 1 8.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	7 Conclusion Future Work.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	resulting model compact, efficiently learnable linguistically expressive.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	paper, make simplifying assumption one-tag-per-word.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	assumption, however, inherent type-based tagging models.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	promising direction future work explicitly model distribution tags word type.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	thank members MIT NLP group suggestions comments.	0
"Following Lee et al.</S><S sid =""130"" ssid = ""36"">(2010) used training sections language."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	1
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	— similar results observed across multiple languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design, readily capture regularities token-level.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	two key benefits model architecture.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	approaches encode sparsity soft constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Conditioned , features word types W drawn.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	parameters depend single hyperparameter α.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3.1 Lexicon Component.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	set fixed constants.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model, associate features type-level lexicon.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	i=1 (f,v)∈Wi	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	t(i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	language investigate contribution component model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	languages make use tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 62.6 45.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 61.7 37.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 56.2 32.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 53.8 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 53.7 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 61.0 44.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 62.2 39.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 68.4 49.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 68.4 48.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 68.1 34.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 54.4 33.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	36.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 55.3 34.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 65.5 46.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 64.7 42.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 58.3 40.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 57.3 51.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 65.9 48.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 60.7 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	41.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	7 68.3 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 70.7 52.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 70.9 42.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	37.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 55.8 38.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	36.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 66.4 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 66.4 52.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 61.2 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 60.7 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 69.0 51.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 67.3 55.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 70.4 46.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 61.7 64.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 74.5 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 70.1 58.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 68.9 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 57.2 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 61.7 38.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	See Section 5.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5.1 Data Sets.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	train test CoNLL-X training set.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Statistics data sets shown Table 2.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5.2 Setup.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	final model tions.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	perform five runs different random initialization sampling state.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, settings report results median run setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	See Table 2 tag set size languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	6 Results Analysis.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) reports best unsupervised results English.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second point comparison Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	compare Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Similar behavior observed adding features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 1 0.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 2 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 1 2.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 1 8.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	7 Conclusion Future Work.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	1
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	— similar results observed across multiple languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design, readily capture regularities token-level.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	two key benefits model architecture.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	approaches encode sparsity soft constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Conditioned , features word types W drawn.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	parameters depend single hyperparameter α.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3.1 Lexicon Component.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	set fixed constants.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model, associate features type-level lexicon.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	i=1 (f,v)∈Wi	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	t(i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	language investigate contribution component model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	languages make use tagging dictionary.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 62.6 45.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 61.7 37.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 56.2 32.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 53.8 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 53.7 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 61.0 44.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 62.2 39.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 68.4 49.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 68.4 48.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 68.1 34.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 54.4 33.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	36.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 55.3 34.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 65.5 46.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 64.7 42.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 58.3 40.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 57.3 51.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 65.9 48.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 60.7 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	41.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	7 68.3 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 70.7 52.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 70.9 42.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	37.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 55.8 38.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	36.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	9 66.4 47.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 66.4 52.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 61.2 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 60.7 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 69.0 51.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 67.3 55.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 70.4 46.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	2 61.7 64.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 74.5 56.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 70.1 58.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 68.9 50.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	0 57.2 43.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	3 61.7 38.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	See Section 5.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5.1 Data Sets.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	train test CoNLL-X training set.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Statistics data sets shown Table 2.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5.2 Setup.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	final model tions.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	perform five runs different random initialization sampling state.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Specifically, settings report results median run setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	See Table 2 tag set size languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	6 Results Analysis.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) reports best unsupervised results English.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Berg-Kirkpatrick et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	second point comparison Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	compare Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	(2009).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Similar behavior observed adding features.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 1 0.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	1 2 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 1 2.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	8 1 8.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	7 Conclusion Future Work.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
Given close 95% word occurrences human labeled data tagged frequent part speech (Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
vised POS induction algorithm (Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
vised POS induction algorithm (Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
vised POS induction algorithm (Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
vised POS induction algorithm (Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
vised POS induction algorithm (Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
vised POS induction algorithm (Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
vised POS induction algorithm (Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
vised POS induction algorithm (Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
vised POS induction algorithm (Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
vised POS induction algorithm (Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
vised POS induction algorithm (Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
vised POS induction algorithm (Lee et al., 2010)	— similar results observed across multiple languages.	0
vised POS induction algorithm (Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
vised POS induction algorithm (Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
vised POS induction algorithm (Lee et al., 2010)	design, readily capture regularities token-level.	0
vised POS induction algorithm (Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
vised POS induction algorithm (Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
vised POS induction algorithm (Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
vised POS induction algorithm (Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
vised POS induction algorithm (Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
vised POS induction algorithm (Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
vised POS induction algorithm (Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
vised POS induction algorithm (Lee et al., 2010)	two key benefits model architecture.	0
vised POS induction algorithm (Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
vised POS induction algorithm (Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
vised POS induction algorithm (Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
vised POS induction algorithm (Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
vised POS induction algorithm (Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
vised POS induction algorithm (Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
vised POS induction algorithm (Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
vised POS induction algorithm (Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
vised POS induction algorithm (Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
vised POS induction algorithm (Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
vised POS induction algorithm (Lee et al., 2010)	approaches encode sparsity soft constraint.	0
vised POS induction algorithm (Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
vised POS induction algorithm (Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
vised POS induction algorithm (Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
vised POS induction algorithm (Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
vised POS induction algorithm (Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
vised POS induction algorithm (Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
vised POS induction algorithm (Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
vised POS induction algorithm (Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
vised POS induction algorithm (Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
vised POS induction algorithm (Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	1
vised POS induction algorithm (Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
vised POS induction algorithm (Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
vised POS induction algorithm (Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
vised POS induction algorithm (Lee et al., 2010)	Conditioned , features word types W drawn.	0
vised POS induction algorithm (Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
vised POS induction algorithm (Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
vised POS induction algorithm (Lee et al., 2010)	parameters depend single hyperparameter α.	0
vised POS induction algorithm (Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
vised POS induction algorithm (Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
vised POS induction algorithm (Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
vised POS induction algorithm (Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
vised POS induction algorithm (Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
vised POS induction algorithm (Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
vised POS induction algorithm (Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
vised POS induction algorithm (Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
vised POS induction algorithm (Lee et al., 2010)	3.1 Lexicon Component.	0
vised POS induction algorithm (Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
vised POS induction algorithm (Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
vised POS induction algorithm (Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
vised POS induction algorithm (Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
vised POS induction algorithm (Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
vised POS induction algorithm (Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
vised POS induction algorithm (Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
vised POS induction algorithm (Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
vised POS induction algorithm (Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
vised POS induction algorithm (Lee et al., 2010)	set fixed constants.	0
vised POS induction algorithm (Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
vised POS induction algorithm (Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
vised POS induction algorithm (Lee et al., 2010)	model, associate features type-level lexicon.	0
vised POS induction algorithm (Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
vised POS induction algorithm (Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
vised POS induction algorithm (Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
vised POS induction algorithm (Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
vised POS induction algorithm (Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
vised POS induction algorithm (Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
vised POS induction algorithm (Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
vised POS induction algorithm (Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
vised POS induction algorithm (Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
vised POS induction algorithm (Lee et al., 2010)	i=1 (f,v)∈Wi	0
vised POS induction algorithm (Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
vised POS induction algorithm (Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
vised POS induction algorithm (Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
vised POS induction algorithm (Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
vised POS induction algorithm (Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
vised POS induction algorithm (Lee et al., 2010)	t(i).	0
vised POS induction algorithm (Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
vised POS induction algorithm (Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
vised POS induction algorithm (Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
vised POS induction algorithm (Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
vised POS induction algorithm (Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
vised POS induction algorithm (Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
vised POS induction algorithm (Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
vised POS induction algorithm (Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
vised POS induction algorithm (Lee et al., 2010)	language investigate contribution component model.	0
vised POS induction algorithm (Lee et al., 2010)	languages make use tagging dictionary.	0
vised POS induction algorithm (Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
vised POS induction algorithm (Lee et al., 2010)	2 62.6 45.	0
vised POS induction algorithm (Lee et al., 2010)	1 61.7 37.	0
vised POS induction algorithm (Lee et al., 2010)	2 56.2 32.	0
vised POS induction algorithm (Lee et al., 2010)	1 53.8 47.	0
vised POS induction algorithm (Lee et al., 2010)	4 53.7 43.	0
vised POS induction algorithm (Lee et al., 2010)	9 61.0 44.	0
vised POS induction algorithm (Lee et al., 2010)	2 62.2 39.	0
vised POS induction algorithm (Lee et al., 2010)	3 68.4 49.	0
vised POS induction algorithm (Lee et al., 2010)	0 68.4 48.	0
vised POS induction algorithm (Lee et al., 2010)	5 68.1 34.	0
vised POS induction algorithm (Lee et al., 2010)	3 54.4 33.	0
vised POS induction algorithm (Lee et al., 2010)	36.	0
vised POS induction algorithm (Lee et al., 2010)	0 55.3 34.	0
vised POS induction algorithm (Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
vised POS induction algorithm (Lee et al., 2010)	9 65.5 46.	0
vised POS induction algorithm (Lee et al., 2010)	5 64.7 42.	0
vised POS induction algorithm (Lee et al., 2010)	3 58.3 40.	0
vised POS induction algorithm (Lee et al., 2010)	0 57.3 51.	0
vised POS induction algorithm (Lee et al., 2010)	4 65.9 48.	0
vised POS induction algorithm (Lee et al., 2010)	3 60.7 50.	0
vised POS induction algorithm (Lee et al., 2010)	41.	0
vised POS induction algorithm (Lee et al., 2010)	7 68.3 56.	0
vised POS induction algorithm (Lee et al., 2010)	2 70.7 52.	0
vised POS induction algorithm (Lee et al., 2010)	0 70.9 42.	0
vised POS induction algorithm (Lee et al., 2010)	37.	0
vised POS induction algorithm (Lee et al., 2010)	1 55.8 38.	0
vised POS induction algorithm (Lee et al., 2010)	36.	0
vised POS induction algorithm (Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
vised POS induction algorithm (Lee et al., 2010)	9 66.4 47.	0
vised POS induction algorithm (Lee et al., 2010)	8 66.4 52.	0
vised POS induction algorithm (Lee et al., 2010)	1 61.2 43.	0
vised POS induction algorithm (Lee et al., 2010)	2 60.7 56.	0
vised POS induction algorithm (Lee et al., 2010)	4 69.0 51.	0
vised POS induction algorithm (Lee et al., 2010)	5 67.3 55.	0
vised POS induction algorithm (Lee et al., 2010)	4 70.4 46.	0
vised POS induction algorithm (Lee et al., 2010)	2 61.7 64.	0
vised POS induction algorithm (Lee et al., 2010)	1 74.5 56.	0
vised POS induction algorithm (Lee et al., 2010)	5 70.1 58.	0
vised POS induction algorithm (Lee et al., 2010)	3 68.9 50.	0
vised POS induction algorithm (Lee et al., 2010)	0 57.2 43.	0
vised POS induction algorithm (Lee et al., 2010)	3 61.7 38.	0
vised POS induction algorithm (Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
vised POS induction algorithm (Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
vised POS induction algorithm (Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
vised POS induction algorithm (Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
vised POS induction algorithm (Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
vised POS induction algorithm (Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
vised POS induction algorithm (Lee et al., 2010)	See Section 5.	0
vised POS induction algorithm (Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
vised POS induction algorithm (Lee et al., 2010)	5.1 Data Sets.	0
vised POS induction algorithm (Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
vised POS induction algorithm (Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
vised POS induction algorithm (Lee et al., 2010)	train test CoNLL-X training set.	0
vised POS induction algorithm (Lee et al., 2010)	Statistics data sets shown Table 2.	0
vised POS induction algorithm (Lee et al., 2010)	5.2 Setup.	0
vised POS induction algorithm (Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
vised POS induction algorithm (Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
vised POS induction algorithm (Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
vised POS induction algorithm (Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
vised POS induction algorithm (Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
vised POS induction algorithm (Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
vised POS induction algorithm (Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
vised POS induction algorithm (Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
vised POS induction algorithm (Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
vised POS induction algorithm (Lee et al., 2010)	final model tions.	0
vised POS induction algorithm (Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
vised POS induction algorithm (Lee et al., 2010)	perform five runs different random initialization sampling state.	0
vised POS induction algorithm (Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
vised POS induction algorithm (Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, settings report results median run setting.	0
vised POS induction algorithm (Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
vised POS induction algorithm (Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
vised POS induction algorithm (Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
vised POS induction algorithm (Lee et al., 2010)	See Table 2 tag set size languages.	0
vised POS induction algorithm (Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
vised POS induction algorithm (Lee et al., 2010)	6 Results Analysis.	0
vised POS induction algorithm (Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
vised POS induction algorithm (Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
vised POS induction algorithm (Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) reports best unsupervised results English.	0
vised POS induction algorithm (Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
vised POS induction algorithm (Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
vised POS induction algorithm (Lee et al., 2010)	Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
vised POS induction algorithm (Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
vised POS induction algorithm (Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
vised POS induction algorithm (Lee et al., 2010)	second point comparison Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
vised POS induction algorithm (Lee et al., 2010)	compare Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
vised POS induction algorithm (Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
vised POS induction algorithm (Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
vised POS induction algorithm (Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
vised POS induction algorithm (Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
vised POS induction algorithm (Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Similar behavior observed adding features.	0
vised POS induction algorithm (Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
vised POS induction algorithm (Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
vised POS induction algorithm (Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
vised POS induction algorithm (Lee et al., 2010)	1 1 0.	0
vised POS induction algorithm (Lee et al., 2010)	1 2 3.	0
vised POS induction algorithm (Lee et al., 2010)	8 1 2.	0
vised POS induction algorithm (Lee et al., 2010)	8 1 8.	0
vised POS induction algorithm (Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
vised POS induction algorithm (Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
vised POS induction algorithm (Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
vised POS induction algorithm (Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
vised POS induction algorithm (Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
vised POS induction algorithm (Lee et al., 2010)	7 Conclusion Future Work.	0
vised POS induction algorithm (Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
vised POS induction algorithm (Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
vised POS induction algorithm (Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
vised POS induction algorithm (Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
vised POS induction algorithm (Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
vised POS induction algorithm (Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
vised POS induction algorithm (Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
vised POS induction algorithm (Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
vised POS induction algorithm (Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
vised POS induction algorithm (Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
vised POS induction algorithm (Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
vised POS induction algorithm (Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
vised POS induction algorithm (Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
vised POS induction algorithm (Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
vised POS induction algorithm (Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
vised POS induction algorithm (Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
vised POS induction algorithm (Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
vised POS induction algorithm (Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
vised POS induction algorithm (Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
vised POS induction algorithm (Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
vised POS induction algorithm (Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
vised POS induction algorithm (Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
vised POS induction algorithm (Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
vised POS induction algorithm (Lee et al., 2010)	— similar results observed across multiple languages.	0
vised POS induction algorithm (Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
vised POS induction algorithm (Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
vised POS induction algorithm (Lee et al., 2010)	design, readily capture regularities token-level.	0
vised POS induction algorithm (Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
vised POS induction algorithm (Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
vised POS induction algorithm (Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
vised POS induction algorithm (Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
vised POS induction algorithm (Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
vised POS induction algorithm (Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
vised POS induction algorithm (Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
vised POS induction algorithm (Lee et al., 2010)	two key benefits model architecture.	0
vised POS induction algorithm (Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
vised POS induction algorithm (Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
vised POS induction algorithm (Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
vised POS induction algorithm (Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
vised POS induction algorithm (Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
vised POS induction algorithm (Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
vised POS induction algorithm (Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
vised POS induction algorithm (Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
vised POS induction algorithm (Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
vised POS induction algorithm (Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
vised POS induction algorithm (Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
vised POS induction algorithm (Lee et al., 2010)	approaches encode sparsity soft constraint.	0
vised POS induction algorithm (Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
vised POS induction algorithm (Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
vised POS induction algorithm (Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
vised POS induction algorithm (Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
vised POS induction algorithm (Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
vised POS induction algorithm (Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
vised POS induction algorithm (Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
vised POS induction algorithm (Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
vised POS induction algorithm (Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
vised POS induction algorithm (Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
vised POS induction algorithm (Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
vised POS induction algorithm (Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	1
vised POS induction algorithm (Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
vised POS induction algorithm (Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
vised POS induction algorithm (Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
vised POS induction algorithm (Lee et al., 2010)	Conditioned , features word types W drawn.	0
vised POS induction algorithm (Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
vised POS induction algorithm (Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
vised POS induction algorithm (Lee et al., 2010)	parameters depend single hyperparameter α.	0
vised POS induction algorithm (Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
vised POS induction algorithm (Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
vised POS induction algorithm (Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
vised POS induction algorithm (Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
vised POS induction algorithm (Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
vised POS induction algorithm (Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
vised POS induction algorithm (Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
vised POS induction algorithm (Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
vised POS induction algorithm (Lee et al., 2010)	3.1 Lexicon Component.	0
vised POS induction algorithm (Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
vised POS induction algorithm (Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
vised POS induction algorithm (Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
vised POS induction algorithm (Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
vised POS induction algorithm (Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
vised POS induction algorithm (Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
vised POS induction algorithm (Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
vised POS induction algorithm (Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
vised POS induction algorithm (Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
vised POS induction algorithm (Lee et al., 2010)	set fixed constants.	0
vised POS induction algorithm (Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
vised POS induction algorithm (Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
vised POS induction algorithm (Lee et al., 2010)	model, associate features type-level lexicon.	0
vised POS induction algorithm (Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
vised POS induction algorithm (Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
vised POS induction algorithm (Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
vised POS induction algorithm (Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
vised POS induction algorithm (Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
vised POS induction algorithm (Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
vised POS induction algorithm (Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
vised POS induction algorithm (Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
vised POS induction algorithm (Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
vised POS induction algorithm (Lee et al., 2010)	i=1 (f,v)∈Wi	0
vised POS induction algorithm (Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
vised POS induction algorithm (Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
vised POS induction algorithm (Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
vised POS induction algorithm (Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
vised POS induction algorithm (Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
vised POS induction algorithm (Lee et al., 2010)	t(i).	0
vised POS induction algorithm (Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
vised POS induction algorithm (Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
vised POS induction algorithm (Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
vised POS induction algorithm (Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
vised POS induction algorithm (Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
vised POS induction algorithm (Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
vised POS induction algorithm (Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
vised POS induction algorithm (Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
vised POS induction algorithm (Lee et al., 2010)	language investigate contribution component model.	0
vised POS induction algorithm (Lee et al., 2010)	languages make use tagging dictionary.	0
vised POS induction algorithm (Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
vised POS induction algorithm (Lee et al., 2010)	2 62.6 45.	0
vised POS induction algorithm (Lee et al., 2010)	1 61.7 37.	0
vised POS induction algorithm (Lee et al., 2010)	2 56.2 32.	0
vised POS induction algorithm (Lee et al., 2010)	1 53.8 47.	0
vised POS induction algorithm (Lee et al., 2010)	4 53.7 43.	0
vised POS induction algorithm (Lee et al., 2010)	9 61.0 44.	0
vised POS induction algorithm (Lee et al., 2010)	2 62.2 39.	0
vised POS induction algorithm (Lee et al., 2010)	3 68.4 49.	0
vised POS induction algorithm (Lee et al., 2010)	0 68.4 48.	0
vised POS induction algorithm (Lee et al., 2010)	5 68.1 34.	0
vised POS induction algorithm (Lee et al., 2010)	3 54.4 33.	0
vised POS induction algorithm (Lee et al., 2010)	36.	0
vised POS induction algorithm (Lee et al., 2010)	0 55.3 34.	0
vised POS induction algorithm (Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
vised POS induction algorithm (Lee et al., 2010)	9 65.5 46.	0
vised POS induction algorithm (Lee et al., 2010)	5 64.7 42.	0
vised POS induction algorithm (Lee et al., 2010)	3 58.3 40.	0
vised POS induction algorithm (Lee et al., 2010)	0 57.3 51.	0
vised POS induction algorithm (Lee et al., 2010)	4 65.9 48.	0
vised POS induction algorithm (Lee et al., 2010)	3 60.7 50.	0
vised POS induction algorithm (Lee et al., 2010)	41.	0
vised POS induction algorithm (Lee et al., 2010)	7 68.3 56.	0
vised POS induction algorithm (Lee et al., 2010)	2 70.7 52.	0
vised POS induction algorithm (Lee et al., 2010)	0 70.9 42.	0
vised POS induction algorithm (Lee et al., 2010)	37.	0
vised POS induction algorithm (Lee et al., 2010)	1 55.8 38.	0
vised POS induction algorithm (Lee et al., 2010)	36.	0
vised POS induction algorithm (Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
vised POS induction algorithm (Lee et al., 2010)	9 66.4 47.	0
vised POS induction algorithm (Lee et al., 2010)	8 66.4 52.	0
vised POS induction algorithm (Lee et al., 2010)	1 61.2 43.	0
vised POS induction algorithm (Lee et al., 2010)	2 60.7 56.	0
vised POS induction algorithm (Lee et al., 2010)	4 69.0 51.	0
vised POS induction algorithm (Lee et al., 2010)	5 67.3 55.	0
vised POS induction algorithm (Lee et al., 2010)	4 70.4 46.	0
vised POS induction algorithm (Lee et al., 2010)	2 61.7 64.	0
vised POS induction algorithm (Lee et al., 2010)	1 74.5 56.	0
vised POS induction algorithm (Lee et al., 2010)	5 70.1 58.	0
vised POS induction algorithm (Lee et al., 2010)	3 68.9 50.	0
vised POS induction algorithm (Lee et al., 2010)	0 57.2 43.	0
vised POS induction algorithm (Lee et al., 2010)	3 61.7 38.	0
vised POS induction algorithm (Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
vised POS induction algorithm (Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
vised POS induction algorithm (Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
vised POS induction algorithm (Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
vised POS induction algorithm (Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
vised POS induction algorithm (Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
vised POS induction algorithm (Lee et al., 2010)	See Section 5.	0
vised POS induction algorithm (Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
vised POS induction algorithm (Lee et al., 2010)	5.1 Data Sets.	0
vised POS induction algorithm (Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
vised POS induction algorithm (Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
vised POS induction algorithm (Lee et al., 2010)	train test CoNLL-X training set.	0
vised POS induction algorithm (Lee et al., 2010)	Statistics data sets shown Table 2.	0
vised POS induction algorithm (Lee et al., 2010)	5.2 Setup.	0
vised POS induction algorithm (Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
vised POS induction algorithm (Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
vised POS induction algorithm (Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
vised POS induction algorithm (Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
vised POS induction algorithm (Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
vised POS induction algorithm (Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
vised POS induction algorithm (Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
vised POS induction algorithm (Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
vised POS induction algorithm (Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
vised POS induction algorithm (Lee et al., 2010)	final model tions.	0
vised POS induction algorithm (Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
vised POS induction algorithm (Lee et al., 2010)	perform five runs different random initialization sampling state.	0
vised POS induction algorithm (Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
vised POS induction algorithm (Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
vised POS induction algorithm (Lee et al., 2010)	Specifically, settings report results median run setting.	0
vised POS induction algorithm (Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
vised POS induction algorithm (Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
vised POS induction algorithm (Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
vised POS induction algorithm (Lee et al., 2010)	See Table 2 tag set size languages.	0
vised POS induction algorithm (Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
vised POS induction algorithm (Lee et al., 2010)	6 Results Analysis.	0
vised POS induction algorithm (Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
vised POS induction algorithm (Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
vised POS induction algorithm (Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) reports best unsupervised results English.	0
vised POS induction algorithm (Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
vised POS induction algorithm (Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
vised POS induction algorithm (Lee et al., 2010)	Berg-Kirkpatrick et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
vised POS induction algorithm (Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
vised POS induction algorithm (Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
vised POS induction algorithm (Lee et al., 2010)	second point comparison Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
vised POS induction algorithm (Lee et al., 2010)	compare Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
vised POS induction algorithm (Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
vised POS induction algorithm (Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
vised POS induction algorithm (Lee et al., 2010)	(2009).	0
vised POS induction algorithm (Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
vised POS induction algorithm (Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
vised POS induction algorithm (Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
vised POS induction algorithm (Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Similar behavior observed adding features.	0
vised POS induction algorithm (Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
vised POS induction algorithm (Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
vised POS induction algorithm (Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
vised POS induction algorithm (Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
vised POS induction algorithm (Lee et al., 2010)	1 1 0.	0
vised POS induction algorithm (Lee et al., 2010)	1 2 3.	0
vised POS induction algorithm (Lee et al., 2010)	8 1 2.	0
vised POS induction algorithm (Lee et al., 2010)	8 1 8.	0
vised POS induction algorithm (Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
vised POS induction algorithm (Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
vised POS induction algorithm (Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
vised POS induction algorithm (Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
vised POS induction algorithm (Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
vised POS induction algorithm (Lee et al., 2010)	7 Conclusion Future Work.	0
vised POS induction algorithm (Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
vised POS induction algorithm (Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
vised POS induction algorithm (Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
vised POS induction algorithm (Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
vised POS induction algorithm (Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
vised POS induction algorithm (Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
vised POS induction algorithm (Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
vised POS induction algorithm (Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
vised POS induction algorithm (Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
vised POS induction algorithm (Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
vised POS induction algorithm (Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
vised POS induction algorithm (Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	— similar results observed across multiple languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design, readily capture regularities token-level.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	two key benefits model architecture.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	1
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Conditioned , features word types W drawn.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	parameters depend single hyperparameter α.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3.1 Lexicon Component.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	set fixed constants.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model, associate features type-level lexicon.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	i=1 (f,v)∈Wi	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	t(i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	language investigate contribution component model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	languages make use tagging dictionary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 62.6 45.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 61.7 37.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 56.2 32.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 53.8 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 53.7 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 61.0 44.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 62.2 39.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 68.4 49.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 68.4 48.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 68.1 34.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 54.4 33.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	36.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 55.3 34.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 65.5 46.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 64.7 42.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 58.3 40.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 57.3 51.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 65.9 48.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 60.7 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	41.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	7 68.3 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 70.7 52.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 70.9 42.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	37.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 55.8 38.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	36.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 66.4 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 66.4 52.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 61.2 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 60.7 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 69.0 51.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 67.3 55.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 70.4 46.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 61.7 64.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 74.5 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 70.1 58.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 68.9 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 57.2 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 61.7 38.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	See Section 5.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5.1 Data Sets.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	train test CoNLL-X training set.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Statistics data sets shown Table 2.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5.2 Setup.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	final model tions.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, settings report results median run setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	See Table 2 tag set size languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	6 Results Analysis.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second point comparison Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	compare Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Similar behavior observed adding features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 1 0.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 2 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 1 2.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 1 8.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	7 Conclusion Future Work.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	— similar results observed across multiple languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design, readily capture regularities token-level.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	two key benefits model architecture.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	1
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Conditioned , features word types W drawn.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	parameters depend single hyperparameter α.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3.1 Lexicon Component.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	set fixed constants.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model, associate features type-level lexicon.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	i=1 (f,v)∈Wi	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	t(i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	language investigate contribution component model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	languages make use tagging dictionary.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 62.6 45.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 61.7 37.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 56.2 32.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 53.8 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 53.7 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 61.0 44.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 62.2 39.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 68.4 49.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 68.4 48.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 68.1 34.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 54.4 33.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	36.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 55.3 34.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 65.5 46.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 64.7 42.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 58.3 40.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 57.3 51.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 65.9 48.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 60.7 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	41.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	7 68.3 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 70.7 52.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 70.9 42.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	37.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 55.8 38.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	36.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	9 66.4 47.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 66.4 52.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 61.2 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 60.7 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 69.0 51.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 67.3 55.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 70.4 46.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	2 61.7 64.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 74.5 56.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 70.1 58.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 68.9 50.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	0 57.2 43.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	3 61.7 38.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	See Section 5.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5.1 Data Sets.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	train test CoNLL-X training set.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Statistics data sets shown Table 2.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5.2 Setup.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	final model tions.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Specifically, settings report results median run setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	See Table 2 tag set size languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	6 Results Analysis.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	second point comparison Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	compare Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	(2009).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Similar behavior observed adding features.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 1 0.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	1 2 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 1 2.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	8 1 8.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	7 Conclusion Future Work.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Unsupervised induction POS taggers offers possibility avoiding costly annotation, despite recent progress, accuracy unsupervised POS taggers still falls far behind supervised systems, suitable applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	— similar results observed across multiple languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design, readily capture regularities token-level.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	two key benefits model architecture.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approaches encode sparsity soft constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Conditioned , features word types W drawn.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	parameters depend single hyperparameter α.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3.1 Lexicon Component.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	set fixed constants.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model, associate features type-level lexicon.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	i=1 (f,v)∈Wi	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	t(i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language investigate contribution component model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages make use tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.6 45.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.7 37.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 56.2 32.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 53.8 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 53.7 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 61.0 44.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.2 39.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.4 49.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 68.4 48.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 68.1 34.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 54.4 33.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 55.3 34.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 65.5 46.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 64.7 42.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 58.3 40.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.3 51.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 65.9 48.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 60.7 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	41.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 68.3 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 70.7 52.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 70.9 42.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	37.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 55.8 38.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 66.4 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 66.4 52.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.2 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 60.7 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 69.0 51.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 67.3 55.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 70.4 46.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 61.7 64.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 74.5 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 70.1 58.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.9 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.2 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 61.7 38.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Section 5.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.1 Data Sets.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	train test CoNLL-X training set.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Statistics data sets shown Table 2.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.2 Setup.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	final model tions.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	perform five runs different random initialization sampling state.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, settings report results median run setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Table 2 tag set size languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	6 Results Analysis.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) reports best unsupervised results English.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second point comparison Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	compare Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Similar behavior observed adding features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 1 0.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 2 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 2.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 8.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 Conclusion Future Work.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	1
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simple Type-Level Unsupervised POS Tagging	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, existing systems, expansion come steep increase model complexity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding complex state-of-the art systems.1	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	— similar results observed across multiple languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design, readily capture regularities token-level.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, take direct approach treat word type allowed POS tags primary element model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, high performance attained selecting single tag per word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level HMM reflect lexicon sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	two key benefits model architecture.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	several languages, report performance exceeding state-of-the art systems.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	extent constraint enforced varies greatly across existing methods.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	clusters computed using SVD variant without relying transitional structure.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approaches encode sparsity soft constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design guarantee “structural zeros,” biases towards sparsity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use ILP learning desired grammar significantly increases computational complexity method.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast approaches, method directly incorporates constraints structure model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	design leads significant reduction computational complexity training inference.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider unsupervised POS induction problem without use tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	graphical depiction model well summary random variables parameters found Figure 1.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, use fixed constant K number tagging states.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Conditioned , features word types W drawn.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	parameters depend single hyperparameter α.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	total O(K 2) parameters associated transition parameters.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3.1 Lexicon Component.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	purpose 3 follows since θt St − 1 parameters and.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	set fixed constants.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	explore well induce POS tags using one-tag-per-word constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model, associate features type-level lexicon.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	i=1 (f,v)∈Wi	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	inference, interested posterior probability latent variables model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Performance typically stabilizes across languages number iterations.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	t(i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	use w erations sampling (see Figure 2 depiction).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language investigate contribution component model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages make use tagging dictionary.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.6 45.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.7 37.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 56.2 32.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 53.8 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 53.7 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 61.0 44.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 62.2 39.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.4 49.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 68.4 48.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 68.1 34.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 54.4 33.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 55.3 34.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 50.2 +P RI st dia n 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 65.5 46.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 64.7 42.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 58.3 40.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.3 51.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 65.9 48.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 60.7 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	41.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 68.3 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 70.7 52.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 70.9 42.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	37.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 55.8 38.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	36.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 57.3 +F EA TS st dia n 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	9 66.4 47.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 66.4 52.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 61.2 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 60.7 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 69.0 51.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 67.3 55.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 70.4 46.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	2 61.7 64.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 74.5 56.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 70.1 58.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 68.9 50.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	0 57.2 43.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	3 61.7 38.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second row represents performance median hyperparameter setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Section 5.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.1 Data Sets.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	train test CoNLL-X training set.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Statistics data sets shown Table 2.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5.2 Setup.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	encodes one tag per word constraint uni form type-level tag assignments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	final model tions.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	perform five runs different random initialization sampling state.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report results best median hyperparameter settings obtained way.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Specifically, settings report results median run setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	tokenize MWUs POS tags; reduces tag set size 12.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	See Table 2 tag set size languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	exception Dutch data set, processing performed annotated tags.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	6 Results Analysis.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	report token- type-level accuracy Table 3 6 languages system settings.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	system Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) reports best unsupervised results English.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	consider two variants Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Berg-Kirkpatrick et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	instance, Spanish, absolute gap median performance 10%.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	second point comparison Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	compare Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) Portuguese (Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	However, full model takes advantage word features present Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	(2009).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	novel element model ability capture type-level tag frequencies.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Similar behavior observed adding features.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 1 0.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	1 2 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 2.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	8 1 8.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	7 Conclusion Future Work.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	resulting model compact, efficiently learnable linguistically expressive.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	paper, make simplifying assumption one-tag-per-word.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	assumption, however, inherent type-based tagging models.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	promising direction future work explicitly model distribution tags word type.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	1
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	thank members MIT NLP group suggestions comments.	0
Systems inducing syntactic categories often make use morpheme-like features, word-final characters (Smith Eisner, 2005; Haghighi Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Simple Type-Level Unsupervised POS Tagging	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, existing systems, expansion come steep increase model complexity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	several languages, report performance exceeding complex state-of-the art systems.1	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	— similar results observed across multiple languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design, readily capture regularities token-level.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work, take direct approach treat word type allowed POS tags primary element model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Across languages, high performance attained selecting single tag per word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	token-level HMM reflect lexicon sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	two key benefits model architecture.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	several languages, report performance exceeding state-of-the art systems.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	extent constraint enforced varies greatly across existing methods.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	clusters computed using SVD variant without relying transitional structure.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	approaches encode sparsity soft constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design guarantee “structural zeros,” biases towards sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	use ILP learning desired grammar significantly increases computational complexity method.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast approaches, method directly incorporates constraints structure model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design leads significant reduction computational complexity training inference.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	consider unsupervised POS induction problem without use tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	graphical depiction model well summary random variables parameters found Figure 1.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	standard, use fixed constant K number tagging states.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Conditioned , features word types W drawn.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	parameters depend single hyperparameter α.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	total O(K 2) parameters associated transition parameters.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3.1 Lexicon Component.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	purpose 3 follows since θt St − 1 parameters and.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	set fixed constants.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	explore well induce POS tags using one-tag-per-word constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model, associate features type-level lexicon.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	i=1 (f,v)∈Wi	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	inference, interested posterior probability latent variables model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Performance typically stabilizes across languages number iterations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	t(i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	use w erations sampling (see Figure 2 depiction).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	language investigate contribution component model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	languages make use tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 62.6 45.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 61.7 37.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 56.2 32.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 53.8 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 53.7 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 61.0 44.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 62.2 39.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 68.4 49.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 68.4 48.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 68.1 34.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 54.4 33.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	36.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 55.3 34.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 50.2 +P RI st dia n 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 65.5 46.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 64.7 42.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 58.3 40.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 57.3 51.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 65.9 48.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 60.7 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	41.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	7 68.3 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 70.7 52.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 70.9 42.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	37.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 55.8 38.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	36.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 57.3 +F EA TS st dia n 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 66.4 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 66.4 52.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 61.2 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 60.7 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 69.0 51.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 67.3 55.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 70.4 46.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 61.7 64.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 74.5 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 70.1 58.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 68.9 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 57.2 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 61.7 38.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second row represents performance median hyperparameter setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	See Section 5.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5.1 Data Sets.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	train test CoNLL-X training set.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Statistics data sets shown Table 2.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5.2 Setup.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	encodes one tag per word constraint uni form type-level tag assignments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	final model tions.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	perform five runs different random initialization sampling state.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	report results best median hyperparameter settings obtained way.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, settings report results median run setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	tokenize MWUs POS tags; reduces tag set size 12.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	See Table 2 tag set size languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	exception Dutch data set, processing performed annotated tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	6 Results Analysis.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	report token- type-level accuracy Table 3 6 languages system settings.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	system Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) reports best unsupervised results English.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	consider two variants Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	instance, Spanish, absolute gap median performance 10%.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second point comparison Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	compare Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009) Portuguese (Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, full model takes advantage word features present Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	novel element model ability capture type-level tag frequencies.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Similar behavior observed adding features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 1 0.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 2 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 1 2.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 1 8.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	7 Conclusion Future Work.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	resulting model compact, efficiently learnable linguistically expressive.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	paper, make simplifying assumption one-tag-per-word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	assumption, however, inherent type-based tagging models.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	promising direction future work explicitly model distribution tags word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	1
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	thank members MIT NLP group suggestions comments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Simple Type-Level Unsupervised POS Tagging	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, existing systems, expansion come steep increase model complexity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	several languages, report performance exceeding complex state-of-the art systems.1	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	— similar results observed across multiple languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design, readily capture regularities token-level.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work, take direct approach treat word type allowed POS tags primary element model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Across languages, high performance attained selecting single tag per word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	token-level HMM reflect lexicon sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	two key benefits model architecture.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	several languages, report performance exceeding state-of-the art systems.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	extent constraint enforced varies greatly across existing methods.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	clusters computed using SVD variant without relying transitional structure.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	approaches encode sparsity soft constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design guarantee “structural zeros,” biases towards sparsity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	use ILP learning desired grammar significantly increases computational complexity method.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast approaches, method directly incorporates constraints structure model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	design leads significant reduction computational complexity training inference.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	consider unsupervised POS induction problem without use tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	graphical depiction model well summary random variables parameters found Figure 1.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	standard, use fixed constant K number tagging states.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Conditioned , features word types W drawn.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	parameters depend single hyperparameter α.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	total O(K 2) parameters associated transition parameters.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3.1 Lexicon Component.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	purpose 3 follows since θt St − 1 parameters and.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	set fixed constants.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	explore well induce POS tags using one-tag-per-word constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model, associate features type-level lexicon.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	i=1 (f,v)∈Wi	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	inference, interested posterior probability latent variables model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Performance typically stabilizes across languages number iterations.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	t(i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	use w erations sampling (see Figure 2 depiction).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	language investigate contribution component model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	languages make use tagging dictionary.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 62.6 45.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 61.7 37.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 56.2 32.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 53.8 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 53.7 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 61.0 44.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 62.2 39.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 68.4 49.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 68.4 48.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 68.1 34.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 54.4 33.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	36.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 55.3 34.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 50.2 +P RI st dia n 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 65.5 46.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 64.7 42.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 58.3 40.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 57.3 51.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 65.9 48.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 60.7 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	41.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	7 68.3 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 70.7 52.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 70.9 42.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	37.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 55.8 38.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	36.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 57.3 +F EA TS st dia n 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	9 66.4 47.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 66.4 52.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 61.2 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 60.7 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 69.0 51.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 67.3 55.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 70.4 46.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	2 61.7 64.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 74.5 56.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 70.1 58.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 68.9 50.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	0 57.2 43.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	3 61.7 38.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second row represents performance median hyperparameter setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	See Section 5.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5.1 Data Sets.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	train test CoNLL-X training set.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Statistics data sets shown Table 2.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5.2 Setup.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	encodes one tag per word constraint uni form type-level tag assignments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	final model tions.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	perform five runs different random initialization sampling state.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	report results best median hyperparameter settings obtained way.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Specifically, settings report results median run setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	tokenize MWUs POS tags; reduces tag set size 12.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	See Table 2 tag set size languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	exception Dutch data set, processing performed annotated tags.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	6 Results Analysis.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	report token- type-level accuracy Table 3 6 languages system settings.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	system Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) reports best unsupervised results English.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	consider two variants Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Berg-Kirkpatrick et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	instance, Spanish, absolute gap median performance 10%.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	second point comparison Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	compare Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009) Portuguese (Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	However, full model takes advantage word features present Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	(2009).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	novel element model ability capture type-level tag frequencies.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Similar behavior observed adding features.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 1 0.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	1 2 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 1 2.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	8 1 8.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	7 Conclusion Future Work.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	resulting model compact, efficiently learnable linguistically expressive.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	paper, make simplifying assumption one-tag-per-word.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	assumption, however, inherent type-based tagging models.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	promising direction future work explicitly model distribution tags word type.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	1
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	thank members MIT NLP group suggestions comments.	0
Several unsupervised POS induction systems make use morphological features (Blunsom Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Simple Type-Level Unsupervised POS Tagging	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, existing systems, expansion come steep increase model complexity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	— similar results observed across multiple languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design, readily capture regularities token-level.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Across languages, high performance attained selecting single tag per word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	token-level HMM reflect lexicon sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	two key benefits model architecture.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	several languages, report performance exceeding state-of-the art systems.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	extent constraint enforced varies greatly across existing methods.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	clusters computed using SVD variant without relying transitional structure.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	approaches encode sparsity soft constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design guarantee “structural zeros,” biases towards sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast approaches, method directly incorporates constraints structure model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design leads significant reduction computational complexity training inference.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	standard, use fixed constant K number tagging states.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Conditioned , features word types W drawn.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	parameters depend single hyperparameter α.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	total O(K 2) parameters associated transition parameters.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3.1 Lexicon Component.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	purpose 3 follows since θt St − 1 parameters and.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	set fixed constants.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	explore well induce POS tags using one-tag-per-word constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model, associate features type-level lexicon.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	i=1 (f,v)∈Wi	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	inference, interested posterior probability latent variables model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Performance typically stabilizes across languages number iterations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	t(i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	use w erations sampling (see Figure 2 depiction).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	language investigate contribution component model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	languages make use tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 62.6 45.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 61.7 37.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 56.2 32.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 53.8 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 53.7 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 61.0 44.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 62.2 39.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 68.4 49.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 68.4 48.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 68.1 34.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 54.4 33.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	36.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 55.3 34.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 50.2 +P RI st dia n 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 65.5 46.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 64.7 42.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 58.3 40.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 57.3 51.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 65.9 48.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 60.7 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	41.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	7 68.3 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 70.7 52.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 70.9 42.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	37.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 55.8 38.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	36.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 57.3 +F EA TS st dia n 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 66.4 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 66.4 52.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 61.2 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 60.7 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 69.0 51.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 67.3 55.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 70.4 46.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 61.7 64.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 74.5 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 70.1 58.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 68.9 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 57.2 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 61.7 38.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second row represents performance median hyperparameter setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	See Section 5.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5.1 Data Sets.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	train test CoNLL-X training set.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Statistics data sets shown Table 2.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5.2 Setup.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	final model tions.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	perform five runs different random initialization sampling state.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	report results best median hyperparameter settings obtained way.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, settings report results median run setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	See Table 2 tag set size languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	exception Dutch data set, processing performed annotated tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	6 Results Analysis.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	system Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) reports best unsupervised results English.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	consider two variants Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	instance, Spanish, absolute gap median performance 10%.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second point comparison Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	compare Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009) Portuguese (Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, full model takes advantage word features present Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	novel element model ability capture type-level tag frequencies.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Similar behavior observed adding features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 1 0.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 2 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 1 2.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 1 8.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	7 Conclusion Future Work.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	resulting model compact, efficiently learnable linguistically expressive.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	paper, make simplifying assumption one-tag-per-word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	assumption, however, inherent type-based tagging models.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	promising direction future work explicitly model distribution tags word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	thank members MIT NLP group suggestions comments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Simple Type-Level Unsupervised POS Tagging	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, existing systems, expansion come steep increase model complexity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	— similar results observed across multiple languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design, readily capture regularities token-level.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Across languages, high performance attained selecting single tag per word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	token-level HMM reflect lexicon sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	two key benefits model architecture.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	several languages, report performance exceeding state-of-the art systems.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	extent constraint enforced varies greatly across existing methods.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	clusters computed using SVD variant without relying transitional structure.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	approaches encode sparsity soft constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design guarantee “structural zeros,” biases towards sparsity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast approaches, method directly incorporates constraints structure model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	design leads significant reduction computational complexity training inference.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	standard, use fixed constant K number tagging states.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Conditioned , features word types W drawn.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	parameters depend single hyperparameter α.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	total O(K 2) parameters associated transition parameters.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3.1 Lexicon Component.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	purpose 3 follows since θt St − 1 parameters and.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	set fixed constants.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	explore well induce POS tags using one-tag-per-word constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model, associate features type-level lexicon.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	i=1 (f,v)∈Wi	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	inference, interested posterior probability latent variables model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Performance typically stabilizes across languages number iterations.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	t(i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	use w erations sampling (see Figure 2 depiction).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	language investigate contribution component model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	languages make use tagging dictionary.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 62.6 45.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 61.7 37.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 56.2 32.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 53.8 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 53.7 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 61.0 44.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 62.2 39.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 68.4 49.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 68.4 48.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 68.1 34.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 54.4 33.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	36.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 55.3 34.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 50.2 +P RI st dia n 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 65.5 46.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 64.7 42.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 58.3 40.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 57.3 51.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 65.9 48.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 60.7 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	41.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	7 68.3 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 70.7 52.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 70.9 42.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	37.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 55.8 38.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	36.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 57.3 +F EA TS st dia n 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	9 66.4 47.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 66.4 52.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 61.2 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 60.7 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 69.0 51.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 67.3 55.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 70.4 46.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	2 61.7 64.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 74.5 56.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 70.1 58.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 68.9 50.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	0 57.2 43.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	3 61.7 38.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second row represents performance median hyperparameter setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	See Section 5.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5.1 Data Sets.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	train test CoNLL-X training set.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Statistics data sets shown Table 2.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5.2 Setup.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	final model tions.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	perform five runs different random initialization sampling state.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	report results best median hyperparameter settings obtained way.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Specifically, settings report results median run setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	See Table 2 tag set size languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	exception Dutch data set, processing performed annotated tags.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	6 Results Analysis.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	system Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) reports best unsupervised results English.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	consider two variants Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Berg-Kirkpatrick et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	instance, Spanish, absolute gap median performance 10%.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	second point comparison Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	compare Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009) Portuguese (Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	However, full model takes advantage word features present Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	(2009).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	novel element model ability capture type-level tag frequencies.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Similar behavior observed adding features.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 1 0.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	1 2 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 1 2.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	8 1 8.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	7 Conclusion Future Work.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	resulting model compact, efficiently learnable linguistically expressive.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	paper, make simplifying assumption one-tag-per-word.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	assumption, however, inherent type-based tagging models.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	promising direction future work explicitly model distribution tags word type.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	thank members MIT NLP group suggestions comments.	0
"Recently Lee et al.</S><S sid =""41"" ssid = ""21"">(2010) combined one class per word type constraint (Brown et al., 1992) HMM Dirichlet prior achieve forms sparsity.</S><S sid =""42"" ssid = ""22"">However work approximated derivation Gibbs sampler (omitting interdependence events sampling collapsed model), resulting model underperformed Brown et al.</S><S sid =""43"" ssid = ""23"">(1992)â€™s one-class HMM."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Simple Type-Level Unsupervised POS Tagging	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, existing systems, expansion come steep increase model complexity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	several languages, report performance exceeding complex state-of-the art systems.1	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	— similar results observed across multiple languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design, readily capture regularities token-level.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work, take direct approach treat word type allowed POS tags primary element model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Across languages, high performance attained selecting single tag per word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	token-level HMM reflect lexicon sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	two key benefits model architecture.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	evaluate model seven languages exhibiting substantial syntactic variation.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	several languages, report performance exceeding state-of-the art systems.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	extent constraint enforced varies greatly across existing methods.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	clusters computed using SVD variant without relying transitional structure.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	approaches encode sparsity soft constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design guarantee “structural zeros,” biases towards sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	use ILP learning desired grammar significantly increases computational complexity method.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast approaches, method directly incorporates constraints structure model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design leads significant reduction computational complexity training inference.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	consider unsupervised POS induction problem without use tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	graphical depiction model well summary random variables parameters found Figure 1.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	standard, use fixed constant K number tagging states.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Conditioned , features word types W drawn.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	parameters depend single hyperparameter α.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	total O(K 2) parameters associated transition parameters.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3.1 Lexicon Component.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	purpose 3 follows since θt St − 1 parameters and.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	set fixed constants.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	explore well induce POS tags using one-tag-per-word constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model, associate features type-level lexicon.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast, NNP (proper nouns) form large portion vocabulary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, assume word type W consists feature-value pairs (f, v).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	i=1 (f,v)∈Wi	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	inference, interested posterior probability latent variables model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Performance typically stabilizes across languages number iterations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	t(i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	terms right-hand-side denote type-level token-level probability terms respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	use w erations sampling (see Figure 2 depiction).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	language investigate contribution component model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	languages make use tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 62.6 45.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 61.7 37.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 56.2 32.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 53.8 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 53.7 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 61.0 44.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 62.2 39.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 68.4 49.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 68.4 48.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 68.1 34.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 54.4 33.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	36.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 55.3 34.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 50.2 +P RI st dia n 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 65.5 46.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 64.7 42.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 58.3 40.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 57.3 51.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 65.9 48.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 60.7 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	41.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	7 68.3 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 70.7 52.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 70.9 42.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	37.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 55.8 38.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	36.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 57.3 +F EA TS st dia n 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 66.4 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 66.4 52.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 61.2 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 60.7 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 69.0 51.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 67.3 55.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 70.4 46.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 61.7 64.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 74.5 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 70.1 58.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 68.9 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 57.2 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 61.7 38.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	1
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second row represents performance median hyperparameter setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	See Section 5.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5.1 Data Sets.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	train test CoNLL-X training set.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Statistics data sets shown Table 2.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5.2 Setup.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	β shared hyperparameter tag assignment prior word feature multinomials.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Evaluation Metrics report three metrics evaluate tagging performance.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	encodes one tag per word constraint uni form type-level tag assignments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	final model tions.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	perform five runs different random initialization sampling state.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Hyperparameter settings sorted according median one-to-one metric runs.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	report results best median hyperparameter settings obtained way.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, settings report results median run setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	tokenize MWUs POS tags; reduces tag set size 12.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	See Table 2 tag set size languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	exception Dutch data set, processing performed annotated tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	6 Results Analysis.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	report token- type-level accuracy Table 3 6 languages system settings.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) posterior regular- ization HMM Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	system Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) reports best unsupervised results English.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	consider two variants Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	instance, Spanish, absolute gap median performance 10%.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second point comparison Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	compare Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009) Portuguese (Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, full model takes advantage word features present Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	novel element model ability capture type-level tag frequencies.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Similar behavior observed adding features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 1 0.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 2 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 1 2.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 1 8.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	table shows lexicon tag frequency predicated full model closest gold standard.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	7 Conclusion Future Work.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	resulting model compact, efficiently learnable linguistically expressive.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	paper, make simplifying assumption one-tag-per-word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	assumption, however, inherent type-based tagging models.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	promising direction future work explicitly model distribution tags word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	thank members MIT NLP group suggestions comments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Simple Type-Level Unsupervised POS Tagging	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, existing systems, expansion come steep increase model complexity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	several languages, report performance exceeding complex state-of-the art systems.1	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	— similar results observed across multiple languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design, readily capture regularities token-level.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work, take direct approach treat word type allowed POS tags primary element model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Across languages, high performance attained selecting single tag per word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	token-level HMM reflect lexicon sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	two key benefits model architecture.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	evaluate model seven languages exhibiting substantial syntactic variation.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	several languages, report performance exceeding state-of-the art systems.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	extent constraint enforced varies greatly across existing methods.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	clusters computed using SVD variant without relying transitional structure.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	approaches encode sparsity soft constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design guarantee “structural zeros,” biases towards sparsity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	use ILP learning desired grammar significantly increases computational complexity method.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast approaches, method directly incorporates constraints structure model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	design leads significant reduction computational complexity training inference.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	consider unsupervised POS induction problem without use tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	graphical depiction model well summary random variables parameters found Figure 1.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	standard, use fixed constant K number tagging states.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Conditioned , features word types W drawn.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	parameters depend single hyperparameter α.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	total O(K 2) parameters associated transition parameters.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3.1 Lexicon Component.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	purpose 3 follows since θt St − 1 parameters and.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	set fixed constants.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	explore well induce POS tags using one-tag-per-word constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model, associate features type-level lexicon.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast, NNP (proper nouns) form large portion vocabulary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, assume word type W consists feature-value pairs (f, v).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	i=1 (f,v)∈Wi	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	inference, interested posterior probability latent variables model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Performance typically stabilizes across languages number iterations.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	t(i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	terms right-hand-side denote type-level token-level probability terms respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	use w erations sampling (see Figure 2 depiction).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	language investigate contribution component model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	languages make use tagging dictionary.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 62.6 45.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 61.7 37.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 56.2 32.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 53.8 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 53.7 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 61.0 44.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 62.2 39.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 68.4 49.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 68.4 48.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 68.1 34.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 54.4 33.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	36.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 55.3 34.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 50.2 +P RI st dia n 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 65.5 46.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 64.7 42.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 58.3 40.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 57.3 51.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 65.9 48.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 60.7 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	41.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	7 68.3 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 70.7 52.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 70.9 42.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	37.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 55.8 38.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	36.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 57.3 +F EA TS st dia n 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	9 66.4 47.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 66.4 52.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 61.2 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 60.7 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 69.0 51.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 67.3 55.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 70.4 46.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	2 61.7 64.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 74.5 56.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 70.1 58.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 68.9 50.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	0 57.2 43.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	3 61.7 38.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	1
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second row represents performance median hyperparameter setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	See Section 5.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5.1 Data Sets.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	train test CoNLL-X training set.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Statistics data sets shown Table 2.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5.2 Setup.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	β shared hyperparameter tag assignment prior word feature multinomials.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Evaluation Metrics report three metrics evaluate tagging performance.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	encodes one tag per word constraint uni form type-level tag assignments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	final model tions.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	perform five runs different random initialization sampling state.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Hyperparameter settings sorted according median one-to-one metric runs.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	report results best median hyperparameter settings obtained way.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Specifically, settings report results median run setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	tokenize MWUs POS tags; reduces tag set size 12.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	See Table 2 tag set size languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	exception Dutch data set, processing performed annotated tags.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	6 Results Analysis.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	report token- type-level accuracy Table 3 6 languages system settings.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) posterior regular- ization HMM Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	system Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) reports best unsupervised results English.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	consider two variants Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Berg-Kirkpatrick et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	instance, Spanish, absolute gap median performance 10%.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	second point comparison Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	compare Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009) Portuguese (Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	However, full model takes advantage word features present Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	(2009).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	novel element model ability capture type-level tag frequencies.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Similar behavior observed adding features.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 1 0.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	1 2 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 1 2.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	8 1 8.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	table shows lexicon tag frequency predicated full model closest gold standard.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	7 Conclusion Future Work.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	resulting model compact, efficiently learnable linguistically expressive.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	paper, make simplifying assumption one-tag-per-word.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	assumption, however, inherent type-based tagging models.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	promising direction future work explicitly model distribution tags word type.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	thank members MIT NLP group suggestions comments.	0
"also interesting compare bigram PYP1HMM closely related model Lee et al.</S><S sid =""153"" ssid = ""34"">(2010).</S><S sid =""154"" ssid = ""35"">That model incorrectly assumed independence conditional sampling distributions, resulting accuracy 66.4%"	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Simple Type-Level Unsupervised POS Tagging	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, existing systems, expansion come steep increase model complexity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	several languages, report performance exceeding complex state-of-the art systems.1	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	— similar results observed across multiple languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design, readily capture regularities token-level.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work, take direct approach treat word type allowed POS tags primary element model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Across languages, high performance attained selecting single tag per word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	token-level HMM reflect lexicon sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	two key benefits model architecture.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	several languages, report performance exceeding state-of-the art systems.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	extent constraint enforced varies greatly across existing methods.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	clusters computed using SVD variant without relying transitional structure.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	approaches encode sparsity soft constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design guarantee “structural zeros,” biases towards sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	use ILP learning desired grammar significantly increases computational complexity method.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast approaches, method directly incorporates constraints structure model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design leads significant reduction computational complexity training inference.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	consider unsupervised POS induction problem without use tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	graphical depiction model well summary random variables parameters found Figure 1.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	standard, use fixed constant K number tagging states.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Conditioned , features word types W drawn.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	parameters depend single hyperparameter α.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	total O(K 2) parameters associated transition parameters.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3.1 Lexicon Component.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	purpose 3 follows since θt St − 1 parameters and.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	set fixed constants.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	explore well induce POS tags using one-tag-per-word constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model, associate features type-level lexicon.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	i=1 (f,v)∈Wi	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	inference, interested posterior probability latent variables model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Performance typically stabilizes across languages number iterations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	t(i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	use w erations sampling (see Figure 2 depiction).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	language investigate contribution component model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	languages make use tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 62.6 45.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 61.7 37.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 56.2 32.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 53.8 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 53.7 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 61.0 44.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 62.2 39.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 68.4 49.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 68.4 48.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 68.1 34.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 54.4 33.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	36.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 55.3 34.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 50.2 +P RI st dia n 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 65.5 46.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 64.7 42.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 58.3 40.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 57.3 51.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 65.9 48.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 60.7 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	41.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	7 68.3 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 70.7 52.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 70.9 42.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	37.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 55.8 38.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	36.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 57.3 +F EA TS st dia n 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 66.4 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 66.4 52.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 61.2 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 60.7 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 69.0 51.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 67.3 55.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 70.4 46.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 61.7 64.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 74.5 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 70.1 58.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 68.9 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 57.2 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 61.7 38.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second row represents performance median hyperparameter setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	See Section 5.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5.1 Data Sets.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	train test CoNLL-X training set.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Statistics data sets shown Table 2.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5.2 Setup.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	encodes one tag per word constraint uni form type-level tag assignments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	final model tions.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	perform five runs different random initialization sampling state.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	report results best median hyperparameter settings obtained way.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, settings report results median run setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	tokenize MWUs POS tags; reduces tag set size 12.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	See Table 2 tag set size languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	exception Dutch data set, processing performed annotated tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	6 Results Analysis.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	report token- type-level accuracy Table 3 6 languages system settings.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	system Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) reports best unsupervised results English.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	consider two variants Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	instance, Spanish, absolute gap median performance 10%.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second point comparison Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	compare Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009) Portuguese (Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, full model takes advantage word features present Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	novel element model ability capture type-level tag frequencies.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Similar behavior observed adding features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 1 0.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 2 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 1 2.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 1 8.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	7 Conclusion Future Work.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	1
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	resulting model compact, efficiently learnable linguistically expressive.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	paper, make simplifying assumption one-tag-per-word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	assumption, however, inherent type-based tagging models.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	promising direction future work explicitly model distribution tags word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	thank members MIT NLP group suggestions comments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Simple Type-Level Unsupervised POS Tagging	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, existing systems, expansion come steep increase model complexity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	several languages, report performance exceeding complex state-of-the art systems.1	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	— similar results observed across multiple languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design, readily capture regularities token-level.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work, take direct approach treat word type allowed POS tags primary element model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Across languages, high performance attained selecting single tag per word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	token-level HMM reflect lexicon sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	two key benefits model architecture.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	evaluate model seven languages exhibiting substantial syntactic variation.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	several languages, report performance exceeding state-of-the art systems.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	extent constraint enforced varies greatly across existing methods.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	clusters computed using SVD variant without relying transitional structure.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	approaches encode sparsity soft constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design guarantee “structural zeros,” biases towards sparsity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	use ILP learning desired grammar significantly increases computational complexity method.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast approaches, method directly incorporates constraints structure model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	design leads significant reduction computational complexity training inference.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	consider unsupervised POS induction problem without use tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	graphical depiction model well summary random variables parameters found Figure 1.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	standard, use fixed constant K number tagging states.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Conditioned , features word types W drawn.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	parameters depend single hyperparameter α.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	total O(K 2) parameters associated transition parameters.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3.1 Lexicon Component.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	purpose 3 follows since θt St − 1 parameters and.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	set fixed constants.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	explore well induce POS tags using one-tag-per-word constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model, associate features type-level lexicon.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast, NNP (proper nouns) form large portion vocabulary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, assume word type W consists feature-value pairs (f, v).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	i=1 (f,v)∈Wi	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	inference, interested posterior probability latent variables model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Performance typically stabilizes across languages number iterations.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	t(i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	terms right-hand-side denote type-level token-level probability terms respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	use w erations sampling (see Figure 2 depiction).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	language investigate contribution component model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	languages make use tagging dictionary.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 62.6 45.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 61.7 37.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 56.2 32.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 53.8 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 53.7 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 61.0 44.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 62.2 39.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 68.4 49.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 68.4 48.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 68.1 34.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 54.4 33.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	36.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 55.3 34.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 50.2 +P RI st dia n 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 65.5 46.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 64.7 42.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 58.3 40.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 57.3 51.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 65.9 48.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 60.7 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	41.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	7 68.3 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 70.7 52.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 70.9 42.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	37.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 55.8 38.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	36.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 57.3 +F EA TS st dia n 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	9 66.4 47.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 66.4 52.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 61.2 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 60.7 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 69.0 51.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 67.3 55.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 70.4 46.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	2 61.7 64.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 74.5 56.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 70.1 58.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 68.9 50.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	0 57.2 43.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	3 61.7 38.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second row represents performance median hyperparameter setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	See Section 5.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5.1 Data Sets.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	train test CoNLL-X training set.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Statistics data sets shown Table 2.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5.2 Setup.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	β shared hyperparameter tag assignment prior word feature multinomials.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Evaluation Metrics report three metrics evaluate tagging performance.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	encodes one tag per word constraint uni form type-level tag assignments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	final model tions.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	perform five runs different random initialization sampling state.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Hyperparameter settings sorted according median one-to-one metric runs.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	report results best median hyperparameter settings obtained way.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Specifically, settings report results median run setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	tokenize MWUs POS tags; reduces tag set size 12.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	See Table 2 tag set size languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	exception Dutch data set, processing performed annotated tags.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	6 Results Analysis.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	report token- type-level accuracy Table 3 6 languages system settings.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) posterior regular- ization HMM Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	system Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) reports best unsupervised results English.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	consider two variants Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Berg-Kirkpatrick et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	instance, Spanish, absolute gap median performance 10%.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	second point comparison Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	compare Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009) Portuguese (Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	However, full model takes advantage word features present Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	(2009).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	novel element model ability capture type-level tag frequencies.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Similar behavior observed adding features.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 1 0.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	1 2 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 1 2.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	8 1 8.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	table shows lexicon tag frequency predicated full model closest gold standard.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	7 Conclusion Future Work.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	1
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	resulting model compact, efficiently learnable linguistically expressive.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	paper, make simplifying assumption one-tag-per-word.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	assumption, however, inherent type-based tagging models.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	promising direction future work explicitly model distribution tags word type.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	thank members MIT NLP group suggestions comments.	0
Similar constraints developed part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	— similar results observed across multiple languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design, readily capture regularities token-level.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	two key benefits model architecture.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Conditioned , features word types W drawn.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	parameters depend single hyperparameter α.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3.1 Lexicon Component.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	set fixed constants.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model, associate features type-level lexicon.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	i=1 (f,v)∈Wi	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	t(i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	language investigate contribution component model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	languages make use tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 62.6 45.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 61.7 37.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 56.2 32.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 53.8 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 53.7 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 61.0 44.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 62.2 39.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 68.4 49.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 68.4 48.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 68.1 34.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 54.4 33.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	36.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 55.3 34.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 65.5 46.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 64.7 42.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 58.3 40.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 57.3 51.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 65.9 48.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 60.7 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	41.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	7 68.3 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 70.7 52.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 70.9 42.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	37.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 55.8 38.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	36.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 66.4 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 66.4 52.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 61.2 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 60.7 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 69.0 51.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 67.3 55.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 70.4 46.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 61.7 64.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 74.5 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 70.1 58.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 68.9 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 57.2 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 61.7 38.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	See Section 5.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5.1 Data Sets.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	train test CoNLL-X training set.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Statistics data sets shown Table 2.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5.2 Setup.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	final model tions.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, settings report results median run setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	See Table 2 tag set size languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	6 Results Analysis.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second point comparison Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	compare Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Similar behavior observed adding features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 1 0.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 2 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 1 2.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 1 8.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	7 Conclusion Future Work.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Simple Type-Level Unsupervised POS Tagging	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, existing systems, expansion come steep increase model complexity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	several languages, report performance exceeding complex state-of-the art systems.1	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	— similar results observed across multiple languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design, readily capture regularities token-level.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work, take direct approach treat word type allowed POS tags primary element model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Across languages, high performance attained selecting single tag per word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	token-level HMM reflect lexicon sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	two key benefits model architecture.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	evaluate model seven languages exhibiting substantial syntactic variation.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	several languages, report performance exceeding state-of-the art systems.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	extent constraint enforced varies greatly across existing methods.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	clusters computed using SVD variant without relying transitional structure.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	approaches encode sparsity soft constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design guarantee “structural zeros,” biases towards sparsity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	use ILP learning desired grammar significantly increases computational complexity method.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast approaches, method directly incorporates constraints structure model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	design leads significant reduction computational complexity training inference.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	consider unsupervised POS induction problem without use tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	graphical depiction model well summary random variables parameters found Figure 1.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	standard, use fixed constant K number tagging states.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Conditioned , features word types W drawn.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	parameters depend single hyperparameter α.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	total O(K 2) parameters associated transition parameters.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast Bayesian HMM, θt drawn distribution support n word types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3.1 Lexicon Component.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	purpose 3 follows since θt St − 1 parameters and.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	tokens w generated token-level tags HMM parameterized lexicon structure.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	set fixed constants.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	explore well induce POS tags using one-tag-per-word constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model, associate features type-level lexicon.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast, NNP (proper nouns) form large portion vocabulary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, assume word type W consists feature-value pairs (f, v).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	i=1 (f,v)∈Wi	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	inference, interested posterior probability latent variables model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Performance typically stabilizes across languages number iterations.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	t(i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	terms right-hand-side denote type-level token-level probability terms respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	crucial difference number parameters greatly reduced number variables sampled iteration.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	token-level term similar standard HMM sampling equations found Johnson (2007).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	use w erations sampling (see Figure 2 depiction).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	language investigate contribution component model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	languages make use tagging dictionary.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 62.6 45.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 61.7 37.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 56.2 32.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 53.8 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 53.7 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 61.0 44.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 62.2 39.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 68.4 49.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 68.4 48.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 68.1 34.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 54.4 33.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	36.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 55.3 34.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 50.2 +P RI st dia n 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 65.5 46.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 64.7 42.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 58.3 40.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 57.3 51.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 65.9 48.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 60.7 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	41.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	7 68.3 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 70.7 52.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 70.9 42.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	37.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 55.8 38.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	36.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 57.3 +F EA TS st dia n 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	9 66.4 47.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 66.4 52.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 61.2 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 60.7 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 69.0 51.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 67.3 55.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 70.4 46.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	2 61.7 64.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 74.5 56.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 70.1 58.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 68.9 50.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	0 57.2 43.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	3 61.7 38.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second row represents performance median hyperparameter setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	See Section 5.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5.1 Data Sets.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	train test CoNLL-X training set.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Statistics data sets shown Table 2.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5.2 Setup.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	β shared hyperparameter tag assignment prior word feature multinomials.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Evaluation Metrics report three metrics evaluate tagging performance.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	encodes one tag per word constraint uni form type-level tag assignments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	final model tions.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	perform five runs different random initialization sampling state.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Hyperparameter settings sorted according median one-to-one metric runs.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	report results best median hyperparameter settings obtained way.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Specifically, settings report results median run setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	tokenize MWUs POS tags; reduces tag set size 12.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	See Table 2 tag set size languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	exception Dutch data set, processing performed annotated tags.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	6 Results Analysis.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	report token- type-level accuracy Table 3 6 languages system settings.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) posterior regular- ization HMM Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	system Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) reports best unsupervised results English.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	consider two variants Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Berg-Kirkpatrick et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2010) consistently outperforms English, obtain substantial gains across languages.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	instance, Spanish, absolute gap median performance 10%.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	second point comparison Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	compare Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009) Portuguese (Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009) also report results English, reduced 17 tag set, comparable ours).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	However, full model takes advantage word features present Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	(2009).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	novel element model ability capture type-level tag frequencies.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Similar behavior observed adding features.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 1 0.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	1 2 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 1 2.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	8 1 8.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	table shows lexicon tag frequency predicated full model closest gold standard.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	7 Conclusion Future Work.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	resulting model compact, efficiently learnable linguistically expressive.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	paper, make simplifying assumption one-tag-per-word.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	assumption, however, inherent type-based tagging models.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	promising direction future work explicitly model distribution tags word type.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	thank members MIT NLP group suggestions comments.	0
Here, W refers set word types generated tag t. words, conditioned tag t, generate word w set word types W generated earlier (Lee et al., 2010).	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Simple Type-Level Unsupervised POS Tagging	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, existing systems, expansion come steep increase model complexity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	— similar results observed across multiple languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design, readily capture regularities token-level.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Across languages, high performance attained selecting single tag per word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	token-level HMM reflect lexicon sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	two key benefits model architecture.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	several languages, report performance exceeding state-of-the art systems.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	extent constraint enforced varies greatly across existing methods.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	clusters computed using SVD variant without relying transitional structure.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	approaches encode sparsity soft constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design guarantee “structural zeros,” biases towards sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast approaches, method directly incorporates constraints structure model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design leads significant reduction computational complexity training inference.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	standard, use fixed constant K number tagging states.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Conditioned , features word types W drawn.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	parameters depend single hyperparameter α.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	total O(K 2) parameters associated transition parameters.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3.1 Lexicon Component.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	purpose 3 follows since θt St − 1 parameters and.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	set fixed constants.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	explore well induce POS tags using one-tag-per-word constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model, associate features type-level lexicon.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	i=1 (f,v)∈Wi	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	inference, interested posterior probability latent variables model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Performance typically stabilizes across languages number iterations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	t(i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	use w erations sampling (see Figure 2 depiction).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	language investigate contribution component model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	languages make use tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 62.6 45.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 61.7 37.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 56.2 32.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 53.8 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 53.7 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 61.0 44.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 62.2 39.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 68.4 49.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 68.4 48.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 68.1 34.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 54.4 33.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	36.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 55.3 34.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 50.2 +P RI st dia n 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 65.5 46.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 64.7 42.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 58.3 40.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 57.3 51.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 65.9 48.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 60.7 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	41.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	7 68.3 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 70.7 52.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 70.9 42.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	37.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 55.8 38.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	36.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 57.3 +F EA TS st dia n 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 66.4 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 66.4 52.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 61.2 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 60.7 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 69.0 51.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 67.3 55.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 70.4 46.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 61.7 64.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 74.5 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 70.1 58.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 68.9 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 57.2 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 61.7 38.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second row represents performance median hyperparameter setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	See Section 5.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5.1 Data Sets.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	train test CoNLL-X training set.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Statistics data sets shown Table 2.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5.2 Setup.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	final model tions.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	perform five runs different random initialization sampling state.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	report results best median hyperparameter settings obtained way.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, settings report results median run setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	See Table 2 tag set size languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	exception Dutch data set, processing performed annotated tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	6 Results Analysis.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	system Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) reports best unsupervised results English.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	consider two variants Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	instance, Spanish, absolute gap median performance 10%.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second point comparison Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	compare Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009) Portuguese (Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, full model takes advantage word features present Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	novel element model ability capture type-level tag frequencies.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Similar behavior observed adding features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 1 0.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 2 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 1 2.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 1 8.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	7 Conclusion Future Work.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	1
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	resulting model compact, efficiently learnable linguistically expressive.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	paper, make simplifying assumption one-tag-per-word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	assumption, however, inherent type-based tagging models.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	promising direction future work explicitly model distribution tags word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	thank members MIT NLP group suggestions comments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Simple Type-Level Unsupervised POS Tagging	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Part-of-speech (POS) tag distributions known exhibit sparsity — word likely take single predominant tag corpus.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Recent research demonstrated incorporating sparsity constraint improves tagging accuracy.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, existing systems, expansion come steep increase model complexity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	paper proposes simple effective tagging method directly models tag sparsity distributional properties valid POS tag assignments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	addition, formulation results dramatic reduction number model parameters thereby, enabling unusually rapid training.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiments consistently demonstrate model architecture yields substantial performance gains complex tagging counterparts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	several languages, report performance exceeding complex state-of-the art systems.1	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Since early days statistical NLP, researchers observed part-of-speech tag distribution exhibits “one tag per discourse” sparsity — words likely select single predominant tag corpus, even several tags possible.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Simply assigning word frequent associated tag corpus achieves 94.6% accuracy WSJ portion Penn Treebank.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	distributional sparsity syntactic tags unique English 1 source code work presented paper available http://groups.csail.mit.edu/rbg/code/typetagging/.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	— similar results observed across multiple languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Clearly, explicitly modeling powerful constraint tagging assignment potential significantly improve accuracy unsupervised part-of-speech tagger learned without tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	practice, sparsity constraint difficult incorporate traditional POS induction system (Me´rialdo, 1994; Johnson, 2007; Gao Johnson, 2008; Grac¸a et al., 2009; Berg-Kirkpatrick et al., 2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	sequence models-based approaches commonly treat token-level tag assignment primary latent variable.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design, readily capture regularities token-level.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, approaches ill-equipped directly represent type-based constraints sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Previous work attempted incorporate constraints token-level models via heavy-handed modifications inference procedure objective function (e.g., posterior regularization ILP decoding) (Grac¸a et al., 2009; Ravi Knight, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	cases, however, expansions come steep increase model complexity, respect training procedure inference time.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work, take direct approach treat word type allowed POS tags primary element model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Then, token- level HMM emission parameters drawn conditioned assignments word allowed probability mass single assigned tag.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	way restrict parameterization Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound tagging accuracy assuming word type assigned majority POS tag.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Across languages, high performance attained selecting single tag per word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	token-level HMM reflect lexicon sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model admits simple Gibbs sampling algorithm number latent variables proportional number word types, rather size corpus standard HMM sampler (Johnson, 2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	two key benefits model architecture.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	First, directly encodes linguistic intuitions POS tag assignments: model structure reflects one-tag-per-word property, type- level tag prior captures skew tag assignments (e.g., fewer unique determiners unique nouns).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Second, reduced number hidden variables parameters dramatically speeds learning inference.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	evaluate model seven languages exhibiting substantial syntactic variation.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	several languages, report performance exceeding state-of-the art systems.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	analysis identifies three key factors driving performance gain: 1) selecting model structure directly encodes tag sparsity, 2) type-level prior tag assignments, 3) straightforward na¨ıveBayes approach incorporate features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	observed performance gains, coupled simplicity model implementation, makes compelling alternative existing complex counterparts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Recent work made significant progress unsupervised POS tagging (Me´rialdo, 1994; Smith Eisner, 2005; Haghighi Klein, 2006; Johnson,2007; Goldwater Griffiths, 2007; Gao John son, 2008; Ravi Knight, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work closely related recent approaches incorporate sparsity constraint POS induction process.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	line work motivated empirical findings standard EM-learned unsupervised HMM exhibit sufficient word tag sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	extent constraint enforced varies greatly across existing methods.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	one end spectrum clustering approaches assign single POS tag word type (Schutze, 1995; Lamar et al., 2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	clusters computed using SVD variant without relying transitional structure.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	method also enforces singe tag per word constraint, leverages transition distribution encoded HMM, thereby benefiting richer representation context.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	approaches encode sparsity soft constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	instance, altering emission distribution parameters, Johnson (2007) encourages model put probability mass tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design guarantee “structural zeros,” biases towards sparsity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	forceful approach encoding sparsity posterior regularization, constrains posterior small number expected tag assignments (Grac¸a et al., 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	approach makes training objective complex adding linear constraints proportional number word types, rather prohibitive.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	rigid mechanism modeling sparsity proposed Ravi Knight (2009), minimize size tagging grammar measured number transition types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	use ILP learning desired grammar significantly increases computational complexity method.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast approaches, method directly incorporates constraints structure model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	design leads significant reduction computational complexity training inference.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Another thread relevant research explored use features unsupervised POS induction (Smith Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan Ng, 2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	methods demonstrated benefits incorporating linguistic features using log-linear parameterization, requires elaborate machinery training.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	work, demonstrate using simple na¨ıveBayes approach also yields substantial performance gains, without associated training complexity.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	consider unsupervised POS induction problem without use tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	graphical depiction model well summary random variables parameters found Figure 1.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	standard, use fixed constant K number tagging states.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Model Overview model starts generating tag assignment word type vocabulary, assuming one tag per word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Conditioned , features word types W drawn.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	refer (T , W ) lexicon language ψ parameters generation; ψ depends single hyperparameter β.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	lexicon drawn, model proceeds similarly standard token-level HMM: Emission parameters θ generated conditioned tag assignments . also draw transition parameters φ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	parameters depend single hyperparameter α.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	HMM parameters (θ, φ) drawn, token-level tag word sequence, (t, w), generated standard HMM fashion: tag sequence generated φ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	corresponding token words w drawn conditioned θ.2 full generative model given by: K P (φ, θ|T , α, β) = n (P (φt|α)P (θt|T , α)) t=1 transition distribution φt tag drawn according DIRICHLET(α, K ), α shared transition emission distribution hyperparameter.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	total O(K 2) parameters associated transition parameters.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast Bayesian HMM, θt drawn distribution support n word types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Instead, condition type-level tag assignments . Specifically, let St = {i|Ti = t} denote indices theword types assigned tag accord ing tag assignments . θt drawn DIRICHLET(α, St), symmetric Dirichlet places mass word types indicated St. ensures word assigned single tag inference time (see Section 4).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note standard HMM, O(K n) emission parameters, model O(n) effective parameters.3 Token Component HMM parameters (φ, θ) drawn, HMM generates token-level corpus w standard way: P (w, t|φ, θ) = P (T , W , θ, ψ, φ, t, w|α, β) = P (T , W , ψ|β) [Lexicon]  n n  (w,t)∈(w,t) j  P (tj |φtj−1 )P (wj |tj , θtj ) P (φ, θ|T , α, β) [Parameter] P (w, t|φ, θ) [Token] refer components right hand side lexicon, parameter, token component respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Since parameter token components remain fixed throughout experiments, briefly describe each.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Parameter Component standard Bayesian HMM (Goldwater Griffiths, 2007), distributions independently drawn symmetric Dirichlet distributions: 2 Note w denote tag word sequences respectively, rather individual tokens tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note model, conditioned , precisely one nonzero probability token component, since word, exactly one θt support.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3.1 Lexicon Component.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	present several variations lexical component P (T , W |ψ), adding complex pa rameterizations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Uniform Tag Prior (1TW) initial lexicon component uniform possible tag assignments well word types.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	purpose 3 follows since θt St − 1 parameters and.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P St = n. β VARIABLES ψ W : Word types (W1 ,.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	.., Wn ) (obs) P : Tag assigns (T1 ,.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	.., Tn ) W φ E w : Token word seqs (obs) : Token tag assigns (det ) PARAMETERS ψ : Lexicon parameters θ : Token word emission parameters φ : Token tag transition parameters φ φ t1 t2 θ θ w1 w2 K φ tm K θ E wN N N Figure 1: Graphical depiction model summary latent variables parameters.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	type-level tag assignments generate features associated word types W . tag assignments constrain HMM emission parameters θ.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	tokens w generated token-level tags HMM parameterized lexicon structure.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	hyperparameters α β represent concentration parameters token- type-level components model respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	set fixed constants.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	explore well induce POS tags using one-tag-per-word constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, lexicon generated as: P (T , W |ψ) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work derived benefits features word types, suffix capitalization features (Hasan Ng, 2009; Berg-Kirkpatrick et al.,2010).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Past work however, typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated features token occurrences, typically HMM.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model, associate features type-level lexicon.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Here, conThis model equivalent standard HMM ex cept enforces one-word-per-tag constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Learned Tag Prior (PRIOR) next assume exists single prior distribution ψ tag assignments drawn DIRICHLET(β, K ).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	alters generation follows: n P (T |ψ) = n P (Ti|ψ) i=1 Note distribution captures frequency tag across word types, opposed tokens.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P (T |ψ) distribution, English instance, low mass DT (determiner) tag, since determiners small portion vocabulary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast, NNP (proper nouns) form large portion vocabulary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Note observa sider suffix features, capitalization features, punctuation, digit features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	possible utilize feature-based log-linear approach described Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010), adopt simpler na¨ıve Bayes strategy, features emitted independently.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, assume word type W consists feature-value pairs (f, v).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	feature type f tag t, multinomial ψtf drawn symmetric Dirichlet distribution concentration parameter β.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	P (W |T , ψ) term lexicon component decomposes as: n P (W |T , ψ) = n P (Wi|Ti, ψ) i=1 n   tions modeled standard HMM, = n  n P (v|ψTi f ) instead model token-level frequency.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	i=1 (f,v)∈Wi	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	inference, interested posterior probability latent variables model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	training, treat observed language word types W well token-level corpus w. utilize Gibbs sampling approximate collapsed model posterior: P (T ,t|W , w, α, β) ∝ P (T , t, W , w|α, β) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, ψ, θ, φ, w|α, β)dψdθdφ Note given tag assignments , one setting token-level tags mass posterior.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, ith word type, set token-level tags associated token occurrences word, denoted t(i), must take value Ti nonzero mass. Thus context Gibbs sampling, want block sample Ti t(i), need sample values Ti consider setting t(i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	equation sampling single type-level assignment Ti given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph one-to-one accuracy full model (+FEATS) best hyperparameter setting iteration (see Section 5).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Performance typically stabilizes across languages number iterations.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	represent ith word type emitted HMM: P (t(i)|Ti, t(−i), w, α) ∝ n P (w|Ti, t(−i), w(−i), α) (tb ,ta ) P (Ti, t(i)|T , W , t(−i), w, α, β) = P (T |tb, t(−i), α)P (ta|T , t(−i), α) −i (i) (−i) P (Ti|W , −i, β)P (t |Ti, , w, α) terms Dirichlet distributions parameters analytically computed counts t(−i)where −i denotes type-level tag assignment ex cept Ti t(−i) denotes token-level tags except w (−i) (Johnson, 2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	t(i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	terms right-hand-side denote type-level token-level probability terms respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	type-level posterior term computed according to, P (Ti|W , −i, β) ∝ Note round sampling Ti variables takes time proportional size corpus, standard token-level HMM.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	crucial difference number parameters greatly reduced number variables sampled iteration.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	contrast results reported Johnson (2007), found per P (Ti|T −i, β) n (f,v)∈Wi P (v|Ti, f, W −i, −i, β) formance Gibbs sampler basic 1TW model stabilized quickly 10 full probabilities right-hand-side Dirichlet, distributions computed analytically given counts.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	token-level term similar standard HMM sampling equations found Johnson (2007).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	relevant variables set token-level tags appear instance ith word type; denote context pairs set {(tb, ta)} contained t(−i).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	use w erations sampling (see Figure 2 depiction).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	evaluate approach seven languages: English, Danish, Dutch, German, Portuguese, Spanish, Swedish.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	language investigate contribution component model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	languages make use tagging dictionary.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Mo del Hy per par . E n g li h1 1 m-1 n h1 1 m-1 u c h1 1 m-1 G er n1 1 m-1 Por tug ues e1 1 m-1 p ni h1 1 m-1 w e di h1 1 m-1 1T W st dia n 45.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 62.6 45.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 61.7 37.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 56.2 32.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 53.8 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 53.7 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 61.0 44.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 62.2 39.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 68.4 49.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 68.4 48.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 68.1 34.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 54.4 33.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	36.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 55.3 34.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 50.2 +P RI st dia n 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 65.5 46.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 64.7 42.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 58.3 40.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 57.3 51.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 65.9 48.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 60.7 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	41.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	7 68.3 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 70.7 52.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 70.9 42.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	37.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 55.8 38.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	36.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 57.3 +F EA TS st dia n 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	9 66.4 47.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 66.4 52.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 61.2 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 60.7 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 69.0 51.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 67.3 55.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 70.4 46.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	2 61.7 64.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 74.5 56.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 70.1 58.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 68.9 50.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	0 57.2 43.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	3 61.7 38.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 60.6 Table 3: Multilingual Results: report token-level one-to-one many-to-one accuracy variety languages several experimental settings (Section 5).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	language setting, report one-to-one (11) many- to-one (m-1) accuracies.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	cell, first row corresponds result using best hyperparameter choice, best defined 11 metric.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second row represents performance median hyperparameter setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Model components cascade, row corresponding +FEATS also includes PRIOR component (see Section 3).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	La ng ua ge # ke ns # W Ty pe # Ta gs E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics various corpora utilized experiments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	See Section 5.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	English data comes WSJ portion Penn Treebank languages training set CoNLL-X multilingual dependency parsing shared task.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5.1 Data Sets.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Following setup Johnson (2007), use whole Penn Treebank corpus training evaluation English.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	languages, use CoNLL-X multilingual dependency parsing shared task corpora (Buchholz Marsi, 2006) include gold POS tags (used evaluation).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	train test CoNLL-X training set.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Statistics data sets shown Table 2.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5.2 Setup.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Models assess marginal utility component model (see Section 3), incremen- tally increase sophistication.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, (+FEATS) utilizes tag prior well features (e.g., suffixes orthographic features), discussed Section 3, P (W |T , ψ) component.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Hyperparameters model two Dirichlet concentration hyperparameters: α shared hyperparameter token-level HMM emission transition distributions.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	β shared hyperparameter tag assignment prior word feature multinomials.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiment four values hyperparameter resulting 16 (α, β) combinations: α β 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations run, performed 30 iterations Gibbs sampling type assignment variables W .4 use final sample evaluation.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Evaluation Metrics report three metrics evaluate tagging performance.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	standard, report greedy one-to-one (Haghighi Klein, 2006) many-to-one token-level accuracy obtained mapping model states gold POS tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	also report word type level accuracy, fraction word types assigned majority tag (where mapping model state tag determined greedy one-to-one mapping discussed above).5 language, aggregate results following way: First, hyperparameter setting, evaluate three variants: first model (1TW) 4 Typically, performance stabilizes 10 itera-.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	encodes one tag per word constraint uni form type-level tag assignments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second model (+PRIOR) utilizes independent prior type-level tag assignments P (T |ψ).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	final model tions.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	5 choose two metrics Variation Information measure due deficiencies discussed Gao Johnson (2008).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	perform five runs different random initialization sampling state.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Hyperparameter settings sorted according median one-to-one metric runs.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	report results best median hyperparameter settings obtained way.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Specifically, settings report results median run setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Tag set standard, experiments, set number latent model tag states size annotated tag set.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	original tag set CoNLL-X Dutch data set consists compounded tags used tag multi-word units (MWUs) resulting tag set 300 tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	tokenize MWUs POS tags; reduces tag set size 12.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	See Table 2 tag set size languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	exception Dutch data set, processing performed annotated tags.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	6 Results Analysis.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	report token- type-level accuracy Table 3 6 languages system settings.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	analysis comparison focuses primarily one-to-one accuracy since stricter metric many-to-one accuracy, also report many-to-one completeness.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Comparison state-of-the-art taggers comparison consider two unsupervised tag- gers: HMM log-linear features Berg- Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) posterior regular- ization HMM Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	system Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) reports best unsupervised results English.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	consider two variants Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010)’s richest model: optimized via either EM LBFGS, relative performance depends language.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	model outperforms four five languages best hyperparameter setting three five median setting, yielding average absolute difference across languages 12.9% 3.9% best median settings respectively compared best EM LBFGS performance.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Berg-Kirkpatrick et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2010) consistently outperforms English, obtain substantial gains across languages.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	instance, Spanish, absolute gap median performance 10%.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Top 5 Bot 5 Go ld NN P NN JJ CD NN RB PD # ” , 1T W CD W RB NN VB N NN PR P$ W DT : MD . +P RI CD JJ NN WP $ NN RR B- , $ ” . +F EA TS JJ NN CD NN P UH , PR P$ # . “ Table 5: Type-level English POS Tag Ranking: list top 5 bottom 5 POS tags lexicon predictions models best hyperparameter setting.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	second point comparison Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009), also incorporate sparsity constraint, via altering model objective using posterior regularization.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	compare Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009) Portuguese (Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009) also report results English, reduced 17 tag set, comparable ours).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	best model yields 44.5% one-to-one accuracy, compared best median 56.5% result.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	However, full model takes advantage word features present Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Even without features, still using tag prior, median result 52.0%, still significantly outperforming Grac¸a et al.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	(2009).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Ablation Analysis evaluate impact incorporating various linguistic features model Table 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	novel element model ability capture type-level tag frequencies.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	experiment, compare model uniform tag assignment prior (1TW) learned prior (+PRIOR).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Across languages, +PRIOR consistently outperforms 1TW, reducing error average 9.1% 5.9% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Similar behavior observed adding features.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	difference featureless model (+PRIOR) full model (+FEATS) 13.6% 7.7% average error reduction best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Overall, difference basic model (1TW) full model (+FEATS) 21.2% 13.1% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	One striking example error reduction Spanish, reduces error 36.5% 24.7% best median settings respectively.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	observe similar trends using another measure – type-level accuracy (defined fraction words correctly assigned majority tag), according La ng ua ge etr ic B K 10 E B K 10 L B F G G 10 F EA B es F EA ed ia n E ng lis h 1 1 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 – – 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 h 1 1 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 – – 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 ut ch 1 1 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 – – 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 pa ni sh 1 1 1 – – 4 0 . 6 7 3 . 2 – – 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison method (FEATS) state-of-the-art methods.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): KM model uses variety orthographic features employs EM LBFGS optimization algorithm; Posterior regulariation model (Grac¸a et al., 2009): G10 model uses posterior regular- ization approach ensure tag sparsity constraint.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	La ng ua ge 1T W + P RI R + F E E ng lis h ni h u tc h G e r n P tu g u e e p ni h w e di h 2 1.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 1 0.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	1 2 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 1 2.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	8 1 8.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: cell report type- level accuracy computed frequent tag word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	state-to-tag mapping obtained best hyperparameter setting 11 mapping shown Table 3.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	full model yields 39.3% average error reduction across languages compared basic configuration (1TW).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	Table 5 provides insight behavior different models terms tagging lexicon generate.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	table shows lexicon tag frequency predicated full model closest gold standard.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	7 Conclusion Future Work.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	presented method unsupervised part- of-speech tagging considers word type allowed POS tags primary element model.	1
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	departure traditional token-based tagging approach allows us explicitly capture type- level distributional properties valid POS tag signments part model.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	resulting model compact, efficiently learnable linguistically expressive.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	empirical results demonstrate type-based tagger rivals state-of-the-art tag-level taggers employ sophisticated learning mechanisms exploit similar constraints.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	paper, make simplifying assumption one-tag-per-word.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	assumption, however, inherent type-based tagging models.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	promising direction future work explicitly model distribution tags word type.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	hypothesize modeling morphological information greatly constrain set possible tags, thereby refining representation tag lexicon.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	authors acknowledge support NSF (CAREER grant IIS0448168, grant IIS 0904684).	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	especially grateful Taylor Berg- Kirkpatrick running additional experiments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	thank members MIT NLP group suggestions comments.	0
"Second, learning categories cast unsupervised part-of-speech tagging task (recent work includes Ravi Knight (2009), Lee et al.</S><S sid =""8"" ssid = ""8"">(2010), Lamar et al."	opinions, findings, conclusions, recommendations expressed paper authors, necessarily reflect views funding organizations.	0
