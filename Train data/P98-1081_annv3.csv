Nine combination methods originally suggested Van Halteren et al.	Improving Data Driven Wordclass Tagging System Combination	0
Nine combination methods originally suggested Van Halteren et al.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
Nine combination methods originally suggested Van Halteren et al.	means experiment involving task morpho-syntactic wordclass tagging.	0
Nine combination methods originally suggested Van Halteren et al.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
Nine combination methods originally suggested Van Halteren et al.	comparison, outputs combined using several voting strategies second stage classifiers.	0
Nine combination methods originally suggested Van Halteren et al.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
Nine combination methods originally suggested Van Halteren et al.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
Nine combination methods originally suggested Van Halteren et al.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
Nine combination methods originally suggested Van Halteren et al.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
Nine combination methods originally suggested Van Halteren et al.	Data driven methods appear popular.	0
Nine combination methods originally suggested Van Halteren et al.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
Nine combination methods originally suggested Van Halteren et al.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
Nine combination methods originally suggested Van Halteren et al.	much easier quickly lead model produces results 'reasonably' good quality.	0
Nine combination methods originally suggested Van Halteren et al.	Obviously, 'reasonably good quality' ultimate goal.	0
Nine combination methods originally suggested Van Halteren et al.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
Nine combination methods originally suggested Van Halteren et al.	limiting factors power hard- software used implement learning method availability training material.	0
Nine combination methods originally suggested Van Halteren et al.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
Nine combination methods originally suggested Van Halteren et al.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
Nine combination methods originally suggested Van Halteren et al.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
Nine combination methods originally suggested Van Halteren et al.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
Nine combination methods originally suggested Van Halteren et al.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
Nine combination methods originally suggested Van Halteren et al.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
Nine combination methods originally suggested Van Halteren et al.	underlying assumption twofold.	0
Nine combination methods originally suggested Van Halteren et al.	First, combined votes make system robust quirks learner's particular bias.	0
Nine combination methods originally suggested Van Halteren et al.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
Nine combination methods originally suggested Van Halteren et al.	execute investigation means experiment.	0
Nine combination methods originally suggested Van Halteren et al.	NLP task used experiment morpho-syntactic wordclass tagging.	0
Nine combination methods originally suggested Van Halteren et al.	reasons choice several.	0
Nine combination methods originally suggested Van Halteren et al.	First all, tagging widely researched well-understood task (cf.	0
Nine combination methods originally suggested Van Halteren et al.	van Halteren (ed.)	0
Nine combination methods originally suggested Van Halteren et al.	1998).	0
Nine combination methods originally suggested Van Halteren et al.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
Nine combination methods originally suggested Van Halteren et al.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
Nine combination methods originally suggested Van Halteren et al.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
Nine combination methods originally suggested Van Halteren et al.	van Halteren 1996).	0
Nine combination methods originally suggested Van Halteren et al.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
Nine combination methods originally suggested Van Halteren et al.	varied systems available, variety hope lead better combination effects.	0
Nine combination methods originally suggested Van Halteren et al.	experiment selected four systems, primarily basis availability.	0
Nine combination methods originally suggested Van Halteren et al.	uses different features text tagged, completely different representation language model.	0
Nine combination methods originally suggested Van Halteren et al.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
Nine combination methods originally suggested Van Halteren et al.	Viterbi algorithm used determine probable tag sequence.	0
Nine combination methods originally suggested Van Halteren et al.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
Nine combination methods originally suggested Van Halteren et al.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
Nine combination methods originally suggested Van Halteren et al.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
Nine combination methods originally suggested Van Halteren et al.	cs.	0
Nine combination methods originally suggested Van Halteren et al.	j hu.	0
Nine combination methods originally suggested Van Halteren et al.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
Nine combination methods originally suggested Van Halteren et al.	I. 14.	0
Nine combination methods originally suggested Van Halteren et al.	tar.	0
Nine combination methods originally suggested Van Halteren et al.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
Nine combination methods originally suggested Van Halteren et al.	tagging rules applied sequence new text.	0
Nine combination methods originally suggested Van Halteren et al.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
Nine combination methods originally suggested Van Halteren et al.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
Nine combination methods originally suggested Van Halteren et al.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
Nine combination methods originally suggested Van Halteren et al.	training phase, cases containing information word, context correct tag stored memory.	0
Nine combination methods originally suggested Van Halteren et al.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
Nine combination methods originally suggested Van Halteren et al.	system used access information focus word two positions after, least known words.	0
Nine combination methods originally suggested Van Halteren et al.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
Nine combination methods originally suggested Van Halteren et al.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
Nine combination methods originally suggested Van Halteren et al.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
Nine combination methods originally suggested Van Halteren et al.	beam search used find highest probability tag sequence.	0
Nine combination methods originally suggested Van Halteren et al.	system Brill's system used default settings suggested documentation.	0
Nine combination methods originally suggested Van Halteren et al.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
Nine combination methods originally suggested Van Halteren et al.	data use experiment consists tagged LOB corpus (Johansson 1986).	0
Nine combination methods originally suggested Van Halteren et al.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
Nine combination methods originally suggested Van Halteren et al.	tagging, manually checked corrected, generally accepted quite accurate.	0
Nine combination methods originally suggested Van Halteren et al.	use slight adaptation tagset.	0
Nine combination methods originally suggested Van Halteren et al.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
Nine combination methods originally suggested Van Halteren et al.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
Nine combination methods originally suggested Van Halteren et al.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
Nine combination methods originally suggested Van Halteren et al.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
Nine combination methods originally suggested Van Halteren et al.	experiment, divide corpus three parts.	0
Nine combination methods originally suggested Van Halteren et al.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
Nine combination methods originally suggested Van Halteren et al.	taking first eight utterances every ten.	0
Nine combination methods originally suggested Van Halteren et al.	part used train individual tag- gers.	0
Nine combination methods originally suggested Van Halteren et al.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
Nine combination methods originally suggested Van Halteren et al.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
Nine combination methods originally suggested Van Halteren et al.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
Nine combination methods originally suggested Van Halteren et al.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
Nine combination methods originally suggested Van Halteren et al.	data Test never inspected detail used benchmark tagging quality measurement.	0
Nine combination methods originally suggested Van Halteren et al.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
Nine combination methods originally suggested Van Halteren et al.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
Nine combination methods originally suggested Van Halteren et al.	quality individual tuggers (cf.	0
Nine combination methods originally suggested Van Halteren et al.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
Nine combination methods originally suggested Van Halteren et al.	However, room improvement enough.	0
Nine combination methods originally suggested Van Halteren et al.	explained above, combination lead improvement, component taggers must differ errors make.	0
Nine combination methods originally suggested Van Halteren et al.	indeed case seen Table 1.	0
Nine combination methods originally suggested Van Halteren et al.	shows 99.22% Tune, least one tagger selects correct tag.	0
Nine combination methods originally suggested Van Halteren et al.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
Nine combination methods originally suggested Van Halteren et al.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
Nine combination methods originally suggested Van Halteren et al.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
Nine combination methods originally suggested Van Halteren et al.	patterns brackets give distribution correct/incorrect tags systems.	0
Nine combination methods originally suggested Van Halteren et al.	tag case.	0
Nine combination methods originally suggested Van Halteren et al.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
Nine combination methods originally suggested Van Halteren et al.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
Nine combination methods originally suggested Van Halteren et al.	following sections examine number them.	0
Nine combination methods originally suggested Van Halteren et al.	accuracy measurements listed Table 2.	0
Nine combination methods originally suggested Van Halteren et al.	5 straightforward selection method n-way vote.	0
Nine combination methods originally suggested Van Halteren et al.	tagger allowed vote tag choice tag highest number votes selected.	0
Nine combination methods originally suggested Van Halteren et al.	6 question large vote allow tagger.	0
Nine combination methods originally suggested Van Halteren et al.	democratic option give tagger one vote (Majority).	0
Nine combination methods originally suggested Van Halteren et al.	However, appears useful give weight taggers proved quality.	0
Nine combination methods originally suggested Van Halteren et al.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
Nine combination methods originally suggested Van Halteren et al.	information tagger's quality derived inspection results Tune.	0
Nine combination methods originally suggested Van Halteren et al.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
Nine combination methods originally suggested Van Halteren et al.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
Nine combination methods originally suggested Van Halteren et al.	6In experiment, random selection among winning tags made whenever tie.	0
Nine combination methods originally suggested Van Halteren et al.	Table 2: Accuracy individual taggers combination methods.	1
Nine combination methods originally suggested Van Halteren et al.	even information well taggers perform.	0
Nine combination methods originally suggested Van Halteren et al.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
Nine combination methods originally suggested Van Halteren et al.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
Nine combination methods originally suggested Van Halteren et al.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
Nine combination methods originally suggested Van Halteren et al.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
Nine combination methods originally suggested Van Halteren et al.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
Nine combination methods originally suggested Van Halteren et al.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
Nine combination methods originally suggested Van Halteren et al.	Pairwise Voting far, used information performance individual taggers.	0
Nine combination methods originally suggested Van Halteren et al.	next step examine pairs.	0
Nine combination methods originally suggested Van Halteren et al.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
Nine combination methods originally suggested Van Halteren et al.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
Nine combination methods originally suggested Van Halteren et al.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
Nine combination methods originally suggested Van Halteren et al.	probability tag Tx given tagger suggested tag Ti.	0
Nine combination methods originally suggested Van Halteren et al.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
Nine combination methods originally suggested Van Halteren et al.	principle, could remove restriction gain 22 1111 cases.	0
Nine combination methods originally suggested Van Halteren et al.	practice, chance beat majority slight indeed get hopes high happen often.	0
Nine combination methods originally suggested Van Halteren et al.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
Nine combination methods originally suggested Van Halteren et al.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
Nine combination methods originally suggested Van Halteren et al.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
Nine combination methods originally suggested Van Halteren et al.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
Nine combination methods originally suggested Van Halteren et al.	usually called stacking (Wolpert 1992).	0
Nine combination methods originally suggested Van Halteren et al.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
Nine combination methods originally suggested Van Halteren et al.	first choice use Memory- Based second level learner.	0
Nine combination methods originally suggested Van Halteren et al.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
Nine combination methods originally suggested Van Halteren et al.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
Nine combination methods originally suggested Van Halteren et al.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
Nine combination methods originally suggested Van Halteren et al.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
Nine combination methods originally suggested Van Halteren et al.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
Nine combination methods originally suggested Van Halteren et al.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
Nine combination methods originally suggested Van Halteren et al.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
Nine combination methods originally suggested Van Halteren et al.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
Nine combination methods originally suggested Van Halteren et al.	conjecture pruning beneficial interesting cases rare.	0
Nine combination methods originally suggested Van Halteren et al.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
Nine combination methods originally suggested Van Halteren et al.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
Nine combination methods originally suggested Van Halteren et al.	1°Tags+Word could handled C5.0 due huge number feature values.	0
Nine combination methods originally suggested Van Halteren et al.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
Nine combination methods originally suggested Van Halteren et al.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
Nine combination methods originally suggested Van Halteren et al.	important observation every combination (significantly) outperforms combination strict subset components.	0
Nine combination methods originally suggested Van Halteren et al.	Also note improvement yielded best combination.	0
Nine combination methods originally suggested Van Halteren et al.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
Nine combination methods originally suggested Van Halteren et al.	Maximum Entropy tagger (97.43%).	0
Nine combination methods originally suggested Van Halteren et al.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
Nine combination methods originally suggested Van Halteren et al.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
Nine combination methods originally suggested Van Halteren et al.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
Nine combination methods originally suggested Van Halteren et al.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
Nine combination methods originally suggested Van Halteren et al.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
Nine combination methods originally suggested Van Halteren et al.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
Nine combination methods originally suggested Van Halteren et al.	turns increase individual taggers quite limited compared combination.	0
Nine combination methods originally suggested Van Halteren et al.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
Nine combination methods originally suggested Van Halteren et al.	Conclusion.	0
Nine combination methods originally suggested Van Halteren et al.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
Nine combination methods originally suggested Van Halteren et al.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
Nine combination methods originally suggested Van Halteren et al.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
Nine combination methods originally suggested Van Halteren et al.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
Nine combination methods originally suggested Van Halteren et al.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
Nine combination methods originally suggested Van Halteren et al.	thanks go creators tagger generators used making systems available.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Improving Data Driven Wordclass Tagging System Combination	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	means experiment involving task morpho-syntactic wordclass tagging.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	comparison, outputs combined using several voting strategies second stage classifiers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Data driven methods appear popular.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	data driven method used, model automatically learned implicit structure annotated training corpus.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	much easier quickly lead model produces results 'reasonably' good quality.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Obviously, 'reasonably good quality' ultimate goal.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Unfortunately, quality reached given task limited, merely potential learning method used.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	limiting factors power hard- software used implement learning method availability training material.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	machine learning literature approach known ensemble, stacked, combined classifiers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	underlying assumption twofold.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	First, combined votes make system robust quirks learner's particular bias.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	execute investigation means experiment.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	NLP task used experiment morpho-syntactic wordclass tagging.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	reasons choice several.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	First all, tagging widely researched well-understood task (cf.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	van Halteren (ed.)	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	1998).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	van Halteren 1996).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	varied systems available, variety hope lead better combination effects.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	experiment selected four systems, primarily basis availability.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	uses different features text tagged, completely different representation language model.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Viterbi algorithm used determine probable tag sequence.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	cs.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	j hu.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	I. 14.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tar.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tagging rules applied sequence new text.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	training phase, cases containing information word, context correct tag stored memory.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	system used access information focus word two positions after, least known words.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	beam search used find highest probability tag sequence.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	system Brill's system used default settings suggested documentation.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	data use experiment consists tagged LOB corpus (Johansson 1986).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tagging, manually checked corrected, generally accepted quite accurate.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	use slight adaptation tagset.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	experiment, divide corpus three parts.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	taking first eight utterances every ten.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	part used train individual tag- gers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	data Test never inspected detail used benchmark tagging quality measurement.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	quality individual tuggers (cf.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, room improvement enough.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	explained above, combination lead improvement, component taggers must differ errors make.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	indeed case seen Table 1.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	shows 99.22% Tune, least one tagger selects correct tag.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	patterns brackets give distribution correct/incorrect tags systems.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tag case.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	following sections examine number them.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	accuracy measurements listed Table 2.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	5 straightforward selection method n-way vote.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tagger allowed vote tag choice tag highest number votes selected.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	6 question large vote allow tagger.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	democratic option give tagger one vote (Majority).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, appears useful give weight taggers proved quality.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	information tagger's quality derived inspection results Tune.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	6In experiment, random selection among winning tags made whenever tie.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Table 2: Accuracy individual taggers combination methods.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	even information well taggers perform.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Pairwise Voting far, used information performance individual taggers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	next step examine pairs.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	probability tag Tx given tagger suggested tag Ti.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	principle, could remove restriction gain 22 1111 cases.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	practice, chance beat majority slight indeed get hopes high happen often.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	usually called stacking (Wolpert 1992).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	second stage provided first level outputs, additional information, e.g. original input pattern.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	first choice use Memory- Based second level learner.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	basic version (Tags), case consists tags suggested component taggers correct tag.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	conjecture pruning beneficial interesting cases rare.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	1°Tags+Word could handled C5.0 due huge number feature values.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	important observation every combination (significantly) outperforms combination strict subset components.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Also note improvement yielded best combination.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Maximum Entropy tagger (97.43%).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	turns increase individual taggers quite limited compared combination.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Conclusion.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
.nally, TAGPAIR uses classi.cation pair weights based probability classi.cation predicted classi.cation pair (van Halteren et al., 1998).	thanks go creators tagger generators used making systems available.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Improving Data Driven Wordclass Tagging System Combination	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Data driven methods appear popular.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Obviously, 'reasonably good quality' ultimate goal.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	limiting factors power hard- software used implement learning method availability training material.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	underlying assumption twofold.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	First, combined votes make system robust quirks learner's particular bias.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	execute investigation means experiment.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	reasons choice several.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	First all, tagging widely researched well-understood task (cf.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	van Halteren (ed.)	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	1998).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	van Halteren 1996).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	varied systems available, variety hope lead better combination effects.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	experiment selected four systems, primarily basis availability.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	uses different features text tagged, completely different representation language model.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Viterbi algorithm used determine probable tag sequence.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	cs.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	j hu.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	I. 14.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tar.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tagging rules applied sequence new text.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	training phase, cases containing information word, context correct tag stored memory.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	system used access information focus word two positions after, least known words.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	beam search used find highest probability tag sequence.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	system Brill's system used default settings suggested documentation.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	use slight adaptation tagset.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	experiment, divide corpus three parts.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	taking first eight utterances every ten.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	part used train individual tag- gers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	quality individual tuggers (cf.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, room improvement enough.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	indeed case seen Table 1.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	patterns brackets give distribution correct/incorrect tags systems.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tag case.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	following sections examine number them.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	accuracy measurements listed Table 2.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	5 straightforward selection method n-way vote.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tagger allowed vote tag choice tag highest number votes selected.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	6 question large vote allow tagger.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	democratic option give tagger one vote (Majority).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, appears useful give weight taggers proved quality.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	information tagger's quality derived inspection results Tune.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	6In experiment, random selection among winning tags made whenever tie.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Table 2: Accuracy individual taggers combination methods.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	even information well taggers perform.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Pairwise Voting far, used information performance individual taggers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	next step examine pairs.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	probability tag Tx given tagger suggested tag Ti.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	principle, could remove restriction gain 22 1111 cases.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	usually called stacking (Wolpert 1992).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	first choice use Memory- Based second level learner.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	conjecture pruning beneficial interesting cases rare.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Also note improvement yielded best combination.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Maximum Entropy tagger (97.43%).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	turns increase individual taggers quite limited compared combination.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Conclusion.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Like Van Halteren et al.</S><S sid =""135"" ssid = ""121"">(1998), evaluated two features combinations."	thanks go creators tagger generators used making systems available.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Improving Data Driven Wordclass Tagging System Combination	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	means experiment involving task morpho-syntactic wordclass tagging.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	comparison, outputs combined using several voting strategies second stage classifiers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Data driven methods appear popular.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	data driven method used, model automatically learned implicit structure annotated training corpus.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	much easier quickly lead model produces results 'reasonably' good quality.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Obviously, 'reasonably good quality' ultimate goal.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Unfortunately, quality reached given task limited, merely potential learning method used.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	limiting factors power hard- software used implement learning method availability training material.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	machine learning literature approach known ensemble, stacked, combined classifiers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	underlying assumption twofold.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	First, combined votes make system robust quirks learner's particular bias.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	execute investigation means experiment.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	NLP task used experiment morpho-syntactic wordclass tagging.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	reasons choice several.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	First all, tagging widely researched well-understood task (cf.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	van Halteren (ed.)	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	1998).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	van Halteren 1996).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	varied systems available, variety hope lead better combination effects.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	experiment selected four systems, primarily basis availability.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	uses different features text tagged, completely different representation language model.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Viterbi algorithm used determine probable tag sequence.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	cs.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	j hu.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	I. 14.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tar.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tagging rules applied sequence new text.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	training phase, cases containing information word, context correct tag stored memory.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	system used access information focus word two positions after, least known words.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	beam search used find highest probability tag sequence.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	system Brill's system used default settings suggested documentation.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	data use experiment consists tagged LOB corpus (Johansson 1986).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tagging, manually checked corrected, generally accepted quite accurate.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	use slight adaptation tagset.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	experiment, divide corpus three parts.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	taking first eight utterances every ten.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	part used train individual tag- gers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	data Test never inspected detail used benchmark tagging quality measurement.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	quality individual tuggers (cf.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, room improvement enough.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	explained above, combination lead improvement, component taggers must differ errors make.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	indeed case seen Table 1.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	shows 99.22% Tune, least one tagger selects correct tag.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	patterns brackets give distribution correct/incorrect tags systems.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tag case.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	following sections examine number them.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	accuracy measurements listed Table 2.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	5 straightforward selection method n-way vote.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tagger allowed vote tag choice tag highest number votes selected.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	6 question large vote allow tagger.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	democratic option give tagger one vote (Majority).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, appears useful give weight taggers proved quality.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	information tagger's quality derived inspection results Tune.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	6In experiment, random selection among winning tags made whenever tie.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Table 2: Accuracy individual taggers combination methods.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	even information well taggers perform.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Pairwise Voting far, used information performance individual taggers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	next step examine pairs.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	probability tag Tx given tagger suggested tag Ti.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	principle, could remove restriction gain 22 1111 cases.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	practice, chance beat majority slight indeed get hopes high happen often.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	usually called stacking (Wolpert 1992).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	second stage provided first level outputs, additional information, e.g. original input pattern.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	first choice use Memory- Based second level learner.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	basic version (Tags), case consists tags suggested component taggers correct tag.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	conjecture pruning beneficial interesting cases rare.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	1°Tags+Word could handled C5.0 due huge number feature values.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	important observation every combination (significantly) outperforms combination strict subset components.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Also note improvement yielded best combination.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Maximum Entropy tagger (97.43%).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	turns increase individual taggers quite limited compared combination.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Conclusion.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
alternative method estimating parame­ters Akis approximate performance kth classi.er (a performance-based combiner) (van Halteren et al., 1998; Sang et al., 2000) Ak(xd)=P(Ck_is_correctlxd)(4) therefore giving weight classi.ers smaller classi.cation error (the method re­ferred PB).	thanks go creators tagger generators used making systems available.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Improving Data Driven Wordclass Tagging System Combination	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Data driven methods appear popular.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Obviously, 'reasonably good quality' ultimate goal.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	limiting factors power hard- software used implement learning method availability training material.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	underlying assumption twofold.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	First, combined votes make system robust quirks learner's particular bias.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	execute investigation means experiment.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	reasons choice several.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	First all, tagging widely researched well-understood task (cf.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	van Halteren (ed.)	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	1998).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	van Halteren 1996).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	varied systems available, variety hope lead better combination effects.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	experiment selected four systems, primarily basis availability.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	uses different features text tagged, completely different representation language model.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Viterbi algorithm used determine probable tag sequence.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	cs.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	j hu.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	I. 14.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tar.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tagging rules applied sequence new text.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	training phase, cases containing information word, context correct tag stored memory.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	system used access information focus word two positions after, least known words.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	beam search used find highest probability tag sequence.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	system Brill's system used default settings suggested documentation.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	use slight adaptation tagset.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	experiment, divide corpus three parts.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	taking first eight utterances every ten.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	part used train individual tag- gers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	quality individual tuggers (cf.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, room improvement enough.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	indeed case seen Table 1.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	patterns brackets give distribution correct/incorrect tags systems.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tag case.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	following sections examine number them.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	accuracy measurements listed Table 2.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	5 straightforward selection method n-way vote.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tagger allowed vote tag choice tag highest number votes selected.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	6 question large vote allow tagger.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	democratic option give tagger one vote (Majority).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, appears useful give weight taggers proved quality.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	information tagger's quality derived inspection results Tune.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	6In experiment, random selection among winning tags made whenever tie.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Table 2: Accuracy individual taggers combination methods.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	even information well taggers perform.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Pairwise Voting far, used information performance individual taggers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	next step examine pairs.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	probability tag Tx given tagger suggested tag Ti.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	principle, could remove restriction gain 22 1111 cases.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	usually called stacking (Wolpert 1992).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	first choice use Memory- Based second level learner.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	conjecture pruning beneficial interesting cases rare.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Also note improvement yielded best combination.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Maximum Entropy tagger (97.43%).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	turns increase individual taggers quite limited compared combination.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Conclusion.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Van Halteren et al.</S><S sid =""106"" ssid = ""49"">(1998) introduce modi.ed version voting called TagPair.</S><S sid =""107"" ssid = ""50"">Under model, conditional probability word sense given classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), com­puted development data, posterior prob­ability estimated N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..</S><S sid =""108"" ssid = ""51"">j sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).</S><S sid =""109"" ssid = ""52"">Each classi.er votes classi.cation every pair classi.ers votes sense likely given joint classi.cation.</S><S sid =""110"" ssid = ""53"">In experi­ments presented van Halteren et al.</S><S sid =""111"" ssid = ""54"">(1998), method best performer among presented methods."	thanks go creators tagger generators used making systems available.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Improving Data Driven Wordclass Tagging System Combination	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	means experiment involving task morpho-syntactic wordclass tagging.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	comparison, outputs combined using several voting strategies second stage classifiers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Data driven methods appear popular.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	data driven method used, model automatically learned implicit structure annotated training corpus.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	much easier quickly lead model produces results 'reasonably' good quality.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Obviously, 'reasonably good quality' ultimate goal.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Unfortunately, quality reached given task limited, merely potential learning method used.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	limiting factors power hard- software used implement learning method availability training material.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	machine learning literature approach known ensemble, stacked, combined classifiers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	underlying assumption twofold.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	First, combined votes make system robust quirks learner's particular bias.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	execute investigation means experiment.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	NLP task used experiment morpho-syntactic wordclass tagging.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	reasons choice several.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	First all, tagging widely researched well-understood task (cf.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	van Halteren (ed.)	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	1998).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	van Halteren 1996).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	varied systems available, variety hope lead better combination effects.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	experiment selected four systems, primarily basis availability.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	uses different features text tagged, completely different representation language model.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Viterbi algorithm used determine probable tag sequence.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	cs.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	j hu.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	I. 14.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tar.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tagging rules applied sequence new text.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	training phase, cases containing information word, context correct tag stored memory.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	system used access information focus word two positions after, least known words.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	beam search used find highest probability tag sequence.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	system Brill's system used default settings suggested documentation.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	data use experiment consists tagged LOB corpus (Johansson 1986).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tagging, manually checked corrected, generally accepted quite accurate.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	use slight adaptation tagset.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	experiment, divide corpus three parts.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	taking first eight utterances every ten.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	part used train individual tag- gers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	data Test never inspected detail used benchmark tagging quality measurement.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	quality individual tuggers (cf.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, room improvement enough.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	explained above, combination lead improvement, component taggers must differ errors make.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	indeed case seen Table 1.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	shows 99.22% Tune, least one tagger selects correct tag.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	patterns brackets give distribution correct/incorrect tags systems.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tag case.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	following sections examine number them.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	accuracy measurements listed Table 2.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	5 straightforward selection method n-way vote.	1
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tagger allowed vote tag choice tag highest number votes selected.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	6 question large vote allow tagger.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	democratic option give tagger one vote (Majority).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, appears useful give weight taggers proved quality.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	information tagger's quality derived inspection results Tune.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	6In experiment, random selection among winning tags made whenever tie.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Table 2: Accuracy individual taggers combination methods.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	even information well taggers perform.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Pairwise Voting far, used information performance individual taggers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	next step examine pairs.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	probability tag Tx given tagger suggested tag Ti.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	principle, could remove restriction gain 22 1111 cases.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	practice, chance beat majority slight indeed get hopes high happen often.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	usually called stacking (Wolpert 1992).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	second stage provided first level outputs, additional information, e.g. original input pattern.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	first choice use Memory- Based second level learner.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	basic version (Tags), case consists tags suggested component taggers correct tag.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	conjecture pruning beneficial interesting cases rare.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	1°Tags+Word could handled C5.0 due huge number feature values.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	important observation every combination (significantly) outperforms combination strict subset components.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Also note improvement yielded best combination.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Maximum Entropy tagger (97.43%).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	turns increase individual taggers quite limited compared combination.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Conclusion.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
common technique combination case vot­ing (Brill Wu, 1998; van Halteren et al., 1998; Sang et al., 2000).	thanks go creators tagger generators used making systems available.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Improving Data Driven Wordclass Tagging System Combination	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	means experiment involving task morpho-syntactic wordclass tagging.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Data driven methods appear popular.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	much easier quickly lead model produces results 'reasonably' good quality.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Obviously, 'reasonably good quality' ultimate goal.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	limiting factors power hard- software used implement learning method availability training material.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	underlying assumption twofold.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	First, combined votes make system robust quirks learner's particular bias.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	execute investigation means experiment.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	reasons choice several.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	First all, tagging widely researched well-understood task (cf.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	van Halteren (ed.)	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	1998).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	van Halteren 1996).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	varied systems available, variety hope lead better combination effects.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	experiment selected four systems, primarily basis availability.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	uses different features text tagged, completely different representation language model.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Viterbi algorithm used determine probable tag sequence.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	cs.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	j hu.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	I. 14.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tar.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tagging rules applied sequence new text.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	training phase, cases containing information word, context correct tag stored memory.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	system used access information focus word two positions after, least known words.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	beam search used find highest probability tag sequence.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	system Brill's system used default settings suggested documentation.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tagging, manually checked corrected, generally accepted quite accurate.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	use slight adaptation tagset.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	experiment, divide corpus three parts.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	taking first eight utterances every ten.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	part used train individual tag- gers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	data Test never inspected detail used benchmark tagging quality measurement.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	quality individual tuggers (cf.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, room improvement enough.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	explained above, combination lead improvement, component taggers must differ errors make.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	indeed case seen Table 1.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	shows 99.22% Tune, least one tagger selects correct tag.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	patterns brackets give distribution correct/incorrect tags systems.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tag case.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	following sections examine number them.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	accuracy measurements listed Table 2.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	5 straightforward selection method n-way vote.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tagger allowed vote tag choice tag highest number votes selected.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	6 question large vote allow tagger.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	democratic option give tagger one vote (Majority).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, appears useful give weight taggers proved quality.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	information tagger's quality derived inspection results Tune.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	6In experiment, random selection among winning tags made whenever tie.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Table 2: Accuracy individual taggers combination methods.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	even information well taggers perform.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Pairwise Voting far, used information performance individual taggers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	next step examine pairs.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	probability tag Tx given tagger suggested tag Ti.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	principle, could remove restriction gain 22 1111 cases.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	practice, chance beat majority slight indeed get hopes high happen often.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	usually called stacking (Wolpert 1992).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	first choice use Memory- Based second level learner.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	conjecture pruning beneficial interesting cases rare.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	important observation every combination (significantly) outperforms combination strict subset components.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Also note improvement yielded best combination.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Maximum Entropy tagger (97.43%).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	turns increase individual taggers quite limited compared combination.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Conclusion.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"consider three voting strategies suggested van Halteren et al.</S><S sid =""118"" ssid = ""74"">(1998): equal vote, classifier&apos;s vote weighted equally, overall accuracy, weight depends overall accuracy classifier, pair&apos;wise voting."	thanks go creators tagger generators used making systems available.	0
pairwise voting (van Halteren et al., 1998)	Improving Data Driven Wordclass Tagging System Combination	0
pairwise voting (van Halteren et al., 1998)	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
pairwise voting (van Halteren et al., 1998)	means experiment involving task morpho-syntactic wordclass tagging.	0
pairwise voting (van Halteren et al., 1998)	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
pairwise voting (van Halteren et al., 1998)	comparison, outputs combined using several voting strategies second stage classifiers.	0
pairwise voting (van Halteren et al., 1998)	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
pairwise voting (van Halteren et al., 1998)	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
pairwise voting (van Halteren et al., 1998)	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
pairwise voting (van Halteren et al., 1998)	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
pairwise voting (van Halteren et al., 1998)	Data driven methods appear popular.	0
pairwise voting (van Halteren et al., 1998)	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
pairwise voting (van Halteren et al., 1998)	data driven method used, model automatically learned implicit structure annotated training corpus.	0
pairwise voting (van Halteren et al., 1998)	much easier quickly lead model produces results 'reasonably' good quality.	0
pairwise voting (van Halteren et al., 1998)	Obviously, 'reasonably good quality' ultimate goal.	0
pairwise voting (van Halteren et al., 1998)	Unfortunately, quality reached given task limited, merely potential learning method used.	0
pairwise voting (van Halteren et al., 1998)	limiting factors power hard- software used implement learning method availability training material.	0
pairwise voting (van Halteren et al., 1998)	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
pairwise voting (van Halteren et al., 1998)	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
pairwise voting (van Halteren et al., 1998)	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
pairwise voting (van Halteren et al., 1998)	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
pairwise voting (van Halteren et al., 1998)	machine learning literature approach known ensemble, stacked, combined classifiers.	0
pairwise voting (van Halteren et al., 1998)	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
pairwise voting (van Halteren et al., 1998)	underlying assumption twofold.	0
pairwise voting (van Halteren et al., 1998)	First, combined votes make system robust quirks learner's particular bias.	0
pairwise voting (van Halteren et al., 1998)	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
pairwise voting (van Halteren et al., 1998)	execute investigation means experiment.	0
pairwise voting (van Halteren et al., 1998)	NLP task used experiment morpho-syntactic wordclass tagging.	0
pairwise voting (van Halteren et al., 1998)	reasons choice several.	0
pairwise voting (van Halteren et al., 1998)	First all, tagging widely researched well-understood task (cf.	0
pairwise voting (van Halteren et al., 1998)	van Halteren (ed.)	0
pairwise voting (van Halteren et al., 1998)	1998).	0
pairwise voting (van Halteren et al., 1998)	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
pairwise voting (van Halteren et al., 1998)	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
pairwise voting (van Halteren et al., 1998)	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
pairwise voting (van Halteren et al., 1998)	van Halteren 1996).	0
pairwise voting (van Halteren et al., 1998)	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
pairwise voting (van Halteren et al., 1998)	varied systems available, variety hope lead better combination effects.	0
pairwise voting (van Halteren et al., 1998)	experiment selected four systems, primarily basis availability.	0
pairwise voting (van Halteren et al., 1998)	uses different features text tagged, completely different representation language model.	0
pairwise voting (van Halteren et al., 1998)	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
pairwise voting (van Halteren et al., 1998)	Viterbi algorithm used determine probable tag sequence.	0
pairwise voting (van Halteren et al., 1998)	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
pairwise voting (van Halteren et al., 1998)	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
pairwise voting (van Halteren et al., 1998)	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
pairwise voting (van Halteren et al., 1998)	cs.	0
pairwise voting (van Halteren et al., 1998)	j hu.	0
pairwise voting (van Halteren et al., 1998)	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
pairwise voting (van Halteren et al., 1998)	I. 14.	0
pairwise voting (van Halteren et al., 1998)	tar.	0
pairwise voting (van Halteren et al., 1998)	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
pairwise voting (van Halteren et al., 1998)	tagging rules applied sequence new text.	0
pairwise voting (van Halteren et al., 1998)	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
pairwise voting (van Halteren et al., 1998)	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
pairwise voting (van Halteren et al., 1998)	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
pairwise voting (van Halteren et al., 1998)	training phase, cases containing information word, context correct tag stored memory.	0
pairwise voting (van Halteren et al., 1998)	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
pairwise voting (van Halteren et al., 1998)	system used access information focus word two positions after, least known words.	0
pairwise voting (van Halteren et al., 1998)	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
pairwise voting (van Halteren et al., 1998)	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
pairwise voting (van Halteren et al., 1998)	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
pairwise voting (van Halteren et al., 1998)	beam search used find highest probability tag sequence.	0
pairwise voting (van Halteren et al., 1998)	system Brill's system used default settings suggested documentation.	0
pairwise voting (van Halteren et al., 1998)	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
pairwise voting (van Halteren et al., 1998)	data use experiment consists tagged LOB corpus (Johansson 1986).	0
pairwise voting (van Halteren et al., 1998)	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
pairwise voting (van Halteren et al., 1998)	tagging, manually checked corrected, generally accepted quite accurate.	0
pairwise voting (van Halteren et al., 1998)	use slight adaptation tagset.	0
pairwise voting (van Halteren et al., 1998)	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
pairwise voting (van Halteren et al., 1998)	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
pairwise voting (van Halteren et al., 1998)	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
pairwise voting (van Halteren et al., 1998)	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
pairwise voting (van Halteren et al., 1998)	experiment, divide corpus three parts.	0
pairwise voting (van Halteren et al., 1998)	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
pairwise voting (van Halteren et al., 1998)	taking first eight utterances every ten.	0
pairwise voting (van Halteren et al., 1998)	part used train individual tag- gers.	0
pairwise voting (van Halteren et al., 1998)	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
pairwise voting (van Halteren et al., 1998)	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
pairwise voting (van Halteren et al., 1998)	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
pairwise voting (van Halteren et al., 1998)	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
pairwise voting (van Halteren et al., 1998)	data Test never inspected detail used benchmark tagging quality measurement.	0
pairwise voting (van Halteren et al., 1998)	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
pairwise voting (van Halteren et al., 1998)	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
pairwise voting (van Halteren et al., 1998)	quality individual tuggers (cf.	0
pairwise voting (van Halteren et al., 1998)	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
pairwise voting (van Halteren et al., 1998)	However, room improvement enough.	0
pairwise voting (van Halteren et al., 1998)	explained above, combination lead improvement, component taggers must differ errors make.	0
pairwise voting (van Halteren et al., 1998)	indeed case seen Table 1.	0
pairwise voting (van Halteren et al., 1998)	shows 99.22% Tune, least one tagger selects correct tag.	0
pairwise voting (van Halteren et al., 1998)	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
pairwise voting (van Halteren et al., 1998)	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
pairwise voting (van Halteren et al., 1998)	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
pairwise voting (van Halteren et al., 1998)	patterns brackets give distribution correct/incorrect tags systems.	0
pairwise voting (van Halteren et al., 1998)	tag case.	0
pairwise voting (van Halteren et al., 1998)	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
pairwise voting (van Halteren et al., 1998)	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
pairwise voting (van Halteren et al., 1998)	following sections examine number them.	0
pairwise voting (van Halteren et al., 1998)	accuracy measurements listed Table 2.	0
pairwise voting (van Halteren et al., 1998)	5 straightforward selection method n-way vote.	0
pairwise voting (van Halteren et al., 1998)	tagger allowed vote tag choice tag highest number votes selected.	0
pairwise voting (van Halteren et al., 1998)	6 question large vote allow tagger.	0
pairwise voting (van Halteren et al., 1998)	democratic option give tagger one vote (Majority).	0
pairwise voting (van Halteren et al., 1998)	However, appears useful give weight taggers proved quality.	0
pairwise voting (van Halteren et al., 1998)	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
pairwise voting (van Halteren et al., 1998)	information tagger's quality derived inspection results Tune.	0
pairwise voting (van Halteren et al., 1998)	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
pairwise voting (van Halteren et al., 1998)	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
pairwise voting (van Halteren et al., 1998)	6In experiment, random selection among winning tags made whenever tie.	0
pairwise voting (van Halteren et al., 1998)	Table 2: Accuracy individual taggers combination methods.	0
pairwise voting (van Halteren et al., 1998)	even information well taggers perform.	0
pairwise voting (van Halteren et al., 1998)	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
pairwise voting (van Halteren et al., 1998)	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
pairwise voting (van Halteren et al., 1998)	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
pairwise voting (van Halteren et al., 1998)	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
pairwise voting (van Halteren et al., 1998)	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
pairwise voting (van Halteren et al., 1998)	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
pairwise voting (van Halteren et al., 1998)	Pairwise Voting far, used information performance individual taggers.	0
pairwise voting (van Halteren et al., 1998)	next step examine pairs.	0
pairwise voting (van Halteren et al., 1998)	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
pairwise voting (van Halteren et al., 1998)	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
pairwise voting (van Halteren et al., 1998)	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
pairwise voting (van Halteren et al., 1998)	probability tag Tx given tagger suggested tag Ti.	0
pairwise voting (van Halteren et al., 1998)	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
pairwise voting (van Halteren et al., 1998)	principle, could remove restriction gain 22 1111 cases.	0
pairwise voting (van Halteren et al., 1998)	practice, chance beat majority slight indeed get hopes high happen often.	0
pairwise voting (van Halteren et al., 1998)	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
pairwise voting (van Halteren et al., 1998)	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
pairwise voting (van Halteren et al., 1998)	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
pairwise voting (van Halteren et al., 1998)	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
pairwise voting (van Halteren et al., 1998)	usually called stacking (Wolpert 1992).	0
pairwise voting (van Halteren et al., 1998)	second stage provided first level outputs, additional information, e.g. original input pattern.	0
pairwise voting (van Halteren et al., 1998)	first choice use Memory- Based second level learner.	0
pairwise voting (van Halteren et al., 1998)	basic version (Tags), case consists tags suggested component taggers correct tag.	0
pairwise voting (van Halteren et al., 1998)	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
pairwise voting (van Halteren et al., 1998)	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
pairwise voting (van Halteren et al., 1998)	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
pairwise voting (van Halteren et al., 1998)	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
pairwise voting (van Halteren et al., 1998)	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
pairwise voting (van Halteren et al., 1998)	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
pairwise voting (van Halteren et al., 1998)	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
pairwise voting (van Halteren et al., 1998)	conjecture pruning beneficial interesting cases rare.	0
pairwise voting (van Halteren et al., 1998)	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
pairwise voting (van Halteren et al., 1998)	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
pairwise voting (van Halteren et al., 1998)	1°Tags+Word could handled C5.0 due huge number feature values.	0
pairwise voting (van Halteren et al., 1998)	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
pairwise voting (van Halteren et al., 1998)	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
pairwise voting (van Halteren et al., 1998)	important observation every combination (significantly) outperforms combination strict subset components.	0
pairwise voting (van Halteren et al., 1998)	Also note improvement yielded best combination.	0
pairwise voting (van Halteren et al., 1998)	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
pairwise voting (van Halteren et al., 1998)	Maximum Entropy tagger (97.43%).	0
pairwise voting (van Halteren et al., 1998)	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
pairwise voting (van Halteren et al., 1998)	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
pairwise voting (van Halteren et al., 1998)	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
pairwise voting (van Halteren et al., 1998)	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
pairwise voting (van Halteren et al., 1998)	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
pairwise voting (van Halteren et al., 1998)	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
pairwise voting (van Halteren et al., 1998)	turns increase individual taggers quite limited compared combination.	0
pairwise voting (van Halteren et al., 1998)	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
pairwise voting (van Halteren et al., 1998)	Conclusion.	0
pairwise voting (van Halteren et al., 1998)	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
pairwise voting (van Halteren et al., 1998)	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
pairwise voting (van Halteren et al., 1998)	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
pairwise voting (van Halteren et al., 1998)	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
pairwise voting (van Halteren et al., 1998)	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
pairwise voting (van Halteren et al., 1998)	thanks go creators tagger generators used making systems available.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Improving Data Driven Wordclass Tagging System Combination	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	means experiment involving task morpho-syntactic wordclass tagging.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	comparison, outputs combined using several voting strategies second stage classifiers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Data driven methods appear popular.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	much easier quickly lead model produces results 'reasonably' good quality.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Obviously, 'reasonably good quality' ultimate goal.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	limiting factors power hard- software used implement learning method availability training material.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	underlying assumption twofold.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	First, combined votes make system robust quirks learner's particular bias.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	execute investigation means experiment.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	NLP task used experiment morpho-syntactic wordclass tagging.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	reasons choice several.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	First all, tagging widely researched well-understood task (cf.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	van Halteren (ed.)	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	1998).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	van Halteren 1996).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	varied systems available, variety hope lead better combination effects.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	experiment selected four systems, primarily basis availability.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	uses different features text tagged, completely different representation language model.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Viterbi algorithm used determine probable tag sequence.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	cs.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	j hu.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	I. 14.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tar.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tagging rules applied sequence new text.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	training phase, cases containing information word, context correct tag stored memory.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	system used access information focus word two positions after, least known words.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	beam search used find highest probability tag sequence.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	system Brill's system used default settings suggested documentation.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	data use experiment consists tagged LOB corpus (Johansson 1986).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tagging, manually checked corrected, generally accepted quite accurate.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	use slight adaptation tagset.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	experiment, divide corpus three parts.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	taking first eight utterances every ten.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	part used train individual tag- gers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	data Test never inspected detail used benchmark tagging quality measurement.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	quality individual tuggers (cf.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, room improvement enough.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	explained above, combination lead improvement, component taggers must differ errors make.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	indeed case seen Table 1.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	shows 99.22% Tune, least one tagger selects correct tag.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	patterns brackets give distribution correct/incorrect tags systems.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tag case.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	following sections examine number them.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	accuracy measurements listed Table 2.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	5 straightforward selection method n-way vote.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tagger allowed vote tag choice tag highest number votes selected.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	6 question large vote allow tagger.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	democratic option give tagger one vote (Majority).	1
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, appears useful give weight taggers proved quality.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	information tagger's quality derived inspection results Tune.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	6In experiment, random selection among winning tags made whenever tie.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Table 2: Accuracy individual taggers combination methods.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	even information well taggers perform.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Pairwise Voting far, used information performance individual taggers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	next step examine pairs.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	probability tag Tx given tagger suggested tag Ti.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	principle, could remove restriction gain 22 1111 cases.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	practice, chance beat majority slight indeed get hopes high happen often.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	usually called stacking (Wolpert 1992).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	first choice use Memory- Based second level learner.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	conjecture pruning beneficial interesting cases rare.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	1°Tags+Word could handled C5.0 due huge number feature values.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	important observation every combination (significantly) outperforms combination strict subset components.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Also note improvement yielded best combination.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Maximum Entropy tagger (97.43%).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	turns increase individual taggers quite limited compared combination.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Conclusion.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
Halteren et al (1998) compare number voting methods including Majority Vote scheme combination methods part speech tagging.	thanks go creators tagger generators used making systems available.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Improving Data Driven Wordclass Tagging System Combination	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Data driven methods appear popular.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Obviously, 'reasonably good quality' ultimate goal.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	limiting factors power hard- software used implement learning method availability training material.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	underlying assumption twofold.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	First, combined votes make system robust quirks learner's particular bias.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	execute investigation means experiment.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	reasons choice several.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	First all, tagging widely researched well-understood task (cf.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	van Halteren (ed.)	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	1998).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	van Halteren 1996).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	varied systems available, variety hope lead better combination effects.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	experiment selected four systems, primarily basis availability.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	uses different features text tagged, completely different representation language model.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Viterbi algorithm used determine probable tag sequence.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	cs.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	j hu.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	I. 14.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tar.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tagging rules applied sequence new text.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	training phase, cases containing information word, context correct tag stored memory.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	system used access information focus word two positions after, least known words.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	beam search used find highest probability tag sequence.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	system Brill's system used default settings suggested documentation.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	use slight adaptation tagset.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	experiment, divide corpus three parts.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	taking first eight utterances every ten.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	part used train individual tag- gers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	quality individual tuggers (cf.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, room improvement enough.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	indeed case seen Table 1.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	patterns brackets give distribution correct/incorrect tags systems.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tag case.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	following sections examine number them.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	accuracy measurements listed Table 2.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	5 straightforward selection method n-way vote.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tagger allowed vote tag choice tag highest number votes selected.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	6 question large vote allow tagger.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	democratic option give tagger one vote (Majority).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, appears useful give weight taggers proved quality.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	information tagger's quality derived inspection results Tune.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	6In experiment, random selection among winning tags made whenever tie.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Table 2: Accuracy individual taggers combination methods.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	even information well taggers perform.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Pairwise Voting far, used information performance individual taggers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	next step examine pairs.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	probability tag Tx given tagger suggested tag Ti.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	principle, could remove restriction gain 22 1111 cases.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	usually called stacking (Wolpert 1992).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	first choice use Memory- Based second level learner.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	conjecture pruning beneficial interesting cases rare.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Also note improvement yielded best combination.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Maximum Entropy tagger (97.43%).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	turns increase individual taggers quite limited compared combination.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Conclusion.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	1
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Thirdly, approach compatible in­ corporating multiple components type improve performance (cf.</S><S sid =""43"" ssid = ""18"">(van Halteren et al., 1998) found combining results several part speech taggers increased performance)."	thanks go creators tagger generators used making systems available.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Improving Data Driven Wordclass Tagging System Combination	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Data driven methods appear popular.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Obviously, 'reasonably good quality' ultimate goal.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	limiting factors power hard- software used implement learning method availability training material.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	underlying assumption twofold.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	First, combined votes make system robust quirks learner's particular bias.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	execute investigation means experiment.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	reasons choice several.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	First all, tagging widely researched well-understood task (cf.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	van Halteren (ed.)	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	1998).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	van Halteren 1996).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	varied systems available, variety hope lead better combination effects.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	experiment selected four systems, primarily basis availability.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	uses different features text tagged, completely different representation language model.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Viterbi algorithm used determine probable tag sequence.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	cs.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	j hu.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	I. 14.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tar.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tagging rules applied sequence new text.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	training phase, cases containing information word, context correct tag stored memory.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	system used access information focus word two positions after, least known words.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	beam search used find highest probability tag sequence.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	system Brill's system used default settings suggested documentation.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	use slight adaptation tagset.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	experiment, divide corpus three parts.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	taking first eight utterances every ten.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	part used train individual tag- gers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	quality individual tuggers (cf.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, room improvement enough.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	indeed case seen Table 1.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	patterns brackets give distribution correct/incorrect tags systems.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tag case.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	following sections examine number them.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	accuracy measurements listed Table 2.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	5 straightforward selection method n-way vote.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tagger allowed vote tag choice tag highest number votes selected.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	6 question large vote allow tagger.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	democratic option give tagger one vote (Majority).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, appears useful give weight taggers proved quality.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	information tagger's quality derived inspection results Tune.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	6In experiment, random selection among winning tags made whenever tie.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Table 2: Accuracy individual taggers combination methods.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	even information well taggers perform.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Pairwise Voting far, used information performance individual taggers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	next step examine pairs.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	probability tag Tx given tagger suggested tag Ti.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	principle, could remove restriction gain 22 1111 cases.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	usually called stacking (Wolpert 1992).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	first choice use Memory- Based second level learner.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	conjecture pruning beneficial interesting cases rare.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Also note improvement yielded best combination.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Maximum Entropy tagger (97.43%).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	turns increase individual taggers quite limited compared combination.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Conclusion.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Combination techniques successfully applied part speech tagging (van Halteren et al., 1998; Brill Wu, 1998; van Halteren et al., 2001).</S><S sid =""11"" ssid = ""11"">In cases investigators able achieve significant improvements previous best tagging results."	thanks go creators tagger generators used making systems available.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Improving Data Driven Wordclass Tagging System Combination	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Data driven methods appear popular.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Obviously, 'reasonably good quality' ultimate goal.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	limiting factors power hard- software used implement learning method availability training material.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	underlying assumption twofold.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	First, combined votes make system robust quirks learner's particular bias.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	execute investigation means experiment.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	reasons choice several.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	First all, tagging widely researched well-understood task (cf.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	van Halteren (ed.)	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	1998).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	van Halteren 1996).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	varied systems available, variety hope lead better combination effects.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	experiment selected four systems, primarily basis availability.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	uses different features text tagged, completely different representation language model.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Viterbi algorithm used determine probable tag sequence.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	cs.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	j hu.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	I. 14.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tar.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tagging rules applied sequence new text.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	training phase, cases containing information word, context correct tag stored memory.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	system used access information focus word two positions after, least known words.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	beam search used find highest probability tag sequence.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	system Brill's system used default settings suggested documentation.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	use slight adaptation tagset.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	experiment, divide corpus three parts.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	taking first eight utterances every ten.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	part used train individual tag- gers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	quality individual tuggers (cf.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, room improvement enough.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	indeed case seen Table 1.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	patterns brackets give distribution correct/incorrect tags systems.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tag case.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	following sections examine number them.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	accuracy measurements listed Table 2.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	5 straightforward selection method n-way vote.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tagger allowed vote tag choice tag highest number votes selected.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	6 question large vote allow tagger.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	democratic option give tagger one vote (Majority).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, appears useful give weight taggers proved quality.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	information tagger's quality derived inspection results Tune.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	6In experiment, random selection among winning tags made whenever tie.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Table 2: Accuracy individual taggers combination methods.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	even information well taggers perform.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Pairwise Voting far, used information performance individual taggers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	next step examine pairs.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	probability tag Tx given tagger suggested tag Ti.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	principle, could remove restriction gain 22 1111 cases.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	usually called stacking (Wolpert 1992).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	first choice use Memory- Based second level learner.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	conjecture pruning beneficial interesting cases rare.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Also note improvement yielded best combination.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Maximum Entropy tagger (97.43%).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	turns increase individual taggers quite limited compared combination.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Conclusion.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Van Halteren et al.</S><S sid =""82"" ssid = ""20"">(1998) generalized approach higher number classifiers TotPrecision voting method.</S><S sid =""83"" ssid = ""21"">The vote classifier (parser) weighted respective accuracy."	thanks go creators tagger generators used making systems available.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Improving Data Driven Wordclass Tagging System Combination	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	means experiment involving task morpho-syntactic wordclass tagging.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	comparison, outputs combined using several voting strategies second stage classifiers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Data driven methods appear popular.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	much easier quickly lead model produces results 'reasonably' good quality.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Obviously, 'reasonably good quality' ultimate goal.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	limiting factors power hard- software used implement learning method availability training material.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	underlying assumption twofold.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	First, combined votes make system robust quirks learner's particular bias.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	execute investigation means experiment.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	NLP task used experiment morpho-syntactic wordclass tagging.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	reasons choice several.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	First all, tagging widely researched well-understood task (cf.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	van Halteren (ed.)	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	1998).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	van Halteren 1996).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	varied systems available, variety hope lead better combination effects.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	experiment selected four systems, primarily basis availability.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	uses different features text tagged, completely different representation language model.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Viterbi algorithm used determine probable tag sequence.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	cs.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	j hu.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	I. 14.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tar.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tagging rules applied sequence new text.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	training phase, cases containing information word, context correct tag stored memory.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	system used access information focus word two positions after, least known words.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	beam search used find highest probability tag sequence.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	system Brill's system used default settings suggested documentation.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	data use experiment consists tagged LOB corpus (Johansson 1986).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tagging, manually checked corrected, generally accepted quite accurate.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	use slight adaptation tagset.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	experiment, divide corpus three parts.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	taking first eight utterances every ten.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	part used train individual tag- gers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	data Test never inspected detail used benchmark tagging quality measurement.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	quality individual tuggers (cf.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, room improvement enough.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	explained above, combination lead improvement, component taggers must differ errors make.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	indeed case seen Table 1.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	shows 99.22% Tune, least one tagger selects correct tag.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	patterns brackets give distribution correct/incorrect tags systems.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tag case.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	following sections examine number them.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	accuracy measurements listed Table 2.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	5 straightforward selection method n-way vote.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tagger allowed vote tag choice tag highest number votes selected.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	6 question large vote allow tagger.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	democratic option give tagger one vote (Majority).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, appears useful give weight taggers proved quality.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	information tagger's quality derived inspection results Tune.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	6In experiment, random selection among winning tags made whenever tie.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Table 2: Accuracy individual taggers combination methods.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	even information well taggers perform.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Pairwise Voting far, used information performance individual taggers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	next step examine pairs.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	probability tag Tx given tagger suggested tag Ti.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	principle, could remove restriction gain 22 1111 cases.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	practice, chance beat majority slight indeed get hopes high happen often.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	usually called stacking (Wolpert 1992).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	first choice use Memory- Based second level learner.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	conjecture pruning beneficial interesting cases rare.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	1°Tags+Word could handled C5.0 due huge number feature values.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	important observation every combination (significantly) outperforms combination strict subset components.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Also note improvement yielded best combination.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Maximum Entropy tagger (97.43%).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	turns increase individual taggers quite limited compared combination.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Conclusion.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
Parallel (van Halteren et al., 1998), ran experiments two stacked classifiers, Memory-Based, Decision-Tree-Based.	thanks go creators tagger generators used making systems available.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Improving Data Driven Wordclass Tagging System Combination	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	means experiment involving task morpho-syntactic wordclass tagging.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	comparison, outputs combined using several voting strategies second stage classifiers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Data driven methods appear popular.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	much easier quickly lead model produces results 'reasonably' good quality.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Obviously, 'reasonably good quality' ultimate goal.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	limiting factors power hard- software used implement learning method availability training material.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	underlying assumption twofold.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	First, combined votes make system robust quirks learner's particular bias.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	execute investigation means experiment.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	NLP task used experiment morpho-syntactic wordclass tagging.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	reasons choice several.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	First all, tagging widely researched well-understood task (cf.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	van Halteren (ed.)	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	1998).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	van Halteren 1996).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	varied systems available, variety hope lead better combination effects.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	experiment selected four systems, primarily basis availability.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	uses different features text tagged, completely different representation language model.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Viterbi algorithm used determine probable tag sequence.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	cs.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	j hu.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	I. 14.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tar.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tagging rules applied sequence new text.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	training phase, cases containing information word, context correct tag stored memory.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	system used access information focus word two positions after, least known words.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	beam search used find highest probability tag sequence.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	system Brill's system used default settings suggested documentation.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	data use experiment consists tagged LOB corpus (Johansson 1986).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tagging, manually checked corrected, generally accepted quite accurate.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	use slight adaptation tagset.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	experiment, divide corpus three parts.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	taking first eight utterances every ten.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	part used train individual tag- gers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	data Test never inspected detail used benchmark tagging quality measurement.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	quality individual tuggers (cf.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, room improvement enough.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	explained above, combination lead improvement, component taggers must differ errors make.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	indeed case seen Table 1.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	shows 99.22% Tune, least one tagger selects correct tag.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	patterns brackets give distribution correct/incorrect tags systems.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tag case.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	following sections examine number them.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	accuracy measurements listed Table 2.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	5 straightforward selection method n-way vote.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tagger allowed vote tag choice tag highest number votes selected.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	6 question large vote allow tagger.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	democratic option give tagger one vote (Majority).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, appears useful give weight taggers proved quality.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	information tagger's quality derived inspection results Tune.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	6In experiment, random selection among winning tags made whenever tie.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Table 2: Accuracy individual taggers combination methods.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	even information well taggers perform.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Pairwise Voting far, used information performance individual taggers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	next step examine pairs.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	probability tag Tx given tagger suggested tag Ti.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	principle, could remove restriction gain 22 1111 cases.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	practice, chance beat majority slight indeed get hopes high happen often.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	usually called stacking (Wolpert 1992).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	first choice use Memory- Based second level learner.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	conjecture pruning beneficial interesting cases rare.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	1°Tags+Word could handled C5.0 due huge number feature values.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	important observation every combination (significantly) outperforms combination strict subset components.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Also note improvement yielded best combination.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Maximum Entropy tagger (97.43%).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	turns increase individual taggers quite limited compared combination.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Conclusion.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
experiments, TotPrecision voting scheme (van Halteren et al., 1998) used.	thanks go creators tagger generators used making systems available.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Improving Data Driven Wordclass Tagging System Combination	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	means experiment involving task morpho-syntactic wordclass tagging.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	comparison, outputs combined using several voting strategies second stage classifiers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Data driven methods appear popular.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	data driven method used, model automatically learned implicit structure annotated training corpus.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	much easier quickly lead model produces results 'reasonably' good quality.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Obviously, 'reasonably good quality' ultimate goal.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Unfortunately, quality reached given task limited, merely potential learning method used.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	limiting factors power hard- software used implement learning method availability training material.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	machine learning literature approach known ensemble, stacked, combined classifiers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	underlying assumption twofold.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	First, combined votes make system robust quirks learner's particular bias.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	execute investigation means experiment.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	NLP task used experiment morpho-syntactic wordclass tagging.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	reasons choice several.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	First all, tagging widely researched well-understood task (cf.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	van Halteren (ed.)	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	1998).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	van Halteren 1996).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	varied systems available, variety hope lead better combination effects.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	experiment selected four systems, primarily basis availability.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	uses different features text tagged, completely different representation language model.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Viterbi algorithm used determine probable tag sequence.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	cs.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	j hu.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	I. 14.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tar.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tagging rules applied sequence new text.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	training phase, cases containing information word, context correct tag stored memory.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	system used access information focus word two positions after, least known words.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	beam search used find highest probability tag sequence.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	system Brill's system used default settings suggested documentation.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	data use experiment consists tagged LOB corpus (Johansson 1986).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tagging, manually checked corrected, generally accepted quite accurate.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	use slight adaptation tagset.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	experiment, divide corpus three parts.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	taking first eight utterances every ten.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	part used train individual tag- gers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	data Test never inspected detail used benchmark tagging quality measurement.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	quality individual tuggers (cf.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, room improvement enough.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	explained above, combination lead improvement, component taggers must differ errors make.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	indeed case seen Table 1.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	shows 99.22% Tune, least one tagger selects correct tag.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	patterns brackets give distribution correct/incorrect tags systems.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tag case.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	following sections examine number them.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	accuracy measurements listed Table 2.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	5 straightforward selection method n-way vote.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tagger allowed vote tag choice tag highest number votes selected.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	6 question large vote allow tagger.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	democratic option give tagger one vote (Majority).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, appears useful give weight taggers proved quality.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	information tagger's quality derived inspection results Tune.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	6In experiment, random selection among winning tags made whenever tie.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Table 2: Accuracy individual taggers combination methods.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	even information well taggers perform.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Pairwise Voting far, used information performance individual taggers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	next step examine pairs.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	probability tag Tx given tagger suggested tag Ti.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	principle, could remove restriction gain 22 1111 cases.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	practice, chance beat majority slight indeed get hopes high happen often.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	usually called stacking (Wolpert 1992).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	second stage provided first level outputs, additional information, e.g. original input pattern.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	first choice use Memory- Based second level learner.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	basic version (Tags), case consists tags suggested component taggers correct tag.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	conjecture pruning beneficial interesting cases rare.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	1°Tags+Word could handled C5.0 due huge number feature values.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	important observation every combination (significantly) outperforms combination strict subset components.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Also note improvement yielded best combination.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Maximum Entropy tagger (97.43%).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	turns increase individual taggers quite limited compared combination.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Conclusion.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
advanced voting method ex­ amines output values pairs classifiers assigns weights tags based often appear pair tuning data (Tag­ Pair, Van Halteren et al., (1998)).	thanks go creators tagger generators used making systems available.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Improving Data Driven Wordclass Tagging System Combination	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	means experiment involving task morpho-syntactic wordclass tagging.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	comparison, outputs combined using several voting strategies second stage classifiers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Data driven methods appear popular.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	data driven method used, model automatically learned implicit structure annotated training corpus.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	much easier quickly lead model produces results 'reasonably' good quality.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Obviously, 'reasonably good quality' ultimate goal.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Unfortunately, quality reached given task limited, merely potential learning method used.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	limiting factors power hard- software used implement learning method availability training material.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	machine learning literature approach known ensemble, stacked, combined classifiers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	underlying assumption twofold.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	First, combined votes make system robust quirks learner's particular bias.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	execute investigation means experiment.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	NLP task used experiment morpho-syntactic wordclass tagging.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	reasons choice several.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	First all, tagging widely researched well-understood task (cf.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	van Halteren (ed.)	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	1998).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	van Halteren 1996).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	varied systems available, variety hope lead better combination effects.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	experiment selected four systems, primarily basis availability.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	uses different features text tagged, completely different representation language model.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Viterbi algorithm used determine probable tag sequence.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	cs.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	j hu.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	I. 14.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tar.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tagging rules applied sequence new text.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	training phase, cases containing information word, context correct tag stored memory.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	system used access information focus word two positions after, least known words.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	beam search used find highest probability tag sequence.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	system Brill's system used default settings suggested documentation.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	data use experiment consists tagged LOB corpus (Johansson 1986).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tagging, manually checked corrected, generally accepted quite accurate.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	use slight adaptation tagset.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	experiment, divide corpus three parts.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	taking first eight utterances every ten.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	part used train individual tag- gers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	data Test never inspected detail used benchmark tagging quality measurement.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	quality individual tuggers (cf.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, room improvement enough.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	explained above, combination lead improvement, component taggers must differ errors make.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	indeed case seen Table 1.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	shows 99.22% Tune, least one tagger selects correct tag.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	patterns brackets give distribution correct/incorrect tags systems.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tag case.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	following sections examine number them.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	accuracy measurements listed Table 2.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	5 straightforward selection method n-way vote.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tagger allowed vote tag choice tag highest number votes selected.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	6 question large vote allow tagger.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	democratic option give tagger one vote (Majority).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, appears useful give weight taggers proved quality.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	information tagger's quality derived inspection results Tune.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	6In experiment, random selection among winning tags made whenever tie.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Table 2: Accuracy individual taggers combination methods.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	even information well taggers perform.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Pairwise Voting far, used information performance individual taggers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	next step examine pairs.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	probability tag Tx given tagger suggested tag Ti.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	principle, could remove restriction gain 22 1111 cases.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	practice, chance beat majority slight indeed get hopes high happen often.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	usually called stacking (Wolpert 1992).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	second stage provided first level outputs, additional information, e.g. original input pattern.	1
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	first choice use Memory- Based second level learner.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	basic version (Tags), case consists tags suggested component taggers correct tag.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	conjecture pruning beneficial interesting cases rare.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	1°Tags+Word could handled C5.0 due huge number feature values.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	important observation every combination (significantly) outperforms combination strict subset components.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Also note improvement yielded best combination.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Maximum Entropy tagger (97.43%).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	turns increase individual taggers quite limited compared combination.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Conclusion.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
purpose used part-of-speech tag cur­ rent word compressed representation first stage input (Van Halteren et al., 1998).	thanks go creators tagger generators used making systems available.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Improving Data Driven Wordclass Tagging System Combination	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	means experiment involving task morpho-syntactic wordclass tagging.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Data driven methods appear popular.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	much easier quickly lead model produces results 'reasonably' good quality.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Obviously, 'reasonably good quality' ultimate goal.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	limiting factors power hard- software used implement learning method availability training material.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	underlying assumption twofold.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	First, combined votes make system robust quirks learner's particular bias.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	execute investigation means experiment.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	reasons choice several.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	First all, tagging widely researched well-understood task (cf.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	van Halteren (ed.)	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	1998).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	van Halteren 1996).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	varied systems available, variety hope lead better combination effects.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	experiment selected four systems, primarily basis availability.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	uses different features text tagged, completely different representation language model.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Viterbi algorithm used determine probable tag sequence.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	cs.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	j hu.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	I. 14.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tar.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tagging rules applied sequence new text.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	training phase, cases containing information word, context correct tag stored memory.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	system used access information focus word two positions after, least known words.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	beam search used find highest probability tag sequence.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	system Brill's system used default settings suggested documentation.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tagging, manually checked corrected, generally accepted quite accurate.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	use slight adaptation tagset.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	experiment, divide corpus three parts.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	taking first eight utterances every ten.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	part used train individual tag- gers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	data Test never inspected detail used benchmark tagging quality measurement.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	quality individual tuggers (cf.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, room improvement enough.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	explained above, combination lead improvement, component taggers must differ errors make.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	indeed case seen Table 1.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	shows 99.22% Tune, least one tagger selects correct tag.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	patterns brackets give distribution correct/incorrect tags systems.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tag case.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	following sections examine number them.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	accuracy measurements listed Table 2.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	5 straightforward selection method n-way vote.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tagger allowed vote tag choice tag highest number votes selected.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	6 question large vote allow tagger.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	democratic option give tagger one vote (Majority).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, appears useful give weight taggers proved quality.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	information tagger's quality derived inspection results Tune.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	6In experiment, random selection among winning tags made whenever tie.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Table 2: Accuracy individual taggers combination methods.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	even information well taggers perform.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Pairwise Voting far, used information performance individual taggers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	next step examine pairs.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	probability tag Tx given tagger suggested tag Ti.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	principle, could remove restriction gain 22 1111 cases.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	practice, chance beat majority slight indeed get hopes high happen often.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	usually called stacking (Wolpert 1992).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	first choice use Memory- Based second level learner.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	conjecture pruning beneficial interesting cases rare.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	important observation every combination (significantly) outperforms combination strict subset components.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Also note improvement yielded best combination.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Maximum Entropy tagger (97.43%).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	turns increase individual taggers quite limited compared combination.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Conclusion.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"First experiments (van Halteren, Zavrel, Daelemans 1998; Brill Wu 1998) demonstrated basic validity approach tagging, error rate best combiner 19.1% lower best individual tagger (van Halteren, Zavrel, Daelemans 1998).</S><S sid =""41"" ssid = ""41"">However, experiments restricted single language, single tagset and, importantly, limited amount training data combiners.</S><S sid =""42"" ssid = ""42"">This led us perform further, extensive, 1In previous work (van Halteren, Zavrel, Daelemans 1998), unable confirm latter half hypothesis unequivocally."	thanks go creators tagger generators used making systems available.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Improving Data Driven Wordclass Tagging System Combination	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	means experiment involving task morpho-syntactic wordclass tagging.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Data driven methods appear popular.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	much easier quickly lead model produces results 'reasonably' good quality.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Obviously, 'reasonably good quality' ultimate goal.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	limiting factors power hard- software used implement learning method availability training material.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	underlying assumption twofold.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	First, combined votes make system robust quirks learner's particular bias.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	execute investigation means experiment.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	reasons choice several.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	First all, tagging widely researched well-understood task (cf.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	van Halteren (ed.)	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	1998).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	van Halteren 1996).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	varied systems available, variety hope lead better combination effects.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	experiment selected four systems, primarily basis availability.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	uses different features text tagged, completely different representation language model.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Viterbi algorithm used determine probable tag sequence.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	cs.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	j hu.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	I. 14.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tar.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tagging rules applied sequence new text.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	training phase, cases containing information word, context correct tag stored memory.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	system used access information focus word two positions after, least known words.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	beam search used find highest probability tag sequence.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	system Brill's system used default settings suggested documentation.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tagging, manually checked corrected, generally accepted quite accurate.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	use slight adaptation tagset.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	experiment, divide corpus three parts.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	taking first eight utterances every ten.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	part used train individual tag- gers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	data Test never inspected detail used benchmark tagging quality measurement.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	quality individual tuggers (cf.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, room improvement enough.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	explained above, combination lead improvement, component taggers must differ errors make.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	indeed case seen Table 1.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	shows 99.22% Tune, least one tagger selects correct tag.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	patterns brackets give distribution correct/incorrect tags systems.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tag case.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	following sections examine number them.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	accuracy measurements listed Table 2.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	5 straightforward selection method n-way vote.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tagger allowed vote tag choice tag highest number votes selected.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	6 question large vote allow tagger.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	democratic option give tagger one vote (Majority).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, appears useful give weight taggers proved quality.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	information tagger's quality derived inspection results Tune.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	6In experiment, random selection among winning tags made whenever tie.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Table 2: Accuracy individual taggers combination methods.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	even information well taggers perform.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Pairwise Voting far, used information performance individual taggers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	next step examine pairs.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	probability tag Tx given tagger suggested tag Ti.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	principle, could remove restriction gain 22 1111 cases.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	practice, chance beat majority slight indeed get hopes high happen often.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	usually called stacking (Wolpert 1992).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	first choice use Memory- Based second level learner.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	conjecture pruning beneficial interesting cases rare.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	important observation every combination (significantly) outperforms combination strict subset components.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Also note improvement yielded best combination.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Maximum Entropy tagger (97.43%).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	turns increase individual taggers quite limited compared combination.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Conclusion.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"Compare &quot;tune&quot; set van Halteren, Zavrel, Daelemans (1998).</S><S sid =""184"" ssid = ""34"">This consisted 114K.</S><S sid =""185"" ssid = ""35"">tokens, but, 92.5% agreement four taggers, yielded less 9K tokens useful training material resolve disagreements.</S><S sid =""186"" ssid = ""36"">This suspected main reason relative lack performance sophisticated combiners."	thanks go creators tagger generators used making systems available.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Improving Data Driven Wordclass Tagging System Combination	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	means experiment involving task morpho-syntactic wordclass tagging.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Data driven methods appear popular.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	much easier quickly lead model produces results 'reasonably' good quality.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Obviously, 'reasonably good quality' ultimate goal.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	limiting factors power hard- software used implement learning method availability training material.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	underlying assumption twofold.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	First, combined votes make system robust quirks learner's particular bias.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	execute investigation means experiment.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	reasons choice several.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	First all, tagging widely researched well-understood task (cf.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	van Halteren (ed.)	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	1998).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	van Halteren 1996).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	varied systems available, variety hope lead better combination effects.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	experiment selected four systems, primarily basis availability.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	uses different features text tagged, completely different representation language model.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Viterbi algorithm used determine probable tag sequence.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	cs.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	j hu.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	I. 14.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tar.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tagging rules applied sequence new text.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	training phase, cases containing information word, context correct tag stored memory.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	system used access information focus word two positions after, least known words.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	beam search used find highest probability tag sequence.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	system Brill's system used default settings suggested documentation.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tagging, manually checked corrected, generally accepted quite accurate.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	use slight adaptation tagset.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	experiment, divide corpus three parts.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	taking first eight utterances every ten.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	part used train individual tag- gers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	data Test never inspected detail used benchmark tagging quality measurement.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	quality individual tuggers (cf.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, room improvement enough.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	explained above, combination lead improvement, component taggers must differ errors make.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	indeed case seen Table 1.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	shows 99.22% Tune, least one tagger selects correct tag.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	patterns brackets give distribution correct/incorrect tags systems.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tag case.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	following sections examine number them.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	accuracy measurements listed Table 2.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	5 straightforward selection method n-way vote.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tagger allowed vote tag choice tag highest number votes selected.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	6 question large vote allow tagger.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	democratic option give tagger one vote (Majority).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, appears useful give weight taggers proved quality.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	information tagger's quality derived inspection results Tune.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	6In experiment, random selection among winning tags made whenever tie.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Table 2: Accuracy individual taggers combination methods.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	even information well taggers perform.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Pairwise Voting far, used information performance individual taggers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	next step examine pairs.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	probability tag Tx given tagger suggested tag Ti.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	principle, could remove restriction gain 22 1111 cases.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	practice, chance beat majority slight indeed get hopes high happen often.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	usually called stacking (Wolpert 1992).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	first choice use Memory- Based second level learner.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	conjecture pruning beneficial interesting cases rare.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	important observation every combination (significantly) outperforms combination strict subset components.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Also note improvement yielded best combination.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Maximum Entropy tagger (97.43%).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	turns increase individual taggers quite limited compared combination.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Conclusion.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"part-of-speech tagging, significant increase accuracy combining output different taggers first demonstrated van Halteren, Zavrel, Daelemans (1998) Brill Wu (1998).</S><S sid =""571"" ssid = ""8"">In approaches, different tagger gen­ erators applied training data predictions combined using different combination methods, including stacking.</S><S sid =""573"" ssid = ""10"">As apply methods van Halteren, Zavrel, Daelemans (1998) WSJ well, easier make comparison."	thanks go creators tagger generators used making systems available.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Improving Data Driven Wordclass Tagging System Combination	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	means experiment involving task morpho-syntactic wordclass tagging.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Data driven methods appear popular.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	much easier quickly lead model produces results 'reasonably' good quality.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Obviously, 'reasonably good quality' ultimate goal.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	limiting factors power hard- software used implement learning method availability training material.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	underlying assumption twofold.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	First, combined votes make system robust quirks learner's particular bias.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	execute investigation means experiment.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	reasons choice several.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	First all, tagging widely researched well-understood task (cf.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	van Halteren (ed.)	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	1998).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	van Halteren 1996).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	varied systems available, variety hope lead better combination effects.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	experiment selected four systems, primarily basis availability.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	uses different features text tagged, completely different representation language model.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Viterbi algorithm used determine probable tag sequence.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	cs.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	j hu.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	I. 14.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tar.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tagging rules applied sequence new text.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	training phase, cases containing information word, context correct tag stored memory.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	system used access information focus word two positions after, least known words.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	beam search used find highest probability tag sequence.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	system Brill's system used default settings suggested documentation.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tagging, manually checked corrected, generally accepted quite accurate.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	use slight adaptation tagset.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	experiment, divide corpus three parts.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	taking first eight utterances every ten.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	part used train individual tag- gers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	data Test never inspected detail used benchmark tagging quality measurement.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	quality individual tuggers (cf.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, room improvement enough.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	explained above, combination lead improvement, component taggers must differ errors make.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	indeed case seen Table 1.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	shows 99.22% Tune, least one tagger selects correct tag.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	patterns brackets give distribution correct/incorrect tags systems.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tag case.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	following sections examine number them.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	accuracy measurements listed Table 2.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	5 straightforward selection method n-way vote.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tagger allowed vote tag choice tag highest number votes selected.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	6 question large vote allow tagger.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	democratic option give tagger one vote (Majority).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, appears useful give weight taggers proved quality.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	information tagger's quality derived inspection results Tune.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	6In experiment, random selection among winning tags made whenever tie.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Table 2: Accuracy individual taggers combination methods.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	even information well taggers perform.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Pairwise Voting far, used information performance individual taggers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	next step examine pairs.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	probability tag Tx given tagger suggested tag Ti.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	principle, could remove restriction gain 22 1111 cases.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	practice, chance beat majority slight indeed get hopes high happen often.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	usually called stacking (Wolpert 1992).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	first choice use Memory- Based second level learner.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	conjecture pruning beneficial interesting cases rare.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	important observation every combination (significantly) outperforms combination strict subset components.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Also note improvement yielded best combination.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Maximum Entropy tagger (97.43%).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	turns increase individual taggers quite limited compared combination.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Conclusion.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"One best methods tagger combination (van Halteren, Zavrel, Daele­ mans 1998) TagPair method.</S><S sid =""103"" ssid = ""53"">It looks situations one tagger suggests tag1 tag2 estimates probability situation tag actually tagx.</S><S sid =""104"" ssid = ""54"">Although presented variant voting paper, fact also stacked classifier, necessarily select one tags suggested component taggers."	thanks go creators tagger generators used making systems available.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Improving Data Driven Wordclass Tagging System Combination	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	means experiment involving task morpho-syntactic wordclass tagging.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	comparison, outputs combined using several voting strategies second stage classifiers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Data driven methods appear popular.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	data driven method used, model automatically learned implicit structure annotated training corpus.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	much easier quickly lead model produces results 'reasonably' good quality.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Obviously, 'reasonably good quality' ultimate goal.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Unfortunately, quality reached given task limited, merely potential learning method used.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	limiting factors power hard- software used implement learning method availability training material.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	machine learning literature approach known ensemble, stacked, combined classifiers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	underlying assumption twofold.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	First, combined votes make system robust quirks learner's particular bias.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	execute investigation means experiment.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	NLP task used experiment morpho-syntactic wordclass tagging.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	reasons choice several.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	First all, tagging widely researched well-understood task (cf.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	van Halteren (ed.)	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	1998).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	van Halteren 1996).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	varied systems available, variety hope lead better combination effects.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	experiment selected four systems, primarily basis availability.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	uses different features text tagged, completely different representation language model.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Viterbi algorithm used determine probable tag sequence.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	cs.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	j hu.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	I. 14.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tar.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tagging rules applied sequence new text.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	training phase, cases containing information word, context correct tag stored memory.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	system used access information focus word two positions after, least known words.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	beam search used find highest probability tag sequence.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	system Brill's system used default settings suggested documentation.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	data use experiment consists tagged LOB corpus (Johansson 1986).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tagging, manually checked corrected, generally accepted quite accurate.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	use slight adaptation tagset.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	experiment, divide corpus three parts.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	taking first eight utterances every ten.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	part used train individual tag- gers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	data Test never inspected detail used benchmark tagging quality measurement.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	quality individual tuggers (cf.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, room improvement enough.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	explained above, combination lead improvement, component taggers must differ errors make.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	indeed case seen Table 1.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	shows 99.22% Tune, least one tagger selects correct tag.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	patterns brackets give distribution correct/incorrect tags systems.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tag case.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	following sections examine number them.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	accuracy measurements listed Table 2.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	5 straightforward selection method n-way vote.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tagger allowed vote tag choice tag highest number votes selected.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	6 question large vote allow tagger.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	democratic option give tagger one vote (Majority).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, appears useful give weight taggers proved quality.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	information tagger's quality derived inspection results Tune.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	6In experiment, random selection among winning tags made whenever tie.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Table 2: Accuracy individual taggers combination methods.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	even information well taggers perform.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Pairwise Voting far, used information performance individual taggers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	next step examine pairs.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	probability tag Tx given tagger suggested tag Ti.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	principle, could remove restriction gain 22 1111 cases.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	practice, chance beat majority slight indeed get hopes high happen often.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	usually called stacking (Wolpert 1992).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	second stage provided first level outputs, additional information, e.g. original input pattern.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	first choice use Memory- Based second level learner.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	basic version (Tags), case consists tags suggested component taggers correct tag.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	conjecture pruning beneficial interesting cases rare.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	1°Tags+Word could handled C5.0 due huge number feature values.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	important observation every combination (significantly) outperforms combination strict subset components.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Also note improvement yielded best combination.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Maximum Entropy tagger (97.43%).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	turns increase individual taggers quite limited compared combination.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Conclusion.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
"important result undergone change van Halteren, Zavrel, Daelemans (1998) current experiments relative accuracy TagPair stacked systems MBL.</S><S sid =""488"" ssid = ""82"">Where TagPair used significantly better MBL, roles well reversed."	thanks go creators tagger generators used making systems available.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Improving Data Driven Wordclass Tagging System Combination	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	means experiment involving task morpho-syntactic wordclass tagging.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	comparison, outputs combined using several voting strategies second stage classifiers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Data driven methods appear popular.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	much easier quickly lead model produces results 'reasonably' good quality.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Obviously, 'reasonably good quality' ultimate goal.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	limiting factors power hard- software used implement learning method availability training material.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	underlying assumption twofold.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	First, combined votes make system robust quirks learner's particular bias.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	execute investigation means experiment.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	NLP task used experiment morpho-syntactic wordclass tagging.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	reasons choice several.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	First all, tagging widely researched well-understood task (cf.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	van Halteren (ed.)	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	1998).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	van Halteren 1996).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	varied systems available, variety hope lead better combination effects.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	experiment selected four systems, primarily basis availability.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	uses different features text tagged, completely different representation language model.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Viterbi algorithm used determine probable tag sequence.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	cs.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	j hu.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	I. 14.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tar.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tagging rules applied sequence new text.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	training phase, cases containing information word, context correct tag stored memory.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	system used access information focus word two positions after, least known words.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	beam search used find highest probability tag sequence.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	system Brill's system used default settings suggested documentation.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	data use experiment consists tagged LOB corpus (Johansson 1986).	1
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tagging, manually checked corrected, generally accepted quite accurate.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	use slight adaptation tagset.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	experiment, divide corpus three parts.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	taking first eight utterances every ten.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	part used train individual tag- gers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	data Test never inspected detail used benchmark tagging quality measurement.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	quality individual tuggers (cf.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, room improvement enough.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	explained above, combination lead improvement, component taggers must differ errors make.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	indeed case seen Table 1.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	shows 99.22% Tune, least one tagger selects correct tag.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	patterns brackets give distribution correct/incorrect tags systems.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tag case.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	following sections examine number them.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	accuracy measurements listed Table 2.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	5 straightforward selection method n-way vote.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tagger allowed vote tag choice tag highest number votes selected.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	6 question large vote allow tagger.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	democratic option give tagger one vote (Majority).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, appears useful give weight taggers proved quality.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	information tagger's quality derived inspection results Tune.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	6In experiment, random selection among winning tags made whenever tie.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Table 2: Accuracy individual taggers combination methods.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	even information well taggers perform.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Pairwise Voting far, used information performance individual taggers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	next step examine pairs.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	probability tag Tx given tagger suggested tag Ti.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	principle, could remove restriction gain 22 1111 cases.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	practice, chance beat majority slight indeed get hopes high happen often.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	usually called stacking (Wolpert 1992).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	first choice use Memory- Based second level learner.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	conjecture pruning beneficial interesting cases rare.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	1°Tags+Word could handled C5.0 due huge number feature values.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	important observation every combination (significantly) outperforms combination strict subset components.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Also note improvement yielded best combination.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Maximum Entropy tagger (97.43%).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	turns increase individual taggers quite limited compared combination.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Conclusion.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
first LOB corpus (Johansson 1986), used earlier experiments well (van Halteren, Zavrel, Daelemans 1998) proved good testing ground.	thanks go creators tagger generators used making systems available.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Improving Data Driven Wordclass Tagging System Combination	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	means experiment involving task morpho-syntactic wordclass tagging.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	comparison, outputs combined using several voting strategies second stage classifiers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Data driven methods appear popular.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	data driven method used, model automatically learned implicit structure annotated training corpus.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	much easier quickly lead model produces results 'reasonably' good quality.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Obviously, 'reasonably good quality' ultimate goal.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Unfortunately, quality reached given task limited, merely potential learning method used.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	limiting factors power hard- software used implement learning method availability training material.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	machine learning literature approach known ensemble, stacked, combined classifiers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	underlying assumption twofold.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	First, combined votes make system robust quirks learner's particular bias.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	execute investigation means experiment.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	NLP task used experiment morpho-syntactic wordclass tagging.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	reasons choice several.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	First all, tagging widely researched well-understood task (cf.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	van Halteren (ed.)	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	1998).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	van Halteren 1996).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	varied systems available, variety hope lead better combination effects.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	experiment selected four systems, primarily basis availability.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	uses different features text tagged, completely different representation language model.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Viterbi algorithm used determine probable tag sequence.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	cs.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	j hu.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	I. 14.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tar.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tagging rules applied sequence new text.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	training phase, cases containing information word, context correct tag stored memory.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	system used access information focus word two positions after, least known words.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	beam search used find highest probability tag sequence.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	system Brill's system used default settings suggested documentation.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	data use experiment consists tagged LOB corpus (Johansson 1986).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tagging, manually checked corrected, generally accepted quite accurate.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	use slight adaptation tagset.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	experiment, divide corpus three parts.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	taking first eight utterances every ten.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	part used train individual tag- gers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	data Test never inspected detail used benchmark tagging quality measurement.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	quality individual tuggers (cf.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, room improvement enough.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	explained above, combination lead improvement, component taggers must differ errors make.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	indeed case seen Table 1.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	shows 99.22% Tune, least one tagger selects correct tag.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	patterns brackets give distribution correct/incorrect tags systems.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tag case.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	following sections examine number them.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	accuracy measurements listed Table 2.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	5 straightforward selection method n-way vote.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tagger allowed vote tag choice tag highest number votes selected.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	6 question large vote allow tagger.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	democratic option give tagger one vote (Majority).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, appears useful give weight taggers proved quality.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	information tagger's quality derived inspection results Tune.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	6In experiment, random selection among winning tags made whenever tie.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Table 2: Accuracy individual taggers combination methods.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	even information well taggers perform.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Pairwise Voting far, used information performance individual taggers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	next step examine pairs.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	probability tag Tx given tagger suggested tag Ti.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	principle, could remove restriction gain 22 1111 cases.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	practice, chance beat majority slight indeed get hopes high happen often.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	usually called stacking (Wolpert 1992).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	second stage provided first level outputs, additional information, e.g. original input pattern.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	first choice use Memory- Based second level learner.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	basic version (Tags), case consists tags suggested component taggers correct tag.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	conjecture pruning beneficial interesting cases rare.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	1°Tags+Word could handled C5.0 due huge number feature values.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	important observation every combination (significantly) outperforms combination strict subset components.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Also note improvement yielded best combination.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Maximum Entropy tagger (97.43%).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	turns increase individual taggers quite limited compared combination.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Conclusion.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
van Halteren, Zavrel, Daelemans (1998) used straightforward im­ plementation HMM&apos;s, turned worst accuracy four competing methods.	thanks go creators tagger generators used making systems available.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Improving Data Driven Wordclass Tagging System Combination	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	paper examine differences modelling different data driven systems performing NLP task exploited yield higher accuracy best individual system.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	means experiment involving task morpho-syntactic wordclass tagging.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules Maximum Entropy) trained corpus data.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	comparison, outputs combined using several voting strategies second stage classifiers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	combination taggers outperform best component, best combination showing 19.1% lower error rate best individual tagger.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Natural Language Processing (NLP) systems, find one language models used predict, classify and/or interpret language related observations.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Traditionally, models categorized either rule-based/symbolic corpus-based/probabilistic.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Recent work (e.g. Brill 1992) demonstrated clearly categorization fact mix-up two distinct Categorization systems: one hand representation used language model (rules, Markov model, neural net, case base, etc.) hand manner model constructed (hand crafted vs. data driven).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Data driven methods appear popular.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	explained fact that, general, hand crafting explicit model rather difficult, especially since modelled, natural language, (yet) well- understood.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	data driven method used, model automatically learned implicit structure annotated training corpus.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	much easier quickly lead model produces results 'reasonably' good quality.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Obviously, 'reasonably good quality' ultimate goal.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Unfortunately, quality reached given task limited, merely potential learning method used.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	limiting factors power hard- software used implement learning method availability training material.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	limitations, find tasks (at point time) faced ceiling quality reached (then) available machine learning system.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, fact given system cannot go beyond ceiling mean machine learning whole similarly limited.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	potential loophole type learning method brings 'inductive bias' task therefore different methods tend produce different errors.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	paper, concerned question whether differences models indeed exploited yield data driven model superior performance.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	machine learning literature approach known ensemble, stacked, combined classifiers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	shown that, errors uncorrelated sufficient degree, resulting combined classifier often perform better individual systems (Ali Pazzani 1996; Chan Stolfo 1995; Tumer Gosh 1996).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	underlying assumption twofold.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	First, combined votes make system robust quirks learner's particular bias.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Also, use information individual method's behaviour principle even admits possibility fix collective errors.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	execute investigation means experiment.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	NLP task used experiment morpho-syntactic wordclass tagging.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	reasons choice several.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	First all, tagging widely researched well-understood task (cf.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	van Halteren (ed.)	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	1998).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Second, current performance levels task still leave room improvement: 'state art' performance data driven automatic wordclass taggers (tagging English text single tags low detail tagset) 9697% correctly tagged words.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Finally, number rather different methods available generate fully functional tagging system annotated text.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Component taggers 1992, van Halteren combined number taggers way straightforward majority vote (cf.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	van Halteren 1996).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Since component taggers used n-gram statistics model context probabilities knowledge representation hence fundamentally component, results limited.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	varied systems available, variety hope lead better combination effects.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	experiment selected four systems, primarily basis availability.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	uses different features text tagged, completely different representation language model.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	first oldest system uses traditional trig-ram model (Steetskamp 1995; henceforth tagger T, Trigrams), based context statistics P(ti[ti-l,ti-2) lexical statistics P(tilwi) directly estimated relative corpus frequencies.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Viterbi algorithm used determine probable tag sequence.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Since model facilities handling unknown words, Memory-Based system (see below) used propose distributions potential tags words lexicon.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	second system Transformation Based Learning system described Brill (19941; henceforth tagger R, Rules).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	1 Brill's system available collection C programs Perl scripts ftp ://ftp.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	cs.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	j hu.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	I. 14.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tar.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Z. system starts basic corpus annotation (each word tagged likely tag) searches space transformation rules order reduce discrepancy current annotation correct one (in case 528 rules learned).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tagging rules applied sequence new text.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	four systems, one access information: contextual information (the words tags window spanning three positions focus word) well lexical information (the existence words formed suffix/prefix addition/deletion).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, actual use information severely limited individual information items combined according patterns laid rule templates.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	third system uses Memory-Based Learning described Daelemans et al. (1996; henceforth tagger M, Memory).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	training phase, cases containing information word, context correct tag stored memory.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tagging, case similar focus word retrieved memory, indexed basis Information Gain feature, accompanying tag selected.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	system used access information focus word two positions after, least known words.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	unknown words, single position after, three suffix letters, information capitalization presence hyphen digit used.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	fourth final system MXPOST system described Ratnaparkhi (19962; henceforth tagger E, Entropy).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	uses number word context features rather similar system M, trains Maximum Entropy model assigns weighting parameter feature-value combination features relevant estimation probability P(tag[features).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	beam search used find highest probability tag sequence.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	system Brill's system used default settings suggested documentation.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	2Ratnaparkhi's Java implementation system available ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	data use experiment consists tagged LOB corpus (Johansson 1986).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	corpus comprises one million words, divided 500 samples 2000 words 15 text types.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tagging, manually checked corrected, generally accepted quite accurate.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	use slight adaptation tagset.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	"changes mainly cosmetic, e.g. non-alphabetic characters ""$"" tag names replaced."	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	"However, also retokenization: genitive markers split negative marker ""n't"" reattached."	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	example sentence tagged resulting tagset is: ATI singular plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense verb singular article invitation NN singular common noun preposition ABN pre-quantifier ATI singular plural article parliamentary JJ adjective candidates NNS plural common noun SPER period tagset consists 170 different tags (including ditto tags 3) average ambiguity 2.69 tags per wordform.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	difficulty tagging task judged two baseline measurements Table 2 below, representing completely random choice potential tags token (Random) selection lexically likely tag (LexProb).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	experiment, divide corpus three parts.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	"first part, called Train, consists 80% data (931062 tokens), constructed 3Ditto tags used components multi- token units, e.g. ""as well as"" taken coordination conjunction, tagged ""as_CC1 well_CC2 as_CC3"", using three related different ditto tags."	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	taking first eight utterances every ten.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	part used train individual tag- gers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	second part, Tune, consists 10% data (every ninth utterance, 114479 tokens) used select best tagger parameters applicable develop combination methods.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	third final part, Test, consists remaining 10% (.115101 tokens) used final performance measurements tuggers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Tune Test contain around 2.5% new tokens (wrt Train) 0.2% known tokens new tags.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	data Train (for individual tuggers) Tune (for combination tuggers) information used tagger construction: components tuggers (lexicon, context statistics, etc.) entirely data driven manual adjustments done.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	data Test never inspected detail used benchmark tagging quality measurement.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	order see whether combination component tuggers likely lead improvements tagging quality, first examine results individual taggers applied Tune.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	far know also one first rigorous measurements relative quality different tagger generators, using single tagset dataset identical circumstances.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	quality individual tuggers (cf.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Table 2 below) certainly still leaves room improvement, although tagger E surprises us accuracy well results reported far makes us less confident gain accomplished combination.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, room improvement enough.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	explained above, combination lead improvement, component taggers must differ errors make.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	indeed case seen Table 1.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	shows 99.22% Tune, least one tagger selects correct tag.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, unlikely able identify 4This implies impossible note errors counted tagger fact errors benchmark tagging.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	accept measuring quality relation specific tagging rather linguistic truth (if exists) hope tagged LOB corpus lives reputation.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 Taggers Wrong 0.78 Table 1: Tagger agreement Tune.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	patterns brackets give distribution correct/incorrect tags systems.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tag case.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	rather aim optimal selection cases correct tag outvoted, would ideally lead correct tagging 98.21% words (in Tune).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Simple Voting many ways results component taggers combined, selecting single tag set proposed taggers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	following sections examine number them.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	accuracy measurements listed Table 2.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	5 straightforward selection method n-way vote.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tagger allowed vote tag choice tag highest number votes selected.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	6 question large vote allow tagger.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	democratic option give tagger one vote (Majority).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, appears useful give weight taggers proved quality.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	general quality, e.g. tagger votes overall precision (TotPrecision), quality relation current situation, e.g. tagger votes precision suggested tag (Tag- Precision).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	information tagger's quality derived inspection results Tune.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	5For tag X, precision measures percentage tokens tagged X tagger also tagged X benchmark recall measures percentage tokens tagged X benchmark also tagged X tagger.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	abstracting away individual tags, precision recall equal measure many tokens tagged correctly; case also use generic term accuracy.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	6In experiment, random selection among winning tags made whenever tie.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Table 2: Accuracy individual taggers combination methods.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	even information well taggers perform.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	know whether believe propose (precision) also know often fail recognize correct tag (recall).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	information used forcing tagger also add vote tags suggested opposition, amount equal 1 minus recall opposing tag (Precision-Recall).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	turns out~ voting systems outperform best single tagger, E. 7 Also, best voting system one specific information used, Precision-Recall.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	However, specific information always superior, TotPrecision scores higher TagPrecision.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	might explained fact recall information missing (for overall performance matter, since recall equal precision).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	7Even worst combinator, Majority, significantly better E: using McNemar's chi-square, p--0.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Pairwise Voting far, used information performance individual taggers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	next step examine pairs.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	investigate situations one tagger suggests T1 T2 estimate probability situation tag actually Tx.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	combining taggers, every tagger pair taken turn allowed vote (with probability described above) possible tag, i.e. ones suggested component taggers.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	tag pair T1T2 never observed Tune, fall back information individual taggers, viz.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	probability tag Tx given tagger suggested tag Ti.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Note method (and next section) tag suggested minority (or even none) taggers still chance win.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	principle, could remove restriction gain 22 1111 cases.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	practice, chance beat majority slight indeed get hopes high happen often.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	used Test, pairwise voting strategy (TagPair) clearly outperforms voting strategies, 8 yet approach level tying majority votes handled correctly (98.31%).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Stacked classifiers measurements far appears use detailed information leads better accuracy improvement.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	ought therefore advantageous step away underlying mechanism voting model situations observed Tune closely.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	practice feeding outputs number classifiers features next learner sit significantly better runner-up (Precision-Recall) p=0.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	usually called stacking (Wolpert 1992).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	second stage provided first level outputs, additional information, e.g. original input pattern.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	first choice use Memory- Based second level learner.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	basic version (Tags), case consists tags suggested component taggers correct tag.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	advanced versions also add information word question (Tags+Word) tags suggested taggers previous next position (Tags+Context).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	first two similarity metric used tagging straightforward overlap count; third need use Information Gain weighting (Daelemans ct al. 1997).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Surprisingly, none Memory-Based based methods reaches quality TagPair.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	9 explanation found examine differences within Memory- Based general strategy: feature information stored, higher accuracy Tune, lower accuracy Test.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	likely overtraining effect: Tune probably small collect case bases leverage stacking effect convincingly, especially since 7.51% second stage material shows disagreement featured tags.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	examine overtraining effects specific particular second level classifier, also used C5.0 system, commercial version well-known program C4.5 (Quinlan 1993) induction decision trees, training material.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	1° C5.0 prunes decision tree, overfitting training material (Tune) less Memory-Based learning, results Test also worse.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	conjecture pruning beneficial interesting cases rare.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	realise benefits stacking, either data needed second stage classifier better suited type problem.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	9Tags (Memory-Based) scores significantly worse TagPair (p=0.0274) significantly better Precision-Recall (p=0.2766).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	1°Tags+Word could handled C5.0 due huge number feature values.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Test Increase vs % Reduc- Component tion Error Average Rate Best Component 96.08 - R 96.46 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores Test Pairwise Voting tagger combinations 7 value combination.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	relation accuracy combinations (using TagPair) individual taggers shown Table 3.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	important observation every combination (significantly) outperforms combination strict subset components.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Also note improvement yielded best combination.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	pairwise voting system, using four individual taggers, scores 97.92% correct Test, 19.1% reduction error rate best individual system, viz.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Maximum Entropy tagger (97.43%).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	major factor quality combination results obviously quality best component: combinations E score higher without E (although M, R together able beat E alone11).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	that, decisive factor appears difference language model: generally better combiner R, 12 even though lowest accuracy operating alone.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	possible criticism proposed combi11By margin edge significance: p=0.0608.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	12Although significantly better, e.g. differences within group ME/ER/ET significant.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	nation scheme fact successful combination schemes, one reserve nontrivial portion (in experiment 10% total material) annotated data set parameters combination.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	see whether fact good way spend extra data, also trained two best individual systems (E M, exactly settings first experiments) concatenation Train Tune, access every piece data combination seen.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	turns increase individual taggers quite limited compared combination.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	extensively trained E scored 97.51% correct Test (3.1% error reduction) 97.07% (3.9% error reduction).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Conclusion.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	experiment shows that, least task hand, combination several different systems allows us raise performance ceiling data driven systems.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Obviously still room closer examination differences combination methods, e.g. question whether Memory-Based combination would performed better provided training data Tune, remaining errors, e.g. effects inconsistency data (cf.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Ratnaparkhi 1996 effects Penn Treebank corpus).	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	Regardless closer investigation, feel results encouraging enough extend investigation combination, starting additional component taggers selection strategies, going shifts tagsets and/or languages.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	investigation need limited wordclass tagging, expect many NLP tasks combination could lead worthwhile improvements.	0
LOB single 114K tune set (van Halteren, Zavrel, Daelemans 1998), MBL Decision Trees degraded significantly adding context, MBL degraded adding word	thanks go creators tagger generators used making systems available.	0
