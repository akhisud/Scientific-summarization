In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.	Implication_Citation
Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.	Method_Citation
The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.	Method_Citation
The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.  In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.	Method_Citation
In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.  There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..  Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.	Results_Citation
Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.	Method_Citation
5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..	Results_Citation
CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.  The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.  The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.  Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.	Method_Citation
In this article, the management succession scenario will be used as the basis for discussion.  The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.	Results_Citation
When the outputs are scored in &quot;key-to-response&quot; mode, as though one annotator&apos;s output represented the &quot;key&quot; and the other the &quot;response,&quot; the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.  Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&amp;R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA &quot;fast&quot; configuration 95.66, SRA &quot;fastest&quot; configuration 92.61, SRA &quot;nonames&quot; configuration 94.92, SRI 94.0, Sterling Software 92.74..	Results_Citation
Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..	Method_Citation
No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.  The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.	Implication_Citation
If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?  We will show that in both cases the estimated probability is tight.	Aim_Citation
We will show that in both cases the estimated probability is tight.	Aim_Citation
(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.	Method_Citation
If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?  We will show that in both cases the estimated probability is tight.	Aim_Citation
We will show that in both cases the estimated probability is tight.	Aim_Citation
(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.	Method_Citation
We will show that in both cases the estimated probability is tight.	Aim_Citation
We will show that in both cases the estimated probability is tight.	Aim_Citation
Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization.	Aim_Citation
(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.	Method_Citation
Estimation of Probabilistic Context-Free Grammars	Aim_Citation
Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization.	Method_Citation
If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?  We will show that in both cases the estimated probability is tight.	Aim_Citation
We will show that in both cases the estimated probability is tight.	Aim_Citation
If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?  We will show that in both cases the estimated probability is tight.	Aim_Citation
The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.	Aim_Citation
Estimation of Probabilistic Context-Free Grammars	Aim_Citation
We show here that estimated production probabilities always yield proper distributions.	Results_Citation
Estimation of Probabilistic Context-Free Grammars	Aim_Citation
If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?  We will show that in both cases the estimated probability is tight.	Aim_Citation
The work described here also makes use of a hidden Markov model.	Method_Citation
A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).  A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.	Method_Citation
An alternative to uniformly increasing the order of the conditioning is to extend it selectively.  Mixed higher- order context can be modeled by introducing explicit state sequences.  In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.	Method_Citation
The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.	Aim_Citation
Mixed higher- order context can be modeled by introducing explicit state sequences.  The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.	Method_Citation
In this regard, word equivalence classes were used (Kupiec, 1989).  There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.	Method_Citation
In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.  The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.	Method_Citation
The model has the advantage that a pre-tagged training corpus is not required.	Implication_Citation
A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.	Method_Citation
In this regard, word equivalence classes were used (Kupiec, 1989).  There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.	Method_Citation
The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.  A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.	Method_Citation
Table 2: Accuracy of individual taggers and combination methods.	Method_Citation
A next step is to examine them in pairs.  We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.	Method_Citation
In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.  We accept that we are measuring quality in relation to a specific tagging	Method_Citation
However, it appears more useful to give more weight to taggers which have proved their quality.  This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).  The information about each tagger&apos;s quality is derived from an inspection of its results on Tune.	Method_Citation
A next step is to examine them in pairs.  We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.  When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).	Results_Citation
5 The most straightforward selection method is an n-way vote.	Method_Citation
The most democratic option is to give each tagger one vote (Majority).  This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).  A next step is to examine them in pairs.  When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.	Method_Citation
Pairwise Voting	Method_Citation
The most democratic option is to give each tagger one vote (Majority).	Aim_Citation
Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.	Results_Citation
However, it appears more useful to give more weight to taggers which have proved their quality.  This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).	Method_Citation
The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.  Also of note is the improvement yielded by the best combination.  The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.	Results_Citation
However, it appears more useful to give more weight to taggers which have proved their quality.  This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).	Method_Citation
The first choice for this is to use a Memory- Based second level learner.  To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.	Method_Citation
A next step is to examine them in pairs.  We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.	Method_Citation
The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.	Method_Citation
This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.  The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.  Also of note is the improvement yielded by the best combination.  The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.  Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages.	Results_Citation
The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.  All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.  This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.	Implication_Citation
Improving Data Driven Wordclass Tagging by System Combination  Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.  After comparison, their outputs are combined using several voting strategies and second stage classifiers.  Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.	Method_Citation
A next step is to examine them in pairs.  We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.  The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.  is usually called stacking (Wolpert 1992).  Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	Method_Citation
When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).  Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.	Results_Citation
The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).	Method_Citation
Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.  Table 2: Accuracy of individual taggers and combination methods.	Results_Citation
The first choice for this is to use a Memory- Based second level learner.  Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.  To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.  1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.	Results_Citation
By contrast, the Troll system described in this paper has an etfeetive algorithm f&lt;&gt;r deciding well-formedness	Method_Citation
THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES	Aim_Citation
By contrast, the Troll system described in this paper has an etfeetive algorithm f&lt;&gt;r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.  We write fS, ~vf8 and &apos;D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively.  Say that F&apos;E RFS is a resolvant of F E FS iff F and F&apos; have the same underlying graph and F subsumes F&apos;.  Let type resolution be the total function R:-&gt;DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.  APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F&apos;E DRFS then F U F&apos; E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability.  It would, of course, not be very efficient to work with such large disjunctions of feature structures.	Method_Citation
1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing	Hypothesis_Citation
Let type resolution be the total function R:-&gt;DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.  C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl&quot; the feature structure is satisfiable.  The Troll system, which is based on this idea, effectively inqflements type resolution.	Method_Citation
Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.  Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient.  While the accuracy of this approach was as good as any previously published results, the learned models were complex and diÆcult to interpret, in e?ect acting as very accurate black boxes.	Method_Citation
The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.  The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.  We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.	Method_Citation
This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.	Results_Citation
This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.	Results_Citation
The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.  The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.	Method_Citation
This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.	Method_Citation
However, (Cressie and Read, 1984) suggest that there are cases where Pearson&apos;s statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other.  Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.	Implication_Citation
The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.  Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.  A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson&apos;sX 2 statistic.	Method_Citation
The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).  We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.  Table 9 Combined utterance classification accuracies (chance = 35%).  Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).	Method_Citation
For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.  Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.	Aim_Citation
The computation of likelihoods P(EIU ) depends on the types of evidence used.  Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U).	Method_Citation
&lt;.1% Hey thanks a lot &lt;.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.  For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.	Aim_Citation
A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking.  These are usually referred to in the conversation analysis literature as &quot;continuers&quot; and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970).	Method_Citation
Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I&apos;m in the legal department.	Method_Citation
While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).  Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.	Method_Citation
&lt;.1% Hey thanks a lot &lt;.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.  However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence.  The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.	Aim_Citation
For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).  The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).  The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui).	Aim_Citation
Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.  In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.	Method_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula:	Method_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.	Method_Citation
Using Random Walks for Question-focused Sentence Retrieval  In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary.	Method_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.  The output of our system, a ranked list of sentences relevant to the user’s question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences.  Alternatively, the sentences canbe returned to the user as a question-focused summary.  To apply LexRank, a similarity graph is producedfor the sentences in an input document set.  In thegraph, each node represents a sentence.  There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.  The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.	Method_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.	Aim_Citation
itowever, the problem with his method is that a unitication result graph consists only of newly created structures.  This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.  Copying sharable parts is called redundant copying.	Implication_Citation
Strategic Lazy Incremental Copy Graph Unification	Method_Citation
Strategic Lazy Incremental Copy Graph Unification	Method_Citation
Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.  This reduces repeated calculation of substructures.	Aim_Citation
The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,&apos;educes unnecessary computation.	Implication_Citation
in this method, theretbre, the failure tendency information is acquired by a learning process.  That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.  in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.	Method_Citation
This paper proposes an FS unification method that allows structure sharing with constant m&apos;der node access time.  This method achieves structure sharing by introducing lazy copying to Wroblewski&apos;s incremental copy graph unification method.  Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.	Method_Citation
Strategic Lazy Incremental Copy Graph Unification	Method_Citation
Copying sharable parts is called redundant copying.  Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.	Method_Citation
Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.	Implication_Citation
For example, a spoken Present.  Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.	Results_Citation
The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira&apos;s method.  This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.  Copying sharable parts is called redundant copying.  A better method would nfinimize the copying of sharable varts.	Method_Citation
This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.  Copying sharable parts is called redundant copying.  A better method would nfinimize the copying of sharable varts.	Method_Citation
Copying sharable parts is called redundant copying.  A better method would nfinimize the copying of sharable varts.	Implication_Citation
Copying sharable parts is called redundant copying.  5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.  With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7&gt;) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/&lt;a c&gt; in Fig.	Aim_Citation
A better method would nfinimize the copying of sharable varts.	Method_Citation
To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	Method_Citation
Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.  To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	Method_Citation
We then determined the WordNet synsets which most adequately characterized the sense clusters.	Method_Citation
This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.	Aim_Citation
Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	Method_Citation
Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	Method_Citation
To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	Method_Citation
The algorithm is based on a graph model representing words and relationships between them.  Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	Method_Citation
Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used.	Results_Citation
Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.	Method_Citation
Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. &quot;genomic DNA from rat, mouse and dog&quot;.	Method_Citation
To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).	Method_Citation
The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the &quot;best&quot; cluster, it suffices to build a relatively small graph in 1.	Method_Citation
This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated.	Implication_Citation
To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).  The algorithm is based on a graph model representing words and relationships between them.  Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.	Method_Citation
TheFigure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration	Results_Citation
Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints	Results_Citation
In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94	Results_Citation
6 96	Results_Citation
3 96	Results_Citation
6 95	Results_Citation
5 95	Results_Citation
4 93	Results_Citation
3 95	Results_Citation
6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag	Results_Citation
Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints	Method_Citation
We consider the unsupervised POS induction problem without the use of a tagging dictionary	Method_Citation
We consider the unsupervised POS induction problem without the use of a tagging dictionary	Method_Citation
For all languages we do not make use of a tagging dictionary	Method_Citation
Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts	Results_Citation
Simply assigning to each word its most frequent associated tag in a corpus achieves 94	Results_Citation
6% accuracy on the WSJ portion of the Penn Treebank	Results_Citation
We consider the unsupervised POS induction problem without the use of a tagging dictionary	Method_Citation
We consider the unsupervised POS induction problem without the use of a tagging dictionary	Method_Citation
We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon	Method_Citation
We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon	Aim_Citation
First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e	Method_Citation
g	Method_Citation
, there are fewer unique determiners than unique nouns)	Method_Citation
  Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution ψ over tag assignments drawn from DIRICHLET(β, K )	Method_Citation
  During training, we treat as observed the language word types W as well as the token-level corpus w	Method_Citation
 We utilize Gibbs sampling to approximate our collapsed model posterior:	Method_Citation
5 60	Results_Citation
6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5)	Results_Citation
We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model	Method_Citation
The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word	Method_Citation
  Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag	Method_Citation
We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model	Aim_Citation
We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model	Method_Citation
\Ve treat context-sensitive spelling correction as a task of word disambiguation	Aim_Citation
\Ve treat context-sensitive spelling correction as a task of word disambiguation	Method_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Aim_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
\Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
Table 1 shows the performance of the baseline method for 18 confusion sets	Results_Citation
This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence	Hypothesis_Citation
This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence	Method_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Hypothesis_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
  \Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations	Method_Citation
\Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
Table 1 shows the performance of the baseline method for 18 confusion sets	Results_Citation
  The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Results_Citation
  and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al	Results_Citation
, 1993]	Results_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Aim_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
  The ambiguity among words is modelled by confusion sets	Method_Citation
  A confusion set C = { w 1, 	Method_Citation
,wn} means that each word Wi in the set is ambiguous with each other word in the set	Method_Citation
  Thus if C = {deserÂ·t, desserÂ·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be	Method_Citation
The probability for each Wi is calculated using Bayes&apos; rule: As it stands, the likelihood term, p( c_k	Method_Citation
  , c_ 1, c1, 	Method_Citation
  , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a	Method_Citation
 severe sparse-data problem	Method_Citation
  Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word	Method_Citation
This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence	Method_Citation
This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence	Results_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Results_Citation
  A method for doing this, based on Bayesian classifiers, was presented	Method_Citation
  A method for doing this, based on Bayesian classifiers, was presented	Results_Citation
  It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists	Method_Citation
  It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists	Results_Citation
\Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
  \Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations	Method_Citation
Table 1 shows the performance of the baseline method for 18 confusion sets	Results_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Results_Citation
  The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Aim_Citation
  The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Results_Citation
The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Results_Citation
  and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al	Results_Citation
, 1993]	Results_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
Thus if C = {deserÂ·t, desserÂ·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
  The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;	Method_Citation
  An ambiguous target word is then classified by finding all collocations that match its context	Method_Citation
A method for doing this, based on Bayesian classifiers, was presented	Method_Citation
A method for doing this, based on Bayesian classifiers, was presented	Results_Citation
  It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists	Method_Citation
  It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists	Results_Citation
Trigrams are at their worst when the words in the confusion set have the same part of speech	Results_Citation
  In such cases, the Bayesian hybrid method is clearly better	Results_Citation
\Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
\Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
A method is presented for doing this, based on Bayesian classifiers	Aim_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
This paper takes Yarowsky&apos;s method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence	Hypothesis_Citation
A method is presented for doing this, based on Bayesian classifiers	Method_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Aim_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
The second tests for collocations - patterns of words and part-of-speech tags around the target word	Method_Citation
The second tests for collocations - patterns of words and part-of-speech tags around the target word	Aim_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Results_Citation
  The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Aim_Citation
  The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera	Results_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)	Method_Citation
\Ve treat context-sensitive spelling correction as a task of word disambiguation	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Aim_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Method_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Aim_Citation
  \Ve try two ways of combining these components: decision lists, and Bayesian classifiers	Method_Citation
The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction	Aim_Citation
The focus of our work is on the use of contextual role knowledge for coreference resolution	Aim_Citation
We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions	Method_Citation
  BABAR employs information extraction techniques to represent and learn role relationships	Method_Citation
  We evaluated BABAR on two domains: terrorism and natural disasters	Method_Citation
Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes	Method_Citation
We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions	Method_Citation
  BABAR employs information extraction techniques to represent and learn role relationships	Method_Citation
  Each pattern represents the role that a noun phrase plays in the surrounding context	Method_Citation
Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	Method_Citation
Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution	Aim_Citation
Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision	Method_Citation
Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus	Aim_Citation
Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus	Method_Citation
Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another	Method_Citation
1 2	Method_Citation
1	Method_Citation
2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved	Method_Citation
  Ex: Mr	Method_Citation
 Bush disclosed the policy by reading it	Method_Citation
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning	Method_Citation
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor	Aim_Citation
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor	Method_Citation
   The focus of our work is on the use of contextual role knowledge for coreference resolution	Aim_Citation
   The focus of our work is on the use of contextual role knowledge for coreference resolution	Method_Citation
  Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes	Aim_Citation
  Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes	Method_Citation
  We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain	Aim_Citation
  We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain	Method_Citation
  Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found	Aim_Citation
  Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found	Method_Citation
  One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves	Aim_Citation
  One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves	Method_Citation
A contextual role represents the role that a noun phrase plays in an event or relationship	Implication_Citation
Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes	Method_Citation
2	Method_Citation
2	Method_Citation
2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions	Method_Citation
Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes	Method_Citation
The focus of our work is on the use of contextual role knowledge for coreference resolution	Aim_Citation
Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision	Method_Citation
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning	Method_Citation
Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes	Method_Citation
Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible	Method_Citation
  We evaluated BABAR on two domains: terrorism and natural disasters	Method_Citation
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning	Method_Citation
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning	Method_Citation
  These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible	Method_Citation
The focus of our work is on the use of contextual role knowledge for coreference resolution	Aim_Citation
For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâ€™s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions	Method_Citation
For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâ€™s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions	Implication_Citation
  If so, the CF Network reports that the anaphor and candidate may be coreferent	Method_Citation
  If so, the CF Network reports that the anaphor and candidate may be coreferent	Implication_Citation
The focus of our work is on the use of contextual role knowledge for coreference resolution	Aim_Citation
Table 1: Syntactic Seeding Heuristics BABARâ€™s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge	Method_Citation
Table 1: Syntactic Seeding Heuristics BABARâ€™s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge	Results_Citation
  The confidence level is then used as the belief value for the knowledge source	Method_Citation
  The confidence level is then used as the belief value for the knowledge source	Results_Citation
  Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2	Method_Citation
  Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2	Results_Citation
1	Method_Citation
1	Results_Citation
  The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision	Method_Citation
  The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision	Results_Citation
  The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters	Method_Citation
  The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters	Results_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
We have shown that the maximum entropy framework is able to use global information directly	Results_Citation
In the maximum entropy framework, there is no such constraint	Method_Citation
  Multiple features can be used for the same token	Method_Citation
Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc	Method_Citation
 Token Information: This group consists of 10 features based on the string , as listed in Table 1	Method_Citation
A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc	Implication_Citation
 On its own, a NER can also provide users who are looking for person or organization names with quick information	Implication_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
Such constraints are derived from training data, expressing some relationship between features and outcome	Method_Citation
It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier	Method_Citation
  Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier	Method_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
For all lists except locations, the lists are processed into a list of tokens (unigrams)	Method_Citation
  Location list is processed into a list of unigrams and bigrams (e	Method_Citation
g	Method_Citation
, New York)	Method_Citation
  For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams	Method_Citation
The features we used can be divided into 2 classes: local and global	Method_Citation
  Local features are features that are based on neighboring tokens, as well as the token itself	Method_Citation
  Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc	Method_Citation
 Token Information: This group consists of 10 features based on the string , as listed in Table 1	Method_Citation
The features we used can be divided into 2 classes: local and global	Method_Citation
  Local features are features that are based on neighboring tokens, as well as the token itself	Method_Citation
  Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps	Method_Citation
  For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own	Method_Citation
We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information)	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
We have shown that the maximum entropy framework is able to use global information directly	Results_Citation
In the maximum entropy framework, there is no such constraint	Method_Citation
  Multiple features can be used for the same token	Method_Citation
Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc	Method_Citation
 Token Information: This group consists of 10 features based on the string , as listed in Table 1	Method_Citation
A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc	Implication_Citation
 On its own, a NER can also provide users who are looking for person or organization names with quick information	Implication_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
Such constraints are derived from training data, expressing some relationship between features and outcome	Method_Citation
It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier	Method_Citation
  Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier	Method_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing 	Method_Citation
 Our system is built on a maximum entropy classifier	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
For all lists except locations, the lists are processed into a list of tokens (unigrams)	Method_Citation
  Location list is processed into a list of unigrams and bigrams (e	Method_Citation
g	Method_Citation
, New York)	Method_Citation
  For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams	Method_Citation
The features we used can be divided into 2 classes: local and global	Method_Citation
  Local features are features that are based on neighboring tokens, as well as the token itself	Method_Citation
  Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc	Method_Citation
 Token Information: This group consists of 10 features based on the string , as listed in Table 1	Method_Citation
The features we used can be divided into 2 classes: local and global	Method_Citation
  Local features are features that are based on neighboring tokens, as well as the token itself	Method_Citation
  Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps	Method_Citation
  For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own	Method_Citation
We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information)	Method_Citation
Global features are extracted from other occurrences of the same token in the whole document	Method_Citation
In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data	Results_Citation
Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i	Method_Citation
e	Method_Citation
, at the sentence or word levels)	Method_Citation
  In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Method_Citation
In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Method_Citation
Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model	Hypothesis_Citation
Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model	Method_Citation
  In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Hypothesis_Citation
  In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Method_Citation
In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Method_Citation
In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs	Hypothesis_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Method_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Hypothesis_Citation
Topic-specific translation lexicons are learned by a 3-topic BiTAM1	Results_Citation
Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA)	Method_Citation
Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA)	Results_Citation
  Inter takes the intersection of the two directions and generates high-precision alignments;	Method_Citation
  Inter takes the intersection of the two directions and generates high-precision alignments;	Results_Citation
The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V 	Implication_Citation
The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V 	Method_Citation
 The data sparsity (i	Implication_Citation
 The data sparsity (i	Method_Citation
e	Implication_Citation
e	Method_Citation
, lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al	Implication_Citation
, lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al	Method_Citation
, 2003)	Implication_Citation
, 2003)	Method_Citation
  To reduce the data sparsity problem, we introduce two remedies in our models	Implication_Citation
  To reduce the data sparsity problem, we introduce two remedies in our models	Method_Citation
  First: Laplace smoothing	Implication_Citation
  First: Laplace smoothing	Method_Citation
Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model	Hypothesis_Citation
Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model	Method_Citation
  Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i	Hypothesis_Citation
  Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i	Method_Citation
e	Hypothesis_Citation
e	Method_Citation
, at the sentence or word levels)	Hypothesis_Citation
, at the sentence or word levels)	Method_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Hypothesis_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Method_Citation
Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality	Implication_Citation
  Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices	Implication_Citation
Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices	eanj , Bzn ).</S>
Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices	Implication_Citation
  For example, the word shot in â€œIt was a nice shot	Implication_Citation
â€ should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing	Implication_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Hypothesis_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Aim_Citation
Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation	Hypothesis_Citation
Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation	Aim_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Hypothesis_Citation
  We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Aim_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Hypothesis_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Aim_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Method_Citation
We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT	Results_Citation
  We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework	Method_Citation
  We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework	Results_Citation
  Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity  Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE)	Method_Citation
  Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity  Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE)	Results_Citation
  As shown in Table 4, the baseline IBM1 gives its best performance of 36	Method_Citation
  As shown in Table 4, the baseline IBM1 gives its best performance of 36	Results_Citation
27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40	Method_Citation
27% in the CE direc tion; the UDA alignments from BiTAM1∼3 give 40	Results_Citation
13%, 40	Method_Citation
13%, 40	Results_Citation
26%, and 40	Method_Citation
26%, and 40	Results_Citation
47%, respectively, which are significantly better than IBM1	Method_Citation
47%, respectively, which are significantly better than IBM1	Results_Citation
To reduce the data sparsity problem, we introduce two remedies in our models	Method_Citation
  Second: interpolation smoothing	Method_Citation
  Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting:	Method_Citation
It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic	Method_Citation
The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles	Method_Citation
  The numerical and date expressions got correct representations	Method_Citation
Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it	Method_Citation
Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it	Method_Citation
Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it	Method_Citation
  Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called “boxes” because of the way they are graphically displayed) for English sentences and texts	Method_Citation
Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called “boxes” because of the way they are graphically displayed) for English sentences and texts	Method_Citation
  It is distributed with the C&amp;C tools for natural language processing (Curran et al	Method_Citation
, 2007), which are hosted on this site: http://svn	Method_Citation
ask	Method_Citation
it	Method_Citation
usyd	Method_Citation
edu	Method_Citation
au/trac/candc/wiki/boxer	Method_Citation
Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it	Method_Citation
Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it	Implication_Citation
  We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992)	Method_Citation
  We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992)	Implication_Citation
Boxer is an open-domain tool for computing and reasoning with semantic representations	Implication_Citation
It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic	Method_Citation
6 Joint Segmentation and Parsing	Method_Citation
6 Joint Segmentation and Parsing	Method_Citation
But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline	Results_Citation
6 Joint Segmentation and Parsing	Method_Citation
Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â€“5% F1	Results_Citation
Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6)	Method_Citation
Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (§6)	Results_Citation
  Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0	Results_Citation
54 ADJP 1216 59	Results_Citation
45 S S S R 708 0	Results_Citation
57 SBAR 2918 69	Results_Citation
81 NP NP ADJ P R 803 0	Results_Citation
64 FRAG 254 72	Results_Citation
87 NP NP N P R 2907 0	Results_Citation
66 VP 5507 78	Results_Citation
83 NP NP SBA R R 1035 0	Results_Citation
67 S 6579 78	Results_Citation
91 NP NP P P R 2713 0	Results_Citation
67 PP 7516 80	Results_Citation
93 VP TAG P P R 3230 0	Results_Citation
80 NP 34025 84	Results_Citation
95 NP NP TAG L 805 0	Results_Citation
85 ADVP 1093 90	Results_Citation
64 VP TAG SBA R R 772 0	Results_Citation
86 WHN P 787 96	Results_Citation
00 S VP N P L 961 0	Results_Citation
87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths ≤ 70 (dev set, gold segmentation)	Results_Citation
  Table 9: Dev set results for sentences of length ≤ 70	Results_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (§3)	Results_Citation
Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (§3)	Method_Citation
  We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (§4)	Results_Citation
  We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (§4)	Method_Citation
  When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct � ?f iDafa	Results_Citation
  When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct � ?f iDafa	Method_Citation
  mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences	Results_Citation
  mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences	Method_Citation
We propose a limit of 70 words for Arabic parsing evaluations	Method_Citation
In our grammar, features are realized as annotations to basic category labels	Method_Citation
  We start with noun features since written Arabic contains a very high proportion of NPs	Method_Citation
8 We use head-finding rules specified by a native speaker	Results_Citation
By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English	Results_Citation
Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1	Results_Citation
92% F1	Results_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
We show that noun-noun vs	Results_Citation
 discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c)	Results_Citation
Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency	Results_Citation
We show that noun-noun vs	Results_Citation
 discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c)	Results_Citation
Lattice parsing (Chappelier et al	Method_Citation
, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart	Method_Citation
  We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton	Method_Citation
Better Arabic Parsing: Baselines, Evaluations, and Analysis	Aim_Citation
Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (§6)	Method_Citation
Better Arabic Parsing: Baselines, Evaluations, and Analysis	Aim_Citation
Lattice parsing (Chappelier et al	Method_Citation
, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart	Method_Citation
  We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton	Method_Citation
Preprocessing the raw trees improves parsing performance considerably	Method_Citation
9 We first discard all trees dominated by X, which indicates errors and non-linguistic text	Method_Citation
Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1	Results_Citation
92% F1	Results_Citation
Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1	Results_Citation
92% F1	Results_Citation
To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results	Aim_Citation
The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-	Method_Citation
  able at http://nlp	Method_Citation
stanford	Method_Citation
edu/projects/arabic	Method_Citation
shtml	Method_Citation
At the phrasal level, we remove all function tags and traces	Method_Citation
The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al	Method_Citation
, 2004) were purposefully borrowed without major modification from English (Marcus et al	Method_Citation
, 1993)	Method_Citation
Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features	Implication_Citation
Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features	Results_Citation
  For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate	Implication_Citation
  For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate	Results_Citation
  It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood	Implication_Citation
  It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood	Results_Citation
The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data	Method_Citation
3 9 97	Results_Citation
57 97	Results_Citation
97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset	Results_Citation
A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e	Method_Citation
g	Method_Citation
) computes the most probable POS tag sequence tË†N = tË†1, 	Method_Citation
, tË†N for a given word sequence wN 	Method_Citation
 POS taggers are usually trained on corpora with between 50 and 150 different POS tags	Method_Citation
Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities	Method_Citation
Our tagger generates a predictor for each feature (such as base POS, number, gender etc	Method_Citation
) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc	Method_Citation
 for base POS), the tagger builds a separate decision tree for each value	Method_Citation
Our tagger generates a predictor for each feature (such as base POS, number, gender etc	Method_Citation
) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc	Method_Citation
 for base POS), the tagger builds a separate decision tree for each value	Method_Citation
  A typical context attribute is â€œ1:ART	Method_Citation
Nomâ€ which states that the preceding tag is an article with the attribute â€œNomâ€	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets	Method_Citation
Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features	Method_Citation
  We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood	Results_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
The German Tiger treebank (Brants et al	Results_Citation
, 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall)	Results_Citation
  It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood	Results_Citation
A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e	Implication_Citation
g	Implication_Citation
) computes the most probable POS tag sequence tË†N = tË†1, 	Implication_Citation
, tË†N for a given word sequence wN 	Implication_Citation
 POS taggers are usually trained on corpora with between 50 and 150 different POS tags	Implication_Citation
  Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features	Implication_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood	Results_Citation
We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C  We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
The probability of an attribute (such as â€œNomâ€) is always conditioned on the respective base POS (such as â€œNâ€) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns	Method_Citation
Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging	Aim_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees	Method_Citation
We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags	Method_Citation
We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags	Results_Citation
  3 9 97	Method_Citation
  3 9 97	Results_Citation
57 97	Method_Citation
57 97	Results_Citation
97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset	Method_Citation
97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset	Results_Citation
We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags	Method_Citation
Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications	Method_Citation
The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&amp;Tâ€™s fsm (Mohri et al	Method_Citation
, 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005)	Method_Citation
Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples	Method_Citation
Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers	Method_Citation
Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples	Method_Citation
Foma is largely compatible with the Xerox/PARC finite-state toolkit	Method_Citation
Foma is largely compatible with the Xerox/PARC finite-state toolkit	Method_Citation
This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately	Method_Citation
At present, the â€˜Potsdam Commentary Corpusâ€™ (henceforth â€˜PCCâ€™ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily	Method_Citation
A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees	Method_Citation
The corpus has been annotated with six different types of information, which are characterized in the following subsections	Method_Citation
  Not all the layers have been produced for all the texts yet	Method_Citation
  Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase	Method_Citation
For the â€˜coreâ€™ portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective	Method_Citation
6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i	Method_Citation
e	Method_Citation
, relying largely on connectives) are not bright â€” but see Sections 3	Method_Citation
2 and 3	Method_Citation
3 below	Method_Citation
A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees	Method_Citation
We follow the guidelines developed in the TIGER project (Brants et al	Method_Citation
 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures	Method_Citation
That is, we can use the discourse parser on PCC texts, emulating for instance a â€œco-reference oracleâ€ that adds the information from our co-reference annotations	Method_Citation
The Potsdam Commentary Corpus	Method_Citation
This paper, however, provides a comprehensive overview of the data collection effort and its current state	Method_Citation
  At present, the â€˜Potsdam Commentary Corpusâ€™ (henceforth â€˜PCCâ€™ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily	Method_Citation
  The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus	Method_Citation
A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees	Method_Citation
A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees	Method_Citation
  Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization)	Method_Citation
The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation	Method_Citation
  At present, the â€˜Potsdam Commentary Corpusâ€™ (henceforth â€˜PCCâ€™ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily	Method_Citation
The Potsdam Commentary Corpus  A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure	Aim_Citation
At present, the â€˜Potsdam Commentary Corpusâ€™ (henceforth â€˜PCCâ€™ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily	Method_Citation
At present, the â€˜Potsdam Commentary Corpusâ€™ (henceforth â€˜PCCâ€™ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily	Implication_Citation
  The corpus has been annotated with six different types of information, which are characterized in the following subsections	Method_Citation
  The corpus has been annotated with six different types of information, which are characterized in the following subsections	Implication_Citation
  Not all the layers have been produced for all the texts yet	Method_Citation
  Not all the layers have been produced for all the texts yet	Implication_Citation
  For the â€˜coreâ€™ portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective	Method_Citation
  For the â€˜coreâ€™ portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective	Implication_Citation
6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i	Method_Citation
6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i	Implication_Citation
e	Method_Citation
e	Implication_Citation
, relying largely on connectives) are not bright â€” but see Sections 3	Method_Citation
, relying largely on connectives) are not bright â€” but see Sections 3	Implication_Citation
2 and 3	Method_Citation
2 and 3	Implication_Citation
3 below	Method_Citation
3 below	Implication_Citation
All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988)	Method_Citation
Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers	Method_Citation
A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure	Method_Citation
The Potsdam Commentary Corpus	Method_Citation
We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	Method_Citation
Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.	Implication_Citation
We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	Results_Citation
We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.	Method_Citation
Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research	Method_Citation
All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).	Method_Citation
Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research  We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).	Method_Citation
In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.	Implication_Citation
In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).	Method_Citation
Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).	Method_Citation
7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).	Method_Citation
4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.	Implication_Citation
We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).  This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.  Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.	Method_Citation
Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	Method_Citation
Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	Method_Citation
All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	Results_Citation
We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.	Method_Citation
4.1 Clustering Parameters.	Method_Citation
4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.  Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.	Method_Citation
In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to “the curse of dimensionality”?	Hypothesis_Citation
We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	Method_Citation
We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.	Aim_Citation
Levin’s classes form a hierarchy of verb groupings with shared meaning and syntax.  Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.	Method_Citation
Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.	Method_Citation
We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).	Results_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	Results_Citation
Moreover, we only apply the simple linear kernel, although other kernels can peform better.  The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.	Implication_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Aim_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Method_Citation
Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collins’ parser used in our system achieves the state-of-the-art performance.  Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment.	Implication_Citation
Moreover, we only apply the simple linear kernel, although other kernels can peform better.  The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.	Implication_Citation
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	Method_Citation
Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.	Results_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
For each pair of mentions3, we compute various lexical, syntactic and semantic features.  This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.  It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes “Based-In”, “Located” and “Residence” under the type “AT”, which are difficult even for human experts to differentiate.	Implication_Citation
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	Results_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Results_Citation
Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.  This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	Implication_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.  This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Results_Citation
Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	Method_Citation
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.  This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.	Method_Citation
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
• Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase.	Results_Citation
This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: • WM1: bag-of-words in M1 • HM1: head word of M1 3 In ACE, each mention has a head annotation and an.  extent annotation.	Method_Citation
This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.	Method_Citation
Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Results_Citation
Exploring Various Knowledge in Relation Extraction	Aim_Citation
Moreover, we only apply the simple linear kernel, although other kernels can peform better.	Method_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	Method_Citation
Exploring Various Knowledge in Relation Extraction	Aim_Citation
ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.	Implication_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).  ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype “Founder” under the type “ROLE”.	Method_Citation
This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	Method_Citation
It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	Results_Citation
In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.  The semantic relation is determined between two mentions.  In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.	Method_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	Method_Citation
Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.  This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	Results_Citation
Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.  This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.	Implication_Citation
Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.	Results_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance.	Results_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.  The reason why we choose SVMs for this purpose is that SVMs represent the state-of–the-art in the machine learning research community, and there are good implementations of the algorithm available.	Method_Citation
Exploring Various Knowledge in Relation Extraction	Aim_Citation
This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.	Method_Citation
In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.	Method_Citation
In this paper, we only measure the performance of relation extraction on “true” mentions with “true” chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus.	Method_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).	Method_Citation
It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.	Results_Citation
This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).  In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a “NONE” class for the case where the two mentions are not related.	Method_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	Method_Citation
This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.	Implication_Citation
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).	Method_Citation
Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.	Results_Citation
The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.	Aim_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information.	Method_Citation
Moreover, we only apply the simple linear kernel, although other kernels can peform better.	Method_Citation
Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).	Hypothesis_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.  We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	Results_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for im­ mediate reference.	Method_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).  The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	Method_Citation
Robust pronoun resolution with limited knowledge	Aim_Citation
If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.  The collocation preference here is restricted to the patterns &quot;noun phrase (pronoun), verb&quot; and &quot;verb, noun phrase (pronoun)&quot;.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).	Method_Citation
Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.  Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the anteÂ­ cedent.	Method_Citation
3.1 Evaluation A. Our first evaluation exercise (Mitkov &amp; Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).  The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%.  The evaluation indicated 83.6% success rate.	Results_Citation
Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante­ cedent.  The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section).	Method_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
The approach being robust (an attempt is made to resolve each anaphor and a pro­ posed antecedent is returned), this figure represents both &quot;precision&quot; and &quot;recall&quot; if we use the MUC terminology.	Implication_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
3.1 Evaluation A. Our first evaluation exercise (Mitkov &amp; Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).  The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging).  Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin&apos;s method, which not being available to us, had to be hand-simulated (see 3.3).  The evaluation indicated 83.6% success rate.	Method_Citation
3.1 Evaluation A. Our first evaluation exercise (Mitkov &amp; Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).  The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging).  Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin&apos;s method, which not being available to us, had to be hand-simulated (see 3.3).  The evaluation indicated 83.6% success rate.	Implication_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.	Method_Citation
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).	Implication_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Aim_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Results_Citation
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.  One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.  We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of tech­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Method_Citation
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.  One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.  We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of tech­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Results_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
With a view to avoiding complex syntactic, seman­ tic and discourse analysis (which is vital for real­ world applications), we developed a robust, knowl­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  Evaluation shows a success rate of 89.7% for the genre of tech­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.  We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	Method_Citation
With a view to avoiding complex syntactic, seman­ tic and discourse analysis (which is vital for real­ world applications), we developed a robust, knowl­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  Evaluation shows a success rate of 89.7% for the genre of tech­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.  We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).	Results_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.  Evaluation reports a success rate of 89.7% which is better than the suc­ cess rates of the approaches selected for comparison and tested on the same data.	Method_Citation
This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.  Evaluation reports a success rate of 89.7% which is better than the suc­ cess rates of the approaches selected for comparison and tested on the same data.	Results_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
We have described a robust, knowledge-poor ap­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.  Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	Method_Citation
Robust pronoun resolution with limited knowledge	Aim_Citation
We have described a robust, knowledge-poor ap­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.  Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	Method_Citation
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;nonÂ­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.	Method_Citation
It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	Implication_Citation
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;nonÂ­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	Method_Citation
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Results_Citation
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;nonÂ­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.  Top symptoms like &quot;lexical reiteration&quot; as­ sign score &quot;2&quot; whereas &quot;non-prepositional&quot; noun phrases are given a negative score of &quot;-1&quot;.  We should point out that the antecedent indicators are preferences and not absolute factors.	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.  Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.	Results_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
Robust pronoun resolution with limited knowledge	Aim_Citation
We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.	Method_Citation
These scores have been determined experimentally on an empirical basis and are constantly being up­ dated.  Top symptoms like &quot;lexical reiteration&quot; as­ sign score &quot;2&quot; whereas &quot;non-prepositional&quot; noun phrases are given a negative score of &quot;-1&quot;.  We should point out that the antecedent indicators are preferences and not absolute factors.	Method_Citation
For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not real­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.  It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.	Implication_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.  It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).	Method_Citation
This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.  Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.	Method_Citation
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;nonÂ­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.	Method_Citation
In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.  The success rate of the &quot;Baseline Subject&quot; was 29.2%, whereas the success rate of &quot;Baseline Most Recent NP&quot; was 62.5%.	Method_Citation
In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.  The success rate of the &quot;Baseline Subject&quot; was 29.2%, whereas the success rate of &quot;Baseline Most Recent NP&quot; was 62.5%.	Results_Citation
Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph­ ora resolution.  It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.	Implication_Citation
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.  The present proposal falls into the last group.  Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.	Method_Citation
Chinese word segmentation can be viewed as a stochastic transduction problem.  More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	Method_Citation
The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.  Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.	Implication_Citation
The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.  This method, one instance of which we term the &quot;greedy algorithm&quot; in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin­ ning) of the sentence is reached.  The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.	Method_Citation
In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.  Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.	Method_Citation
Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.	Method_Citation
The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	Method_Citation
The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	Implication_Citation
The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	Results_Citation
An initial step of any textÂ­ analysis task is the tokenization of the input into words.  Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	Method_Citation
The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.  The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15	Results_Citation
This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	Method_Citation
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information.	Results_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	Method_Citation
Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.	Implication_Citation
The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	Implication_Citation
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.	Method_Citation
This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	Method_Citation
Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.  The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.	Implication_Citation
Chinese word segmentation can be viewed as a stochastic transduction problem.	Method_Citation
In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces;  Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	Method_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	Method_Citation
More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	Method_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.	Method_Citation
More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	Method_Citation
More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).	Method_Citation
The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.  However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	Method_Citation
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.	Method_Citation
Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 &apos;Shamir,&apos; which is a legal Chi­ nese personal name, retains a foreign flavor because of liM.  As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabil­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	Method_Citation
There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	Aim_Citation
Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 &apos;Shamir,&apos; which is a legal Chi­ nese personal name, retains a foreign flavor because of liM. 	Method_Citation
As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	Method_Citation
A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.	Method_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.  This method, one instance of which we term the &quot;greedy algorithm&quot; in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin­ ning) of the sentence is reached.	Method_Citation
Previous Work.  There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).	Method_Citation
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	Method_Citation
The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.  However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.	Implication_Citation
The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.	Method_Citation
The performance was 80.99% recall and 61.83% precision.	Results_Citation
The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).  We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.  However, there will remain a large number of words that are not readily adduced to any producÂ­ tive pattern and that would simply have to be added to the dictionary.	Implication_Citation
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.  This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	Method_Citation
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recal	Method_Citation
Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).	Method_Citation
The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).  However, there will remain a large number of words that are not readily adduced to any producÂ­ tive pattern and that would simply have to be added to the dictionary.	Implication_Citation
Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.  Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.	Implication_Citation
The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.  This method, one instance of which we term the &quot;greedy algorithm&quot; in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin­ ning) of the sentence is reached.	Method_Citation
In this paper we present a stochastic finite-state model for segmenting Chinese text into words  Word frequencies are estimated by a re-estimation procedure that involves apply­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.  newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.  This larger corpus was kindly provided to us by United Informatics Inc.,	Method_Citation
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement	Results_Citation
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese  The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement	Results_Citation
More formally, we start by representing the dictionary D as a Weighted Finite State Trans­ ducer (WFST) (Pereira, Riley, and Sproat 1994).  This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).  This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.	Method_Citation
In this paper we present a stochastic finite-state model for segmenting Chinese text into words	Method_Citation
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese	Method_Citation
The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.	Method_Citation
As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.	Method_Citation
A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.	Method_Citation
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement	Results_Citation
In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.  Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.  Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.	Method_Citation
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement	Results_Citation
The average agreement among the human judges is .76	Results_Citation
The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.	Method_Citation
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.  This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.	Implication_Citation
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.  It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen con­ structions, including morphological derivatives and personal names.  We will evaluate various specific aspects of the segmentation, as well as the overall segmentation per­ formance.	Method_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.  This method, one instance of which we term the &quot;greedy algorithm&quot; in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin­ ning) of the sentence is reached.	Method_Citation
The morphological anal­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.  each word in the lexicon whether or not each string is actually an instance of the word in question.	Method_Citation
The most popular approach to dealing with seg­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.  This method, one instance of which we term the &quot;greedy algorithm&quot; in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (begin­ ning) of the sentence is reached.	Method_Citation
We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.  selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.  The performance was 80.99% recall and 61.83% precision.	Method_Citation
We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.  selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.  The performance was 80.99% recall and 61.83% precision.	Results_Citation
In this paper we present a stochastic finite-state model for segmenting Chinese text into words	Method_Citation
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.	Method_Citation
Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	Method_Citation
Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	Method_Citation
Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.	Method_Citation
Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.	Method_Citation
Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.	Method_Citation
Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.	Method_Citation
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETâ€™s hierarchical structure to create many annotated training instances from the synset glosses.	Method_Citation
Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	Method_Citation
Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	Method_Citation
Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.	Method_Citation
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETâ€™s hierarchical structure to create many annotated training instances from the synset glosses.	Method_Citation
Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.	Method_Citation
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETâ€™s hierarchical structure to create many annotated training instances from the synset glosses.	Method_Citation
We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.  We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Results_Citation
We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.  We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	Results_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman­ tic components.	Hypothesis_Citation
2.1 Ambiguities in Levin classes.  We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared seman­ tic components.	Hypothesis_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.	Hypothesis_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
The ad­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise.  The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more &quot;pure&quot; examples of the carry class and always imply the achievement of causation of motion.  Investigating regular sense extensions based on intersective Levin classes	Implication_Citation
The fundamental assumption is that the syntactic frames are a direct reflection of the un­ derlying semantics.	Hypothesis_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping to­ gether subsets of existing classes with over­ lapping members.	Method_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
First, since our experi­ ment was based on a translation from English to Portuguese	Aim_Citation
Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb.	Implication_Citation
A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class.	Method_Citation
Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.	Hypothesis_Citation
We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents.	Method_Citation
The fundamental assumption is that the syntactic frames are a direct reflection of the un­ derlying semantics.	Hypothesis_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.	Hypothesis_Citation
Investigating regular sense extensions based on intersective Levin classes	Aim_Citation
Current approaches to English classifica­ tion, Levin classes and WordNet, have limita­ tions in their applicability that impede their utility as general classification schemes.	Implication_Citation
Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses.	Hypothesis_Citation
Two current approaches to English verb classiÂ­ fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993).	Method_Citation
We proposed an unsupervised method to discover paraphrases from a large untagged corpus.	Method_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Aim_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Method_Citation
For example, in Information Retrieval (IR), we have to match a userâ€™s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâ€™s question even if the formulation of the answer in the document is different from the question.  While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.  In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	Implication_Citation
For example, in Information Retrieval (IR), we have to match a userâ€™s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâ€™s question even if the formulation of the answer in the document is different from the question.  While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.  In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	Method_Citation
In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.	Method_Citation
In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.  The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].	Method_Citation
The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].  These 140 NE categories are designed by extending MUC’s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).	Method_Citation
Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.  Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].	Method_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Aim_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Method_Citation
For each pair we also record the context, i.e. the phrase between the two NEs (Step1).  Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.	Method_Citation
Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.	Method_Citation
Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.	Method_Citation
Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.  In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.	Implication_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Aim_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Method_Citation
In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.  We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	Aim_Citation
In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.  We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	Method_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Aim_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Method_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Aim_Citation
Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs	Method_Citation
Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.	Implication_Citation
There have been other kinds of efforts to discover paraphrase automatically from corpora.	Implication_Citation
There have been other kinds of efforts to discover paraphrase automatically from corpora.	Implication_Citation
Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.	Implication_Citation
Cluster phrases based on Links We now have a set of phrases which share a keyword.  At this step, we will try to link those sets, and put them into a single cluster.	Method_Citation
Hereafter, each pair of NE categories will be called a domain; e.g. the “Company – Company” domain, which we will call CC- domain (Step 2).  For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3).	Method_Citation
In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.	Method_Citation
For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	Method_Citation
In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).  When translating the sentence monotonically from left to right, the translation of the German finite verb &apos;kann&apos;, which is the left verbal brace in this case, is postponed until the German noun phrase &apos;mein Kollege&apos; is translated, which is the subject of the sentence.	Method_Citation
The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.  For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.	Method_Citation
In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).	Method_Citation
For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words.	Method_Citation
We apply a beam search concept as in speech recognition.	Method_Citation
For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.	Method_Citation
(1), Pr(eI 1) is the language model, which is a trigram language model in this case.	Method_Citation
For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.	Method_Citation
A dynamic programming recursion similar to the one in Eq. 2 is evaluated.  We apply a beam search concept as in speech recognition.	Method_Citation
For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words.	Method_Citation
The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Æ = 0 or Æ = 1 new target words.	Implication_Citation
The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Æ = 0 or Æ = 1 new target words.	Method_Citation
What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be &apos;hit&apos; exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation	Implication_Citation
This measure has the advantage of being completely automatic.	Implication_Citation
For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÆcient to consider only the best 50 words.	Method_Citation
We apply a beam search concept as in speech recognition.	Method_Citation
In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document.  We use backoff and linear interpolation for probability estimation:	Method_Citation
To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	Method_Citation
Comparable corpora refer to texts that are not direct translation but are about the same topic.  For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.	Implication_Citation
We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm.	Method_Citation
So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2).	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Aim_Citation
In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.	Method_Citation
In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.	Implication_Citation
To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus.	Method_Citation
Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	Method_Citation
It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.  Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	Method_Citation
It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.  Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	Results_Citation
In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.	Implication_Citation
Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.	Results_Citation
A confidence measure threshold, t, was defined for making a decision based on the value.  If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.	Results_Citation
A confidence measure threshold, t, was defined for making a decision based on the value.  If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.  Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.  However, the R-iv rates were getting worse in return for higher R-oov rates.	Method_Citation
A confidence measure threshold, t, was defined for making a decision based on the value.  If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.  Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.  However, the R-iv rates were getting worse in return for higher R-oov rates.	Results_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	Method_Citation
Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.  We also successfully employed the confidence measure to make a confidence-dependent word segmentation.  We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	Method_Citation
In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.  We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.	Method_Citation
For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.  Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.  We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	Method_Citation
We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.  We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.	Results_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.	Method_Citation
For a character-based IOB tagger, there is only one possibility of re-segmentation.  However, there are multiple choices for a subword-based IOB tagger.	Implication_Citation
